# 同时使用 mHC 和 Flash-IPA（1-10）

## 概述

本实现支持**同时使用 mHC (Manifold-Constrained Hyper-Connections) 和 Flash-IPA**，结合两种技术的优势：

- **mHC**: 提供训练稳定性，通过扩展的残差流和双随机混合
- **Flash-IPA**: 提供内存效率，支持长序列的高效注意力计算

## 为什么要同时使用？

| 特性 | 仅 Flash-IPA | 仅 mHC | **mHC + Flash-IPA** |
|------|-------------|--------|---------------------|
| 内存效率 | ✅ 高效 | ❌ 标准 | ✅ 高效 |
| 训练稳定性 | ⚠️ 一般 | ✅ 稳定 | ✅ 稳定 |
| 长序列支持 | ✅ 512-1024 | ❌ 256-512 | ✅✅ 512-1024+ |
| 大批量训练 | ⚠️ 可能不稳定 | ✅ 稳定 | ✅ 稳定 |
| GPU 要求 | Flash Attention 2/3 | 任意 | Flash Attention 2/3 |

**推荐使用场景**：
- 训练 512-1024 残基的长序列蛋白质
- 需要大批量训练但担心训练不稳定
- 有支持 Flash Attention 的 GPU (Hopper架构)

## 架构说明

### 标准架构
```
Input → Single Features → Pair Features → Pair Transform 
      → Standard IPA → Output
```

### mHC 架构
```
Input → Single Features → Pair Features → Pair Transform 
      → mHC(Standard IPA) → Output
      
mHC 包装:
  s_expanded = H_pre @ s_in
  s_ipa = IPA(s_expanded)
  s_out = H_res @ s_in + H_post @ s_ipa
```

### Flash-IPA 架构
```
Input → Single Features → Pair Features → Factorizer 
      → Flash-IPA(z_factors) → Output
```

### **mHC + Flash-IPA 架构（本实现）**
```
Input → Single Features → Pair Features → Factorizer 
      → mHC(Flash-IPA(z_factors)) → Output
      
组合包装:
  s_expanded = H_pre @ s_in
  s_flash = Flash-IPA(s_expanded, z_factors)  # 内存高效
  s_out = H_res @ s_in + H_post @ s_flash      # 稳定混合
```

## 配置方法

### 1. 基本配置

在配置文件中同时启用两个标志：

```txt
# 同时启用 mHC 和 Flash-IPA
useMHCMode True
useFlashMode True

# Flash-IPA 参数
zFactorRank 2
kNeighbors 10
useFlashAttn3 True

# mHC 参数
mhcExpansionRate 4
mhcSinkhornIters 20
mhcAlphaInit 0.01
```

## 参数调优指南

### Flash-IPA 参数

#### `zFactorRank` (边嵌入分解秩)
- **1**: 最小内存，适合 1024+ 残基超长序列
- **2**: 平衡性能和内存，推荐用于 512-768 残基 
- **>2**: 不支持（Flash Attention 限制）

#### `kNeighbors` (最近邻数量)
- **6-8**: 更快，适合长序列 (768-1024)
- **10-12**: 标准，适合中长序列 (512-768) 
- **14-16**: 更准确，适合较短序列 (256-512)

#### `useFlashAttn3`
- **True**: 在 H100/H800 上使用 FA3 (推荐) 
- **False**: 使用 FA2，兼容 A100 等 Ampere GPU

### mHC 参数

#### `mhcExpansionRate` (扩展率)
- **4**: 标准设置，平衡性能 
- **6-8**: 更强表达能力，但内存开销大
- **2**: 最小开销，但可能影响稳定性

#### `mhcSinkhornIters` (Sinkhorn 迭代)
- **10-15**: 更快，轻微牺牲稳定性
- **20**: 标准值，充分收敛 
- **30+**: 最稳定，但计算开销大

#### `mhcAlphaInit` (门控初始化)
- **0.001**: 初期更多依赖残差
- **0.01**: 标准值 
- **0.1**: 初期更多使用 IPA 输出

## 细节

### 前向传播流程

1. **输入处理**
   ```python
   s = SingleFeatureNet(inputs)  # [B, L, C]
   p = PairFeatureNet(inputs, s)  # [B, L, L, C_p]
   ```

2. **Pair 特征分解**（Flash-IPA 需要）
   ```python
   z_factor_1, z_factor_2 = Factorizer(p, mask)
   # z_factor_1: [B, L, rank, C_z]
   # z_factor_2: [B, L, n_head, rank, C_z//4]
   ```

3. **mHC 扩展**（第一层）
   ```python
   s_expanded = mHC.expand_input(s)  # [B, L, C] → [B, L, n, C]
   ```

4. **mHC + Flash-IPA 层**
   ```python
   H_pre, H_post, H_res = mHC.compute_mappings(s_expanded)
   s_contracted = H_pre @ s_expanded  # [B, L, n, C] → [B, L, C]
   s_ipa = FlashIPA(s_contracted, z_factors, rigid, mask)  # Flash attention
   s_residual = H_res @ s_expanded  # [B, L, n, C]
   s_output = s_residual + (H_post^T @ s_ipa)  # [B, L, n, C]
   ```

5. **mHC 压缩**（最后一层）
   ```python
   s_final = s_expanded.mean(dim=-2)  # [B, L, n, C] → [B, L, C]
   ```

### 稳定性机制

- **双随机混合**: H_res 投影到 Birkhoff 多面体，保证稳定的残差混合
- **门控初始化**: 通过 `mhcAlphaInit` 控制 IPA vs 残差的初始权重
- **梯度流**: 扩展的残差流提供更多梯度路径

## 可能故障排除

### "mHC + Flash-IPA Denoiser not available"

**解决方案**:

```bash
# 确保已安装 flash-attention
pip install flash-attn --no-build-isolation

# 检查导入
python -c "from genie.model.mhc_flash_denoiser import mHCFlashDenoiser; print('OK')"
```

### 显存不足 (OOM)

**解决方案**:

1. 减小批量大小: `batchSize 32`
2. 降低 `zFactorRank`: `zFactorRank 1`
3. 减少 `kNeighbors`: `kNeighbors 8`
4. 降低 `mhcExpansionRate`: `mhcExpansionRate 2`
5. 启用梯度检查点: `useGradientCheckpointing True`

### 训练不稳定/NaN loss

**解决方案**:
1. 增加 Sinkhorn 迭代: `mhcSinkhornIters 30`
2. 降低学习率: `learningRate 1e-4`
3. 增加预热: `warmupEpoches 200`
4. 启用梯度裁剪: `gradientClipVal 0.5`

### 训练太慢

**解决方案**:
1. 确保使用 FA3 (Hopper GPU): `useFlashAttn3 True`
2. 减少 Sinkhorn 迭代: `mhcSinkhornIters 15`
3. 禁用梯度检查点: `useGradientCheckpointing False`
4. 减少 Pair Transform 层: `numPairTransformLayers 4`
