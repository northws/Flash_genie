# ğŸ¯ Genie é•¿åºåˆ—æ‰©å±•é¡¹ç›®æ€»ç»“

> [!CAUTION]
>
> GitHubç½‘é¡µä¸Š**éƒ¨åˆ†æ•°å­¦å…¬å¼æ— æ³•æ­£å¸¸æ˜¾ç¤º**ï¼Œå¦‚éœ€æŸ¥çœ‹åªèƒ½åˆ°æœ¬åœ°markdownç¼–è¾‘å™¨ä¸­æŸ¥çœ‹

## ğŸ“Š é¡¹ç›®ç»“æœ

### å½“å‰å®ç°

| ç»´åº¦ | è¯´æ˜ |
|------|------|
| **Flash-IPA é›†æˆ(Stage 1)** | âœ… å·²å®ç° |
| **mHC é›†æˆ(Stage 1)** | âœ… å·²å®ç° |
| **Pair Features(Stage 1)** | âœ… å·²å®ç°ï¼Œ V2ä¿®å¤æ•°å­¦é—®é¢˜ï¼Œä¿ç•™å‡ ä½•ä¿¡æ¯ |
| **Triangle Ops (Stage 2)** | âœ… å› å­åŒ–ä¸‰è§’æ“ä½œ |
| **Training (Stage 3)** | âœ… æ¸è¿›å¼è®­ç»ƒ+æ··åˆç²¾åº¦ |
| **Sparse Pairs (Stage 3 V2)** | âœ… k-NNç¨€ç–å¯¹ |
| **Axial Attention (Stage 4)** | âœ… è®¡ç®—æ•ˆç‡ä¼˜åŒ– |
| **Model Compression (Stage 4)** | âœ… å‚æ•°å…±äº« |
| **Distributed Training (Stage 5)** | âœ… å¤šGPUæ”¯æŒ |
| **è®­ç»ƒç¨³å®šæ€§** | âœ… mHC + Progressive Training |
| **æ–‡æ¡£å®Œæ•´æ€§** | âœ… å®Œæ•´çš„æ–‡æ¡£å’Œæµ‹è¯• |

## v1 + v1.2: Factorized Pair Features

 `factorized_pair_features.py` æ¨¡å—æ ¸å¿ƒåŠŸèƒ½çš„æ•°å­¦é€»è¾‘æè¿°å¦‚ä¸‹

è¯¥æ¨¡å—çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°† $O(L^2)$ çš„é…å¯¹å¼ é‡ $\mathbf{P}$ åˆ†è§£ä¸ºä¸¤ä¸ªç§©ä¸º $R$ çš„ä½ç§©å› å­ $\mathbf{F}_L$ å’Œ $\mathbf{F}_R$ï¼Œä»è€Œå°†å†…å­˜å¤æ‚åº¦ä» $O(L^2 \cdot C)$ é™ä½åˆ° $O(L \cdot R \cdot C)$ã€‚

### 1. æ ¸å¿ƒåˆ†è§£å…¬å¼ (Core Factorization)

å®Œæ•´çš„é…å¯¹ç‰¹å¾ $\mathbf{p}_{ij} \in \mathbb{R}^C$ è¢«è¿‘ä¼¼ä¸ºä¸¤ä¸ªå› å­å¼ é‡çš„æ”¶ç¼©ã€‚

è®¾ï¼š

- $L$ ä¸ºåºåˆ—é•¿åº¦
- $R$ ä¸ºåˆ†è§£çš„ç§© (Rank)
- $C$ ä¸ºç‰¹å¾ç»´åº¦

é‡æ„å…¬å¼ï¼ˆå¯¹åº”ä»£ç ä¸­çš„ `reconstruct_pair` å’Œ `forward` çš„é€»è¾‘ï¼‰ï¼š

$$\mathbf{p}_{ij} = \sum_{r=1}^{R} \left( \mathbf{f}_{L, i, r} \odot \mathbf{f}_{R, j, r} \right)$$

å…¶ä¸­ï¼š

- $\mathbf{f}_{L, i, r} \in \mathbb{R}^C$ æ˜¯å·¦å› å­åœ¨ä½ç½® $i$ã€ç§© $r$ çš„ç‰¹å¾ã€‚
- $\mathbf{f}_{R, j, r} \in \mathbb{R}^C$ æ˜¯å³å› å­åœ¨ä½ç½® $j$ã€ç§© $r$ çš„ç‰¹å¾ã€‚
- $\odot$ è¡¨ç¤ºæ²¿ç‰¹å¾ç»´åº¦ $C$ çš„é€å…ƒç´ ä¹˜æ³• (Hadamard product)ã€‚

------

### 2. å› å­ç”Ÿæˆè¿‡ç¨‹ (Factor Generation)

å› å­ $\mathbf{F}_L$ å’Œ $\mathbf{F}_R$ ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šå•åºåˆ—æŠ•å½±ã€ç›¸å¯¹ä½ç½®ç¼–ç å’Œæ¨¡æ¿ç‰¹å¾ã€‚

$$\mathbf{f}_{L, i, r} = \mathbf{f}_{L, i, r}^{(s)} + \mathbf{f}_{L, i, r}^{(\text{rel})} + \mathbf{f}_{L, i, r}^{(\text{tmpl})}$$

$$\mathbf{f}_{R, j, r} = \mathbf{f}_{R, j, r}^{(s)} + \mathbf{f}_{R, j, r}^{(\text{rel})} + \mathbf{f}_{R, j, r}^{(\text{tmpl})}$$

#### 2.1 å•åºåˆ—ç‰¹å¾æŠ•å½± (Single Feature Projection)

ç»™å®šå•åºåˆ—ç‰¹å¾ $\mathbf{s}_i \in \mathbb{R}^{C_s}$ï¼š

$$\mathbf{f}_{L, i, r}^{(s)} = \mathbf{W}_{L}^{(s)} \mathbf{s}_i, \quad \mathbf{f}_{R, j, r}^{(s)} = \mathbf{W}_{R}^{(s)} \mathbf{s}_j$$

#### 2.2 å› å­åŒ–åå¯¹ç§°ç›¸å¯¹ä½ç½®ç¼–ç  (Factorized Antisymmetric RelPos)

ä»£ç ç±»ï¼š`FactorizedRelPos`

ä¸ºäº†åœ¨å› å­åŒ–å½¢å¼ä¸­ä¿ç•™ç›¸å¯¹ä½ç½® $i-j$ çš„åå¯¹ç§°æ€§è´¨ï¼ˆå› ä¸º $i-j \neq j-i$ï¼‰ï¼Œå¼•å…¥äº†åå¯¹ç§°åç½® $\mathbf{b}_{rel}$ã€‚

1. **ä½ç½®åµŒå…¥ç»„åˆ**:

   $$\mathbf{h}_i = \left[ \text{Emb}_{\text{abs}}(i) \, ; \, \text{Emb}_{\text{bin}}\left( \text{clamp}(i - \frac{L}{2}) \right) \right]$$

2. **ç”Ÿæˆå› å­**:

   $$\mathbf{f}_{L, i, r}^{(\text{rel})} = \mathbf{W}_{L}^{(\text{pos})} \mathbf{h}_i + \mathbf{b}_{\text{rel}, r}$$

   $$\mathbf{f}_{R, j, r}^{(\text{rel})} = \mathbf{W}_{R}^{(\text{pos})} \mathbf{h}_j - \mathbf{b}_{\text{rel}, r}$$

   *æ³¨ï¼šå½“è®¡ç®—äº¤å‰é¡¹æ—¶ï¼Œ$(+b)(\dots) + (\dots)(-b)$ çš„ç»“æ„æœ‰åŠ©äºæ‰“ç ´åå¯¹ç§°æ€§ã€‚*

#### 2.3 å› å­åŒ–æ¨¡æ¿ç‰¹å¾ (Factorized Template - SVD Style)

ä»£ç ç±»ï¼š`FactorizedTemplate`

ç»™å®šæ¨¡æ¿å¼ é‡ $\mathbf{T} \in \mathbb{R}^{L \times L \times C_t}$ï¼Œä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–èšåˆè¡Œå’Œåˆ—ä¿¡æ¯ã€‚

1. **ç‰¹å¾æå–**:

   - å¯¹è§’çº¿: $\mathbf{v}_{\text{diag}, i} = \mathbf{T}_{ii}$
   - è¡Œèšåˆ: $\mathbf{v}_{\text{row}, i} = \sum_k \text{Softmax}(\mathbf{w}_q^T \mathbf{T}_{ik}) \cdot \mathbf{T}_{ik}$
   - åˆ—èšåˆ: $\mathbf{v}_{\text{col}, i} = \sum_k \text{Softmax}(\mathbf{w}_q^T \mathbf{T}_{ki}) \cdot \mathbf{T}_{ki}$

2. **éå¯¹ç§°ç»„åˆ (Asymmetric Combination)**:

   $$\mathbf{u}_{L, i} = \text{LayerNorm}([\mathbf{v}_{\text{diag}, i}, \mathbf{v}_{\text{row}, i}, \mathbf{v}_{\text{col}, i}])$$

   $$\mathbf{u}_{R, j} = \text{LayerNorm}([\mathbf{v}_{\text{diag}, j}, \mathbf{v}_{\text{col}, j}, \mathbf{v}_{\text{row}, j}]) \quad (\text{æ³¨æ„è¡Œåˆ—é¡ºåºäº¤æ¢})$$

3. **SVDé£æ ¼æŠ•å½±**:

   $$\mathbf{f}_{L, i, r}^{(\text{tmpl})} = (\mathbf{W}_U \mathbf{u}_{L, i}) \cdot \sigma_r$$

   $$\mathbf{f}_{R, j, r}^{(\text{tmpl})} = \mathbf{W}_V \mathbf{u}_{R, j}$$

   å…¶ä¸­ $\sigma_r$ æ˜¯å¯å­¦ä¹ çš„å¥‡å¼‚å€¼æ ‡é‡ï¼Œç”¨äºåŠ æƒä¸åŒç§©çš„é‡è¦æ€§ã€‚

------

### 3. å› å­åŒ–é…å¯¹ç²¾ç‚¼ (Factorized Pair Refinement)

ä»£ç ç±»ï¼š`FactorizedPairRefinementLayer`

è¯¥æ¨¡å—æ¨¡æ‹Ÿä¸‰è§’æ›´æ–°ï¼ˆTriangular Updateï¼‰ï¼Œä½†ä¸æ„å»º $L^2$ çŸ©é˜µï¼Œè€Œæ˜¯é€šè¿‡å› å­é—´çš„äº¤å‰æ³¨æ„åŠ›å®ç°ã€‚

å¯¹äºæ¯ä¸€å±‚ $l$ å’Œæ¯ä¸€ä¸ªç§© $r$ï¼š

$$\mathbf{\hat{f}}_{L, i, r} = \text{LayerNorm}(\mathbf{f}_{L, i, r})$$

$$\mathbf{\hat{f}}_{R, j, r} = \text{LayerNorm}(\mathbf{f}_{R, j, r})$$

**äº¤å‰æ³¨æ„åŠ› (Cross-Factor Interaction):**

$$\mathbf{z}_{L, i, r} = \text{Attention}(Q=\mathbf{\hat{f}}_{L, \cdot, r}, K=\mathbf{\hat{f}}_{R, \cdot, r}, V=\mathbf{\hat{f}}_{R, \cdot, r})_i$$

$$\mathbf{z}_{R, j, r} = \text{Attention}(Q=\mathbf{\hat{f}}_{R, \cdot, r}, K=\mathbf{\hat{f}}_{L, \cdot, r}, V=\mathbf{\hat{f}}_{L, \cdot, r})_j$$

**é—¨æ§æ›´æ–° (Gated Update):**

$$\mathbf{f}_{L, i, r} \leftarrow \mathbf{f}_{L, i, r} + \sigma(\mathbf{W}_{g1}[\mathbf{f}_{L, i, r}; \mathbf{z}_{L, i, r}]) \odot \mathbf{z}_{L, i, r}$$

æœ€åï¼Œé€šè¿‡ç§©æ··åˆå±‚ (Rank Mixing) å…è®¸ä¸åŒç§©ä¹‹é—´äº¤æ¢ä¿¡æ¯ï¼š

$$\mathbf{F}_L \leftarrow \text{Linear}_{\text{rank}}(\mathbf{F}_L), \quad \mathbf{F}_R \leftarrow \text{Linear}_{\text{rank}}(\mathbf{F}_R)$$

## Stage 2 æ›´æ–° (2026-01-11)

### Triangle Operations ä¼˜åŒ–

Stage 2 å®ç°äº†ç±»ä¼¼AlphaFold2 Evoformer çš„ Triangle Operationsï¼Œä½†æ­¤å¤„é€šè¿‡å› å­åŒ–ï¼Œé¿å… O(LÂ³) å†…å­˜å¼€é”€ã€‚

#### **é—®é¢˜**: åŸå§‹ä¸‰è§’ä¹˜æ³•æ›´æ–°éœ€è¦ O(LÂ³) å†…å­˜ï¼Œä¸‰è§’æ³¨æ„åŠ›éœ€è¦ O(LÂ²) æ³¨æ„åŠ›çŸ©é˜µ

 `factorized_triangle_ops.py` æ¨¡å—å®ç°çš„å› å­åŒ–ä¸‰è§’æ“ä½œçš„æ•°å­¦é€»è¾‘æè¿°å¦‚ä¸‹ï¼Œè¯¥æ¨¡å—ä¸»è¦é’ˆå¯¹ AlphaFold2 ä¸­å†…å­˜æ¶ˆè€—å·¨å¤§çš„â€œä¸‰è§’æ›´æ–°â€å’Œâ€œä¸‰è§’æ³¨æ„åŠ›â€è¿›è¡Œäº†ä½ç§©å› å­åŒ–ï¼ˆFactorizedï¼‰å’Œåˆ†å—ï¼ˆChunkedï¼‰ä¼˜åŒ–ã€‚

### ç¬¦å·å®šä¹‰

- $L$: åºåˆ—é•¿åº¦ (Sequence Length)
- $R$: ç§© (Rank)
- $C$: é€šé“æ•° (Channels/Feature dimension)
- $H$: æ³¨æ„åŠ›å¤´æ•° (Number of Heads)
- $d_h$: æ¯ä¸ªå¤´çš„ç»´åº¦ ($C_{hidden} / H$)
- $\sigma$: Sigmoid æ¿€æ´»å‡½æ•°
- $\text{LN}$: Layer Normalization

------

### 1. å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (Factorized Triangle Multiplicative Update)

è¯¥æ¨¡å—å¯¹åº”ç±» `FactorizedTriangleMultiplicativeUpdate`ã€‚å®ƒå°†åŸæœ¬ $O(L^3)$ çš„å¤æ‚åº¦é™ä½ä¸º $O(L^2 \times R)$ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š

åŸå§‹çš„ä¸‰è§’ä¹˜æ³•æ›´æ–°è®¡ç®—å…¬å¼ä¸ºï¼š

$$z_{ij} \leftarrow \sum_{k} a_{ik} \odot b_{kj}$$

ä»£ç ä¸­é€šè¿‡ç»´æŠ¤ä½ç§©å› å­ $Z_{left}, Z_{right} \in \mathbb{R}^{L \times R \times C}$ æ¥é¿å…æ„å»ºå®Œæ•´çš„ $L \times L$ å¼ é‡ã€‚

#### 1.1 é¢„å¤„ç†ä¸æŠ•å½±

é¦–å…ˆå¯¹è¾“å…¥å› å­è¿›è¡Œå½’ä¸€åŒ–å’Œé—¨æ§çº¿æ€§æŠ•å½±ï¼š

$$\begin{aligned} Z'_{left} &= \text{LN}(Z_{left}) \\ Z'_{right} &= \text{LN}(Z_{right}) \end{aligned}$$

ç”Ÿæˆä¸­é—´å˜é‡ $A$ å’Œ $B$ï¼ˆåŒ…å«é—¨æ§æœºåˆ¶ï¼‰ï¼š

$$\begin{aligned} A_{left} &= \text{Linear}_{a}(Z'_{left}) \odot \sigma(\text{Linear}_{g\_a}(Z'_{left})) \\ B_{right} &= \text{Linear}_{b}(Z'_{right}) \odot \sigma(\text{Linear}_{g\_b}(Z'_{right})) \end{aligned}$$

#### 1.2 è·¨ç§©æ··åˆ (Rank Mixing)

ä¸ºäº†å…è®¸ä¸åŒç§©ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ï¼Œå¯¹ç§©ç»´åº¦ $R$ è¿›è¡Œçº¿æ€§å˜æ¢ï¼š

$$\begin{aligned} \tilde{A} &= A_{left} W_{mix\_a}^T \\ \tilde{B} &= B_{right} W_{mix\_b}^T \end{aligned}$$

å…¶ä¸­ $W_{mix} \in \mathbb{R}^{R \times R}$ã€‚

#### 1.3 å› å­åŒ–èšåˆ (Factorized Aggregation)

è¿™æ˜¯ä¼˜åŒ–çš„æ ¸å¿ƒã€‚ä»¥ **å‡ºè¾¹ (Outgoing)** ä¸ºä¾‹ï¼Œä»£ç è¿‘ä¼¼è®¡ç®—äº†æ‰€æœ‰ä¸­é—´èŠ‚ç‚¹ $k$ çš„èšåˆä¿¡æ¯ï¼š

$$\bar{B} = \frac{1}{L} \sum_{k=1}^{L} \tilde{B}_{k}$$

è¿™é‡Œ $\bar{B} \in \mathbb{R}^{1 \times R \times C_{hidden}}$ æ˜¯å¯¹åºåˆ—ç»´åº¦çš„å‡å€¼èšåˆã€‚

æ›´æ–°å·¦å› å­çš„å…¬å¼ä¸ºï¼š

$$U_{left} = \tilde{A} \odot \bar{B}$$

(æ³¨ï¼šå¯¹äºå…¥è¾¹ Incomingï¼Œæ“ä½œæ˜¯å¯¹ç§°çš„ï¼šèšåˆ $\tilde{A}$ å¹¶æ›´æ–° $\tilde{B}$)ã€‚

#### 1.4 è¾“å‡ºæŠ•å½±

æœ€ç»ˆè¾“å‡ºç»è¿‡å½’ä¸€åŒ–ã€æŠ•å½±ã€é—¨æ§å’Œ Dropoutï¼š

$$\begin{aligned} O_{left} &= \text{Linear}_{out}(\text{LN}(U_{left})) \\ Gate_{left} &= \sigma(\text{Linear}_{gate}(Z_{left})) \\ Z_{out} &= \text{Dropout}(O_{left} \odot Gate_{left}) \end{aligned}$$

------

### 2. åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (Chunked Triangle Attention)

è¯¥æ¨¡å—å¯¹åº”ç±» `ChunkedTriangleAttention`ã€‚å®ƒè§£å†³äº†æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶ä¸­ $O(L^2)$ çš„æ˜¾å­˜å ç”¨é—®é¢˜ã€‚

#### 2.1 ç‰¹å¾èšåˆ (Feature Aggregation)

ç”±äºè¾“å…¥æ˜¯å› å­åŒ–çš„å½¢å¼ï¼Œé¦–å…ˆå°†å…¶èšåˆä¸ºä¼ªé…å¯¹ç‰¹å¾ï¼ˆPseudo-pair featuresï¼‰ä»¥è®¡ç®— Query, Key, Valueï¼š

$$Z_{pair} = \text{LN}\left(\sum_{r=1}^{R} Z_{left, r} + \sum_{r=1}^{R} Z_{right, r}\right)$$

å…¶ä¸­ $Z_{pair} \in \mathbb{R}^{L \times C}$ã€‚

#### 2.2 Q, K, V æŠ•å½±

$$\begin{aligned} Q &= Z_{pair} W_Q \\ K &= Z_{pair} W_K \\ V &= Z_{pair} W_V \end{aligned}$$

è¿™ä¸‰ä¸ªçŸ©é˜µè¢«é‡å¡‘ä¸º $\mathbb{R}^{L \times H \times d_h}$ã€‚

#### 2.3 åç½®è®¡ç®— (Bias Calculation)

ä¸‰è§’æ³¨æ„åŠ›é€šå¸¸åŒ…å«ä»é…å¯¹è¡¨ç¤ºä¸­å¯¼å‡ºçš„åç½®é¡¹ $b_{ij}$ã€‚ä»£ç ä¸­é€šè¿‡å·¦å› å­å¯¼å‡ºï¼š

$$\text{BiasFeatures} = \sum_{r=1}^{R} Z_{left, r}$$

$$B_{bias} = \text{Linear}_{bias}(\text{BiasFeatures})$$

$B_{bias}$ å¹¿æ’­åå½¢çŠ¶ä¸º $\mathbb{R}^{H \times L \times L}$ã€‚

#### 2.4 åˆ†å—æ³¨æ„åŠ›æœºåˆ¶ (Chunked Attention Mechanism)

ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæ³¨æ„åŠ›åˆ†æ•°ä¸ä¸€æ¬¡æ€§è®¡ç®—å®Œæ•´çš„ $L \times L$ çŸ©é˜µï¼Œè€Œæ˜¯æ²¿ Query çš„åºåˆ—ç»´åº¦åˆ‡åˆ†ä¸ºå¤§å°ä¸º $S$ ($chunk\_size$) çš„å—ã€‚

å¯¹äºç¬¬ $m$ ä¸ªå—ï¼ˆç´¢å¼•èŒƒå›´ $[i, i+S]$ï¼‰ï¼š

$$\begin{aligned} \text{Scores}^{(m)} &= \frac{Q_{[i:i+S]} K^T}{\sqrt{d_h}} + B_{bias[i:i+S]} \\ \text{Attn}^{(m)} &= \text{Softmax}(\text{Scores}^{(m)}) \\ O^{(m)} &= \text{Attn}^{(m)} V \end{aligned}$$

æœ€ç»ˆå°†æ‰€æœ‰å—çš„è¾“å‡ºæ‹¼æ¥ï¼š

$$O = \text{Concat}(O^{(1)}, O^{(2)}, \dots)$$

#### 2.5 è¾“å‡ºåˆ†å‘

æ³¨æ„åŠ›è¾“å‡ºç»è¿‡æŠ•å½±å’Œé—¨æ§åï¼Œé‡æ–°åˆ†é…å›å› å­åŒ–è¡¨ç¤ºï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå‡åˆ†ç»™ç§©ç»´åº¦ï¼‰ï¼š



$$\begin{aligned} Y &= \text{Linear}_{out}(O) \odot \sigma(\text{Linear}_{gate}(Z_{pair})) \\ Z_{left}^{new} &= \frac{1}{R} \cdot Y \quad (\text{Broadcast over rank}) \end{aligned}$$


### 2. Factorized Pair Transform Network 

 `factorized_pair_transform.py` å®ç°äº†ä¸€ä¸ª**å› å­åŒ–é…å¯¹å˜æ¢ç½‘ç»œ (Factorized Pair Transform Network)** ï¼Œä»¥ä¸‹æ˜¯**å› å­åŒ–é…å¯¹å˜æ¢ç½‘ç»œ (Factorized Pair Transform Network)** çš„æ•°å­¦é€»è¾‘ã€‚

è¯¥æ¨¡å—çš„ä¸»è¦åŠŸèƒ½æ˜¯å°† AlphaFold2 ä¸­çš„ `PairTransformNet`ï¼ˆåŒ…å«ä¸‰è§’æ›´æ–°å’Œä¸‰è§’æ³¨æ„åŠ›ï¼‰é€‚é…ä¸ºå› å­åŒ–å½¢å¼ã€‚æ ¸å¿ƒè®¾è®¡æ¨¡å¼æ˜¯**â€œè®¡ç®—å› å­æ›´æ–° $\to$ èšåˆä¸ºå…¨å±€æ›´æ–° $\to$ å‡åŒ€åˆ†é…å›å› å­â€**ï¼Œè¿™ç§æœºåˆ¶åœ¨ä¿æŒä½ç§©ç»“æ„çš„åŒæ—¶ï¼Œä¿ƒè¿›äº†ä¸åŒç§©ä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€‚

------

### 1. æ€»ä½“æ¶æ„ (General Architecture)

ç½‘ç»œç”± $N$ ä¸ªå †å çš„ **å› å­åŒ–é…å¯¹å˜æ¢å±‚ (FactorizedPairTransformLayer)** ç»„æˆã€‚

**è¾“å…¥/è¾“å‡ºçŠ¶æ€**:

- $\mathbf{F}_L^{(l)} \in \mathbb{R}^{L \times R \times C}$: ç¬¬ $l$ å±‚çš„å·¦å› å­å¼ é‡ã€‚
- $\mathbf{F}_R^{(l)} \in \mathbb{R}^{L \times R \times C}$: ç¬¬ $l$ å±‚çš„å³å› å­å¼ é‡ã€‚
- $R$: å› å­åŒ–ç§© (Rank)ã€‚

æ¯ä¸€å±‚çš„å¤„ç†æµç¨‹åŒ…å« 5 ä¸ªé¡ºåºå­æ¨¡å—ï¼š

1. å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (å‡ºè¾¹)
2. å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (å…¥è¾¹)
3. åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (èµ·å§‹èŠ‚ç‚¹)
4. åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (ç»“æŸèŠ‚ç‚¹)
5. é…å¯¹è¿‡æ¸¡ (Pair Transition)

------

### 2. é€šç”¨æ›´æ–°æœºåˆ¶ (The Update-Aggregate-Distribute Mechanism)

ä»£ç ä¸­æ‰€æœ‰å­æ¨¡å—ï¼ˆé™¤äº† Pair Transitionï¼‰éƒ½éµå¾ªç›¸åŒçš„æ®‹å·®è¿æ¥æ¨¡å¼ã€‚ä¸ºäº†æ•°å­¦ä¸Šçš„ç®€æ´ï¼Œå®šä¹‰ä¸€ä¸ªé€šç”¨ç®—å­ $\text{ApplyBlock}$ï¼š

**è®¾å­æ¨¡å—æ“ä½œä¸º $\Phi(\mathbf{F}_L, \mathbf{F}_R)$ï¼Œè¿”å›æ›´æ–°é‡ $(\Delta \mathbf{F}_L, \Delta \mathbf{F}_R)$ã€‚**

**èšåˆä¸åˆ†é…æ­¥éª¤**:

1. **æ‰§è¡Œæ“ä½œ**:

   $$(\Delta \mathbf{F}_L, \Delta \mathbf{F}_R) = \Phi(\mathbf{F}_L, \mathbf{F}_R)$$

2. ç§©èšåˆ (Rank Aggregation):

   å°†å·¦ã€å³å› å­çš„æ›´æ–°é‡æ²¿ç§©ç»´åº¦æ±‚å’Œï¼Œå¾—åˆ°å…¨å±€åºåˆ—æ›´æ–°é‡ $\Delta \mathbf{z} \in \mathbb{R}^{L \times C}$ï¼š

   $$\Delta \mathbf{z} = \sum_{r=1}^{R} \Delta \mathbf{F}_{L, \cdot, r, \cdot} + \sum_{r=1}^{R} \Delta \mathbf{F}_{R, \cdot, r, \cdot}$$

3. **Dropout**:

   $$\Delta \mathbf{z}' = \text{Dropout}(\Delta \mathbf{z})$$

4. å‡åŒ€åˆ†é…æ®‹å·® (Distribute Residual):

   å°†èšåˆåçš„æ›´æ–°é‡å‡åŒ€åˆ†é…å›æ‰€æœ‰ç§©ï¼Œå®ç°ä¿¡æ¯åŒæ­¥ï¼š

   $$\mathbf{F}_{L, \cdot, r, \cdot} \leftarrow \mathbf{F}_{L, \cdot, r, \cdot} + \frac{1}{R} \Delta \mathbf{z}'$$

   $$\mathbf{F}_{R, \cdot, r, \cdot} \leftarrow \mathbf{F}_{R, \cdot, r, \cdot} + \frac{1}{R} \Delta \mathbf{z}'$$

------

### 3. å­æ¨¡å—é€»è¾‘è¯¦è§£

#### 3.1 å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (Factorized Triangle Multiplicative Update)

åˆ©ç”¨å‰ä¸€ä¸ªæ–‡ä»¶å®šä¹‰çš„ `FactorizedTriangleMultiplication` æ¨¡å—ã€‚

**å‡ºè¾¹æ›´æ–° (Outgoing):**

$$\Phi_{\text{mul\_out}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

- æ³¨ï¼šåœ¨ `FactorizedTriangleMultiplicationOutgoing` ä¸­ï¼Œé€šå¸¸ $\Delta \mathbf{F}_R = \mathbf{0}$ï¼Œå› æ­¤èšåˆæ­¥éª¤ä¸»è¦æ”¶é›† $\Delta \mathbf{F}_L$ çš„ä¿¡æ¯ã€‚

**å…¥è¾¹æ›´æ–° (Incoming):**

$$\Phi_{\text{mul\_in}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

#### 3.2 åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (Chunked Triangle Attention)

åˆ©ç”¨å‰ä¸€ä¸ªæ–‡ä»¶å®šä¹‰çš„ `ChunkedTriangleAttention` æ¨¡å—ã€‚

**èµ·å§‹èŠ‚ç‚¹æ³¨æ„åŠ› (Starting Node):**

$$\Phi_{\text{att\_start}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

- æ­¤å¤„ä½¿ç”¨ **Row-wise Dropout**ã€‚

**ç»“æŸèŠ‚ç‚¹æ³¨æ„åŠ› (Ending Node):**

$$\Phi_{\text{att\_end}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

- æ­¤å¤„ä½¿ç”¨ **Column-wise Dropout**ã€‚

#### 3.3 å› å­åŒ–é…å¯¹è¿‡æ¸¡ (Factorized Pair Transition)

æ ‡å‡†çš„ Pair Transition æ˜¯ä¸€ä¸ª MLPï¼Œä½œç”¨äº $L \times L \times C$ å¼ é‡çš„æ¯ä¸ªå…ƒç´ ã€‚ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæ­¤å®ç°å¯¹èšåˆåçš„ $L \times C$ ç‰¹å¾è¿›è¡Œæ“ä½œã€‚

1. **ç‰¹å¾èšåˆ**:

   $$\mathbf{z}_{\text{agg}} = \sum_{r=1}^R \mathbf{F}_{L, \cdot, r, \cdot} + \sum_{r=1}^R \mathbf{F}_{R, \cdot, r, \cdot}$$

2. è¿‡æ¸¡å±‚ (Transition):

   åº”ç”¨ä¸¤å±‚ MLP (Linear $\to$ ReLU $\to$ Linear)ï¼š

   $$\mathbf{z}_{\text{trans}} = \text{Linear}_2(\text{ReLU}(\text{Linear}_1(\text{LayerNorm}(\mathbf{z}_{\text{agg}}))))$$

   *æ³¨ï¼šä»£ç å¤ç”¨äº†æ ‡å‡†çš„ `PairTransition` æ¨¡å—ï¼Œä½†è¾“å…¥è¢« reshape ä¸ºæ¨¡æ‹Ÿå½¢çŠ¶ä»¥é€‚åº”æ¥å£ï¼Œå®é™…è®¡ç®—æ˜¯é€ä½ç½® (position-wise) çš„ã€‚*

3. **æ®‹å·®åˆ†é…**:

   $$\mathbf{F}_{L, \cdot, r, \cdot} \leftarrow \mathbf{F}_{L, \cdot, r, \cdot} + \frac{1}{R} \mathbf{z}_{\text{trans}}$$

   $$\mathbf{F}_{R, \cdot, r, \cdot} \leftarrow \mathbf{F}_{R, \cdot, r, \cdot} + \frac{1}{R} \mathbf{z}_{\text{trans}}$$

### 4. æ©ç å¤„ç† (Masking)

åœ¨æ¯ä¸€å±‚çš„æœ€åï¼Œåº”ç”¨åºåˆ—æ©ç ä»¥ç¡®ä¿æ— æ•ˆä½ç½®ä¿æŒä¸º 0ï¼š

$$\mathbf{F}_{L, i, r} \leftarrow \mathbf{F}_{L, i, r} \cdot m_i$$

$$\mathbf{F}_{R, i, r} \leftarrow \mathbf{F}_{R, i, r} \cdot m_i$$

å…¶ä¸­ $m_i \in \{0, 1\}$ æ˜¯åºåˆ—æ©ç ã€‚

### æ€»ç»“ï¼šå†…å­˜å¤æ‚åº¦å¯¹æ¯”

å¯¹äºå±‚æ•° $N$ï¼Œåºåˆ—é•¿åº¦ $L$ï¼Œé€šé“æ•° $C$ï¼Œç§© $R$ï¼š

- **æ ‡å‡† PairTransform**: $O(N \cdot L^2 \cdot C)$
- **å› å­åŒ– PairTransform**: $O(N \cdot L \cdot R \cdot C)$

## Stage 3 æ›´æ–° (2026-01-12)

### è®­ç»ƒæ•ˆç‡ä¼˜åŒ–

Stage 3 å®ç°äº†å®Œæ•´çš„è®­ç»ƒä¼˜åŒ–æµç¨‹ï¼Œå¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦å’Œç¨³å®šæ€§ã€‚

 `progressive_training.py`æ¨¡å—ä¸»è¦å®ç°äº†ä¸¤ä¸ªå…³é”®çš„è®­ç»ƒä¼˜åŒ–æœºåˆ¶ï¼š**æ¸è¿›å¼è®­ç»ƒè°ƒåº¦ï¼ˆCurriculum Learningï¼‰ï¼Œåˆ†å—æŸå¤±è®¡ç®—ï¼ˆChunked Loss Computationï¼‰**ã€‚

ä»¥ä¸‹æ˜¯å„ä¸ªæ¨¡å—å¯¹åº”çš„æ•°å­¦å…¬å¼è¯´æ˜ã€‚

### ç¬¦å·å®šä¹‰

- $t$: å½“å‰è®­ç»ƒæ­¥æ•° (Current Step)
- $L_{curr}$: å½“å‰è®­ç»ƒä½¿ç”¨çš„åºåˆ—é•¿åº¦
- $L_{min}, L_{max}$: æœ€å°å’Œæœ€å¤§åºåˆ—é•¿åº¦
- $T_{warmup}$: é¢„çƒ­æ­¥æ•°
- $T_{growth}$: å¢é•¿é˜¶æ®µæ­¥æ•°
- $\mathbf{x}_i, \hat{\mathbf{x}}_i \in \mathbb{R}^3$: ç¬¬ $i$ ä¸ªæ®‹åŸºçš„çœŸå®åæ ‡å’Œé¢„æµ‹åæ ‡
- $M_{ij} \in \{0, 1\}$: æ©ç çŸ©é˜µï¼Œå½“æ®‹åŸº $i$ å’Œ $j$ éƒ½å­˜åœ¨æ—¶ä¸º 1

------

### 1. æ¸è¿›å¼è®­ç»ƒè°ƒåº¦å™¨ (Progressive Training Scheduler)

è¯¥æ¨¡å—é€šè¿‡è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰åŠ¨æ€è°ƒæ•´åºåˆ—è£å‰ªé•¿åº¦ï¼Œä½¿æ¨¡å‹å…ˆå­¦ä¹ çŸ­åºåˆ—çš„å±€éƒ¨ç‰¹å¾ï¼Œå†é€æ­¥è¿‡æ¸¡åˆ°é•¿åºåˆ—çš„å…¨å±€ç‰¹å¾ã€‚

#### 1.1 è¿›åº¦è®¡ç®— (Progress Calculation)

å®šä¹‰å½’ä¸€åŒ–çš„å¢é•¿è¿›åº¦ $p \in [0, 1]$ï¼š

$$p = \text{clamp}\left(\frac{t - T_{warmup}}{T_{growth}}, 0, 1\right)$$

#### 1.2 å¢é•¿ç­–ç•¥ (Growth Schedule)

å¯¹åº” `growth_schedule` å‚æ•°ï¼Œæ’å€¼ç³»æ•° $\alpha$ çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

- çº¿æ€§ (Linear):

  $$\alpha = p$$

- ä½™å¼¦ (Cosine):

  $$\alpha = \frac{1 - \cos(\pi \cdot p)}{2}$$

- æŒ‡æ•° (Exponential):

  $$\alpha = p^2$$

#### 1.3 å½“å‰åºåˆ—é•¿åº¦ (Current Max Length)

$$L_{curr}(t) = \begin{cases} L_{min} & \text{if } t < T_{warmup} \\ \lfloor L_{min} + \alpha \cdot (L_{max} - L_{min}) \rfloor & \text{if } T_{warmup} \le t < T_{total} \\ L_{max} & \text{otherwise} \end{cases}$$

------

### 2. åˆ†å—æŸå¤±è®¡ç®— (Chunked Loss Computation)

ä¸ºäº†é¿å…æ„å»ºå½¢çŠ¶ä¸º $B \times L \times L$ çš„å®Œæ•´è·ç¦»çŸ©é˜µï¼ˆæ˜¾å­˜æ¶ˆè€— $O(L^2)$ï¼‰ï¼Œä»£ç å°† $L$ ç»´åº¦åˆ‡åˆ†ä¸ºå¤§å°ä¸º $S$ çš„å—ï¼ˆChunkï¼‰ï¼Œå°†ç©ºé—´å¤æ‚åº¦é™ä½ä¸º $O(S \cdot L)$ã€‚

#### 2.1 è·ç¦»è®¡ç®—åŸºç¡€

å¯¹äºä»»æ„ä¸¤ç‚¹ $i, j$ï¼Œå…¶æ¬§å‡ é‡Œå¾—è·ç¦»ä¸ºï¼š

$$d_{ij} = \|\mathbf{x}_i - \mathbf{x}_j\|_2, \quad \hat{d}_{ij} = \|\hat{\mathbf{x}}_i - \hat{\mathbf{x}}_j\|_2$$

åœ¨åˆ†å—æ¨¡å¼ä¸‹ï¼Œå¤–å±‚å¾ªç¯éå†å—ç´¢å¼• $k$ï¼Œå†…å±‚è®¡ç®—å—å†…æ®‹åŸº $i \in [kS, (k+1)S]$ ä¸å…¨åºåˆ—æ®‹åŸº $j \in [1, L]$ çš„è·ç¦»ã€‚

#### 2.2 æŸå¤± 

> [!WARNING]
>
> *è™½ç„¶ä»£ç å‡½æ•°åä¸º `compute_fape_loss_chunked`ï¼Œä½†å…¶å®æˆ‘ç°åœ¨çš„é€»è¾‘ä»ç„¶æ˜¯åŸºäº**æ ‡é‡è·ç¦»çŸ©é˜µçš„è¯¯å·®**ï¼ˆDistance Deviationï¼‰ï¼Œè€Œé AlphaFold2 åŸæ–‡ä¸­åŸºäºå±€éƒ¨åæ ‡ç³»çš„ Frame Aligned Point Errorï¼ˆ**å¯èƒ½æœªæ¥ä¼šè€ƒè™‘å¼•å…¥**ï¼‰ã€‚ä»¥ä¸‹å…¬å¼å¯¹åº”ä»£ç çš„å®é™…é€»è¾‘ã€‚*

å¯¹äºæ¯ä¸ªå—ï¼Œè®¡ç®—è·ç¦»è¯¯å·®å¹¶è¿›è¡Œæˆªæ–­ï¼ˆClampingï¼‰ï¼š

$$E_{ij} = \min\left( \left| \hat{d}_{ij} - d_{ij} \right|, \tau \right)$$

å…¶ä¸­ $\tau$ æ˜¯æˆªæ–­é˜ˆå€¼ (clamp_distanceï¼Œé»˜è®¤ 10.0)ã€‚

æ€»æŸå¤±ä¸ºåŠ æƒå¹³å‡ï¼š

$$\mathcal{L}_{dist} = \frac{\sum_{k} \sum_{i \in \text{chunk}_k} \sum_{j=1}^L M_{ij} E_{ij}}{\sum_{i,j} M_{ij} + \epsilon}$$

#### 2.3 dRMSD æŸå¤± (åˆ†å—ç‰ˆ)

è·ç¦»å‡æ–¹æ ¹åå·®ï¼ˆDistance RMSDï¼‰ç”¨äºè¡¡é‡å†…éƒ¨å‡ ä½•ç»“æ„çš„ç›¸ä¼¼æ€§ï¼Œä¸ä¾èµ–äºå…¨å±€å åŠ ã€‚

$$SquaredError_{ij} = (\hat{d}_{ij} - d_{ij})^2$$

åˆ†å—ç´¯ç§¯è®¡ç®—åï¼š

$$\mathcal{L}_{dRMSD} = \sqrt{ \frac{\sum_{k} \sum_{i \in \text{chunk}_k} \sum_{j=1}^L M_{ij} (\hat{d}_{ij} - d_{ij})^2}{\sum_{i,j} M_{ij} + \epsilon} }$$

### 3. å†…å­˜ä¸æ•ˆç‡åˆ†æ

é€šè¿‡åˆ†å—ï¼Œè·ç¦»çŸ©é˜µçš„æ˜¾å­˜å ç”¨ä»äºŒæ¬¡æ–¹é™ä½ä¸ºçº¿æ€§ï¼š

$$Memory_{standard} \propto B \cdot L^2$$

$$Memory_{chunked} \propto B \cdot S \cdot L$$

å½“ $L=1024, S=64$ æ—¶ï¼Œå†…å­˜å ç”¨å‡å°‘çº¦ **16 å€**ï¼Œè¿™å…è®¸åœ¨æœ‰é™æ˜¾å­˜çš„ GPU ä¸Šè®­ç»ƒæ›´é•¿çš„è›‹ç™½è´¨åºåˆ—ã€‚

## Stage 3 V2 æ›´æ–° (2026-01-13)

Stage 3 V2 é€šè¿‡Sparse k-NN Pairsï¼Œæ”¯æŒæ›´é•¿åºåˆ—

 `sparse_pairs.py`æ¨¡å—å®ç°äº†ä¸€ç§**ç¨€ç– $k$-æœ€è¿‘é‚» ($k$-NN) é…å¯¹é€‰æ‹©æœºåˆ¶**ã€‚å…¶æ ¸å¿ƒç›®çš„æ˜¯å°†é•¿åºåˆ—è›‹ç™½è´¨å»ºæ¨¡ä¸­çš„é…å¯¹ç‰¹å¾ä»ç¨ å¯†çš„ $O(L^2)$ é™ä½åˆ°ç¨€ç–çš„ $O(L \cdot k)$ã€‚

ä»¥ä¸‹æ˜¯å„ä¸ªé€‰æ‹©ç­–ç•¥çš„æ•°å­¦é€»è¾‘ï¼š

### ç¬¦å·å®šä¹‰

- $L$: åºåˆ—é•¿åº¦
- $k$: æ¯ä¸ªæ®‹åŸºé€‰æ‹©çš„é‚»å±…æ•°é‡
- $\mathbf{x}_i \in \mathbb{R}^3$: ç¬¬ $i$ ä¸ªæ®‹åŸºçš„ç©ºé—´åæ ‡ (é€šå¸¸æ˜¯ $C_\alpha$ æˆ– $C_\beta$)
- $\mathcal{N}_i$: ç¬¬ $i$ ä¸ªæ®‹åŸºçš„é‚»å±…ç´¢å¼•é›†åˆ
- $w$: å±€éƒ¨çª—å£å¤§å° (`local_window`)

------

### 1. åŸºäºåæ ‡çš„é€‰æ‹© (Coordinate-based Selection)

è¿™æ˜¯é€šè¿‡è®¡ç®— 3D ç©ºé—´ä¸­çš„æ¬§å‡ é‡Œå¾—è·ç¦»æ¥å¯»æ‰¾æœ€è¿‘çš„é‚»å±…ã€‚

#### 1.1 è·ç¦»çŸ©é˜µè®¡ç®—

é¦–å…ˆè®¡ç®—æ‰€æœ‰æ®‹åŸºå¯¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦» $D \in \mathbb{R}^{L \times L}$ï¼š

$$d_{ij} = \|\mathbf{x}_i - \mathbf{x}_j\|_2 = \sqrt{\sum_{d=1}^3 (x_{i,d} - x_{j,d})^2}$$

#### 1.2 æ©ç å¤„ç† (Masking)

ä¸ºäº†å¤„ç† padding æˆ–ç¼ºå¤±çš„æ®‹åŸºï¼Œåº”ç”¨æ©ç  $M \in \{0, 1\}^L$ï¼š

$$d'_{ij} = \begin{cases} d_{ij} & \text{if } M_i \cdot M_j = 1 \\ \infty & \text{if } M_i \cdot M_j = 0 \end{cases}$$

#### 1.3 Top-k é€‰æ‹©

å¯¹äºæ¯ä¸ªæ®‹åŸº $i$ï¼Œé€‰æ‹©è·ç¦»æœ€å°çš„ $k$ ä¸ªç´¢å¼•ï¼š

$$\mathcal{N}_i^{coord} = \underset{j}{\text{argtopk}}(d'_{i, :}, k, \text{largest=False})$$

è¿™å¯¹åº”ä»£ç ä¸­çš„ `torch.topk(dist, k, largest=False)`ã€‚

------

### 2. åŸºäºåºåˆ—çš„é€‰æ‹© (Sequence-based Selection)

è¿™ç§æ–¹æ³•ä»…åŸºäºæ®‹åŸºåœ¨æ°¨åŸºé…¸åºåˆ—ä¸­çš„ç´¢å¼•è·ç¦» $|i-j|$ æ¥é€‰æ‹©é‚»å±…ï¼Œæ•æ‰ä¸€çº§åºåˆ—ä¸Šçš„å±€éƒ¨æ€§ã€‚

å¯¹äºæ®‹åŸº $i$ï¼Œé€‰æ‹©å…¶å‰ $k/2$ ä¸ªå’Œå $k/2$ ä¸ªæ®‹åŸºï¼š

$$\mathcal{N}_i^{seq} = \{j \mid \max(0, i - \lfloor \frac{k}{2} \rfloor) \le j \le \min(L-1, i + \lfloor \frac{k}{2} \rfloor), j \neq i \}$$

å¦‚æœè¾¹ç•Œå¤„çš„é‚»å±…æ•°é‡ä¸è¶³ $k$ï¼Œä»£ç é€»è¾‘ä¼šç”¨ $i$ è‡ªèº«è¿›è¡Œå¡«å……ä»¥ä¿æŒå¼ é‡å½¢çŠ¶å›ºå®šã€‚

------

### 3. æ··åˆç­–ç•¥ (Hybrid Strategy)

æ··åˆç­–ç•¥ç»“åˆäº†ç©ºé—´å‡ ä½•ä¿¡æ¯å’Œåºåˆ—å±€éƒ¨ä¿¡æ¯ã€‚å®ƒå°† $k$ åˆ†é…ä¸ºä¸¤éƒ¨åˆ†ï¼š$k_{coord} = \lfloor k/2 \rfloor$ å’Œ $k_{seq} = k - k_{coord}$ã€‚

$$\mathcal{N}_i^{hybrid} = \mathcal{N}_i^{coord}(k_{coord}) \cup \mathcal{N}_i^{seq}(k_{seq})$$

æœ€ç»ˆçš„é‚»å±…å¼ é‡æ˜¯ä¸¤è€…çš„æ‹¼æ¥ï¼ˆConcatenationï¼‰ï¼š

$$\text{Indices}_{hybrid} = \text{Concat}(\text{Indices}_{coord}, \text{Indices}_{seq})$$

------

### 4. å¼ºåˆ¶å±€éƒ¨é…å¯¹ (Mandatory Local Pairs)

ä¸ºäº†ä¿è¯æ¨¡å‹å§‹ç»ˆèƒ½çœ‹åˆ°å±€éƒ¨çš„äºŒçº§ç»“æ„ä¿¡æ¯ï¼Œä»£ç æä¾›äº†ä¸€ä¸ªé€‰é¡¹ `include_all_local`ï¼Œå¼ºåˆ¶åŒ…å«çª—å£ $w$ å†…çš„æ‰€æœ‰æ®‹åŸºã€‚

å±€éƒ¨ç´¢å¼•é›†åˆï¼š

$$\mathcal{N}_i^{local} = \{j \mid |i - j| \le w \}$$

æœ€ç»ˆé›†åˆä¸º $k$-NN é›†åˆä¸å±€éƒ¨é›†åˆçš„å¹¶é›†ï¼š

$$\mathcal{N}_i^{final} = \mathcal{N}_i^{knn} \cup \mathcal{N}_i^{local}$$

ç”±äºå¹¶é›†æ“ä½œä¼šå¯¼è‡´æ¯ä¸ªæ®‹åŸºçš„é‚»å±…æ•°é‡ä¸ä¸€è‡´ï¼Œä»£ç å®ç°ä¸­é€šå¸¸ä¼šå–æœ€å¤§é•¿åº¦å¹¶è¿›è¡Œ Paddingã€‚

------

### 5. å¤æ‚åº¦ä¸å†…å­˜åˆ†æ

é€šè¿‡è¿™ç§ç¨€ç–åŒ–ï¼Œå†…å­˜æ¶ˆè€—ä»åºåˆ—é•¿åº¦çš„å¹³æ–¹çº§é™ä½åˆ°çº¿æ€§çº§ï¼š

- å¯†é›†é…å¯¹ (Dense):

  $$\text{Memory} \propto O(L^2 \cdot C)$$

  å¯¹äº $L=4096$ï¼Œè¿™æ˜¯ä¸å¯è¡Œçš„ã€‚

- ç¨€ç–é…å¯¹ (Sparse):

  $$\text{Memory} \propto O(L \cdot k \cdot C)$$

  å¯¹äº $L=4096, k=32$ï¼Œå†…å­˜å‡å°‘çº¦ 120å€ã€‚

## Stage 4 æ›´æ–° (2026-01-14)

Stage 4 å®ç°äº†è®¡ç®—æ•ˆç‡ä¼˜åŒ–å’Œæ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚

### 1. Axial Attention 

 `axial_attention.py`ï¼Œæ¨¡å—å®ç°äº†**è½´å‘æ³¨æ„åŠ› (Axial Attention)**ï¼Œæ—¨åœ¨å°† AlphaFold2 ä¸­è®¡ç®—å¤æ‚åº¦ä¸º $O(L^3)$ çš„ä¸‰è§’æ³¨æ„åŠ›é™ä½ä¸º $O(L^2)$ã€‚æ­¤å¤–ï¼Œè¿˜å®ç°äº†ä¸€ä¸ªç»“åˆäº†ä½ç§©å› å­åŒ–çš„ç‰ˆæœ¬ã€‚

ä»¥ä¸‹æ˜¯æ ¸å¿ƒæ¨¡å—çš„æ•°å­¦å…¬å¼è¯´æ˜ã€‚

#### ç¬¦å·å®šä¹‰

- $X \in \mathbb{R}^{B \times L \times L \times C}$: è¾“å…¥çš„é…å¯¹å¼ é‡ (Batch, Row, Column, Channel)
- $L$: åºåˆ—é•¿åº¦
- $C$: é€šé“ç»´åº¦
- $H$: æ³¨æ„åŠ›å¤´æ•°
- $\mathcal{A}$: æ³¨æ„åŠ›å‡½æ•° (Attention)
- $\sigma$: Sigmoid æ¿€æ´»å‡½æ•°

------

#### 1. è½´å‘æ³¨æ„åŠ› (Axial Attention)

è½´å‘æ³¨æ„åŠ›å°†å…¨äºŒç»´æ³¨æ„åŠ›åˆ†è§£ä¸ºä¸¤ä¸ªé¡ºåºæ‰§è¡Œçš„ä¸€ç»´æ³¨æ„åŠ›ï¼š**è¡Œæ³¨æ„åŠ› (Row Attention)** å’Œ **åˆ—æ³¨æ„åŠ› (Column Attention)**ã€‚

##### 1.1 æ³¨æ„åŠ›é€šç”¨å½¢å¼

å¯¹äºè¾“å…¥åºåˆ— $Y$ï¼ˆå½¢çŠ¶ä¸º $N \times S \times C$ï¼‰ï¼Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—å¦‚ä¸‹ï¼š

$$\begin{aligned} Q &= Y W_Q, \quad K = Y W_K, \quad V = Y W_V \\ \text{Scores} &= \frac{Q K^T}{\sqrt{d_k}} \\ \text{Attn} &= \text{Softmax}(\text{Scores} + M) \\ O &= \text{Attn} \cdot V \end{aligned}$$

æœ€ç»ˆè¾“å‡ºç»è¿‡çº¿æ€§æŠ•å½±å’Œé—¨æ§ï¼š

$$Y_{out} = (O W_O) \odot \sigma(Y W_G)$$

##### 1.2 è¡Œæ³¨æ„åŠ› (Row-wise Attention)

å¯¹æ¯ä¸€è¡Œ $i$ï¼Œåœ¨æ‰€æœ‰åˆ— $j \in [1, L]$ ä¸Šè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚

å°† $X$ è§†ä¸º $B \cdot L$ ä¸ªé•¿åº¦ä¸º $L$ çš„åºåˆ—ï¼š

$$X_{row\_view} \in \mathbb{R}^{(B \cdot L) \times L \times C}$$

è®¡ç®—ï¼š

$$X^{(1)} = X + \text{Attention}_{row}(X_{row\_view})$$

å…¶ä¸­æ³¨æ„åŠ›å‘ç”Ÿåœ¨ç¬¬ 3 ç»´åº¦ï¼ˆåˆ—ç´¢å¼• $j$ï¼‰ä¸Šã€‚

##### 1.3 åˆ—æ³¨æ„åŠ› (Column-wise Attention)

å¯¹æ¯ä¸€åˆ— $j$ï¼Œåœ¨æ‰€æœ‰è¡Œ $i \in [1, L]$ ä¸Šè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚

é¦–å…ˆè½¬ç½®è¾“å…¥ï¼Œäº¤æ¢è¡Œå’Œåˆ—ç»´åº¦ï¼š

$$X_{transposed} = (X^{(1)})^T \in \mathbb{R}^{B \times L \times L \times C} \quad (\text{dim } 1 \leftrightarrow 2)$$

å°†è½¬ç½®åçš„å¼ é‡è§†ä¸º $B \cdot L$ ä¸ªé•¿åº¦ä¸º $L$ çš„åºåˆ—ï¼š

$$X^{(2)} = X^{(1)} + \left( \text{Attention}_{col}(X_{transposed}) \right)^T$$

å…¶ä¸­æ³¨æ„åŠ›å‘ç”Ÿåœ¨ç¬¬ 2 ç»´åº¦ï¼ˆè¡Œç´¢å¼• $i$ï¼‰ä¸Šã€‚

------

#### 2. å› å­åŒ–è½´å‘æ³¨æ„åŠ› (Factorized Axial Attention)

è¯¥æ¨¡å—å°è¯•åœ¨å› å­åŒ–è¡¨ç¤º $Z_{left}, Z_{right} \in \mathbb{R}^{L \times R \times C}$ ä¸Šåº”ç”¨è½´å‘æ³¨æ„åŠ›ï¼Œé€šè¿‡æ„å»ºâ€œä¼ªé…å¯¹â€æ¥è®¡ç®—äº¤äº’ï¼Œç„¶åæŠ•å½±å›å› å­ã€‚

å¯¹äºæ¯ä¸ªç§© $r \in [1, R]$ï¼š

##### 2.1 ä¼ªé…å¯¹æ„å»º (Pseudo-Pair Construction)

é€šè¿‡å¹¿æ’­åŠ æ³•æ„å»ºä¸´æ—¶çš„ $L \times L$ ç‰¹å¾ï¼ˆä»£ç ä¸­æ˜¾å¼æ‰©å±•äº†ç»´åº¦ï¼Œå®é™…åº”ç”¨ä¸­é€šå¸¸ä½¿ç”¨åˆ†å—ä»¥èŠ‚çœå†…å­˜ï¼Œæ­¤å¤„å±•ç¤ºé€»è¾‘å…¬å¼ï¼‰ï¼š

$$P^{(r)}_{ij} = Z_{left, i, r} + Z_{right, j, r}$$

å…¶ä¸­ $P^{(r)} \in \mathbb{R}^{L \times L \times C}$ã€‚

##### 2.2 è½´å‘æ³¨æ„åŠ›åº”ç”¨

ä»£ç ä¸­ä»…åº”ç”¨äº†è¡Œæ³¨æ„åŠ›ï¼ˆRow Attentionï¼‰ä½œä¸ºæ¼”ç¤ºæˆ–ç‰¹å®šå˜ä½“ï¼š

$$U^{(r)} = \text{Attention}_{row}(P^{(r)})$$

$U^{(r)}$ åŒ…å«äº†æ›´æ–°åçš„é…å¯¹ä¿¡æ¯ã€‚

##### 2.3 æŠ•å½±å›å› å­ (Projection Back to Factors)

é€šè¿‡å¹³å‡æ± åŒ–å°† $L \times L$ ä¿¡æ¯å‹ç¼©å› $L$ ç»´åº¦ï¼š

æ›´æ–°å·¦å› å­ (å¯¹åˆ—æ±‚å‡å€¼):

$$Z'_{left, i, r} = \frac{1}{L} \sum_{j=1}^L U^{(r)}_{ij}$$

æ›´æ–°å³å› å­ (å¯¹è¡Œæ±‚å‡å€¼):

$$Z'_{right, j, r} = \frac{1}{L} \sum_{i=1}^L U^{(r)}_{ij}$$

##### 2.4 æœ€ç»ˆåˆå¹¶

$$Z_{left}^{new} = \text{Concat}([Z'_{left, \cdot, 1}, \dots, Z'_{left, \cdot, R}], \text{dim}=rank)$$

$$Z_{right}^{new} = \text{Concat}([Z'_{right, \cdot, 1}, \dots, Z'_{right, \cdot, R}], \text{dim}=rank)$$

------

#### 3. å¤æ‚åº¦å¯¹æ¯”

| **æ–¹æ³•**             | **æ˜¾å­˜å¤æ‚åº¦**             | **è®¡ç®—å¤æ‚åº¦**                |
| -------------------- | -------------------------- | ----------------------------- |
| **æ ‡å‡†ä¸‰è§’æ³¨æ„åŠ›**   | $O(L^2)$                   | $O(L^3)$                      |
| **è½´å‘æ³¨æ„åŠ›**       | $O(L^2)$ (åˆ†å—å¯è¾¾ $O(L)$) | $O(L^2)$                      |
| **å› å­åŒ–è½´å‘æ³¨æ„åŠ›** | $O(L \cdot R)$             | $O(L^2 \cdot R)$ (å«é‡å»ºè¿‡ç¨‹) |

å¯¹äº $L=2048$ï¼Œè½´å‘æ³¨æ„åŠ›å°†è®¡ç®—é‡å‡å°‘äº†çº¦ 24 å€ã€‚

### 2. Advanced Gradient Checkpointing 

è‡ªé€‚åº”æ¢¯åº¦æ£€æŸ¥ç‚¹ç­–ç•¥
```python
class AdaptiveCheckpointManager:
    """
    æ ¹æ®åºåˆ—é•¿åº¦å’Œå¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´æ£€æŸ¥ç‚¹ç­–ç•¥

    çŸ­åºåˆ— + å……è¶³å†…å­˜: ä¸æ£€æŸ¥ç‚¹ (é€Ÿåº¦ä¼˜å…ˆ)
    ä¸­ç­‰åºåˆ—: é€‰æ‹©æ€§æ£€æŸ¥ç‚¹ (å¹³è¡¡)
    é•¿åºåˆ— + ç´§å¼ å†…å­˜: å…¨æ£€æŸ¥ç‚¹ (å†…å­˜ä¼˜å…ˆ)
    """
    def get_adaptive_config(seq_len, available_memory_gb):
        if seq_len < 256 and available_memory_gb > 10:
            return CheckpointConfig(enabled=False)  # æ— éœ€æ£€æŸ¥ç‚¹
        elif seq_len < 512:
            return CheckpointConfig(
                checkpoint_triangles=True,  # åªæ£€æŸ¥ç‚¹ä¸‰è§’æ“ä½œ
            )
        else:
            return CheckpointConfig(
                checkpoint_structure=True,  # æ£€æŸ¥ç‚¹æ‰€æœ‰
                checkpoint_pairs=True,
                checkpoint_triangles=True,
            )
```

### 3. Model Compression 

å±‚å‚æ•°å…±äº« (Universal Transformer)é£æ ¼

 `model_compression.py`æ¨¡å—å®ç°äº†ä¸€ç³»åˆ—**å‚æ•°é«˜æ•ˆçš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ (Parameter-Efficient Model Compression)**ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**å±‚å…±äº« (Layer Sharing)** å’Œ **ç“¶é¢ˆç»“æ„ (Bottleneck Architectures)** æ¥åœ¨å‡å°‘å‚æ•°é‡çš„åŒæ—¶ä¿æŒç½‘ç»œçš„æ·±åº¦å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

ä»¥ä¸‹æ˜¯å„ä¸ªæ ¸å¿ƒç»„ä»¶çš„æ•°å­¦é€»è¾‘ã€‚

#### ç¬¦å·å®šä¹‰

- $x^{(l)}$: ç¬¬ $l$ å±‚çš„è¾“å…¥å¼ é‡ã€‚
- $f_\theta(\cdot)$: å‚æ•°ä¸º $\theta$ çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆé€šå¸¸åŒ…å« Attention, MLP, Norm ç­‰ï¼‰ã€‚
- $L$: ç½‘ç»œçš„æ€»å±‚æ•°ã€‚
- $C$: è¾“å…¥ç‰¹å¾ç»´åº¦ (Channels)ã€‚
- $C_{bot}$: ç“¶é¢ˆå±‚ç»´åº¦ ($C / r$)ã€‚

------

##### 1. é€šç”¨å±‚å…±äº« (Universal Layer Sharing)

è¯¥ç­–ç•¥å¯¹åº”ç±» `SharedLayerModule`ã€‚è¿™æ˜¯ **Universal Transformer** çš„æ ¸å¿ƒæœºåˆ¶ã€‚ç½‘ç»œåœ¨æ·±åº¦æ–¹å‘ä¸Šåº”ç”¨é€’å½’ï¼ˆRecurrenceï¼‰ï¼Œå³æ¯ä¸€å±‚å¤ç”¨å®Œå…¨ç›¸åŒçš„å‚æ•° $\theta$ã€‚

å¯¹äº $l = 1, \dots, L$ï¼š

$$x^{(l)} = f_{\theta}(x^{(l-1)})$$

- **å‚æ•°å¤æ‚åº¦**: $O(1 \times |\theta|)$ï¼Œä¸ç½‘ç»œæ€»æ·±åº¦ $L$ æ— å…³ã€‚
- **ç‰©ç†æ„ä¹‰**: å¯ä»¥çœ‹ä½œæ˜¯åœ¨æ—¶é—´æ­¥ä¸Šå±•å¼€çš„ RNNï¼Œä½†åœ¨ç©ºé—´ï¼ˆæ·±åº¦ï¼‰ç»´åº¦ä¸Šæ“ä½œï¼Œæ—¨åœ¨å¯»æ‰¾ä¸åŠ¨ç‚¹æˆ–é€æ­¥ç»†åŒ–è¡¨ç¤ºã€‚

------

##### 2. äº¤æ›¿å±‚å…±äº« (Alternating Layer Sharing)

è¯¥ç­–ç•¥å¯¹åº”ç±» `AlternatingSharedLayers`ã€‚è¿™æ˜¯ **ALBERT (A Lite BERT)** ä¸­ä½¿ç”¨çš„ä¸€ç§å˜ä½“ã€‚å®ƒå°†å‚æ•°åˆ†ä¸ºä¸¤ç»„ï¼šå¥‡æ•°å±‚å‚æ•° $\theta_{odd}$ å’Œå¶æ•°å±‚å‚æ•° $\theta_{even}$ã€‚

å¯¹äº $l = 0, \dots, L-1$ï¼š

$$x^{(l+1)} = \begin{cases} f_{\theta_{even}}(x^{(l)}) & \text{if } l \equiv 0 \pmod 2 \\ f_{\theta_{odd}}(x^{(l)}) & \text{if } l \equiv 1 \pmod 2 \end{cases}$$

- **å‚æ•°å¤æ‚åº¦**: $O(2 \times |\theta|)$ã€‚
- **å‹ç¼©ç‡**: ç›¸æ¯”æ ‡å‡†ç½‘ç»œï¼Œå‹ç¼©ç‡çº¦ä¸º $L/2$ã€‚

------

##### 3. åˆ†å—å±‚å…±äº« (Block-wise Layer Sharing)

è¯¥ç­–ç•¥å¯¹åº”ç±» `BlockSharedLayers`ã€‚ç½‘ç»œè¢«åˆ’åˆ†ä¸º $K$ ä¸ªå— (Block)ï¼Œæ¯ä¸ªå—åŒ…å« $M$ ä¸ªå­å±‚ ($L = K \times M$)ã€‚å—ä¸å—ä¹‹é—´å‚æ•°ä¸åŒï¼Œä½†å—å†…çš„ $M$ æ¬¡è¿­ä»£å…±äº«å‚æ•°ã€‚

ä»¤ $\theta_k$ ä¸ºç¬¬ $k$ ä¸ªå—çš„å‚æ•°ã€‚å¯¹äºç¬¬ $k$ ä¸ªå—ä¸­çš„ç¬¬ $m$ æ¬¡è¿­ä»£ï¼š

$$x^{(k, m+1)} = f_{\theta_k}(x^{(k, m)}), \quad m \in [0, M-1]$$

- **å‚æ•°å¤æ‚åº¦**: $O(K \times |\theta|)$ï¼Œå…¶ä¸­ $K \ll L$ã€‚
- å…è®¸ç½‘ç»œåœ¨ä¸åŒé˜¶æ®µï¼ˆå¦‚æµ…å±‚ç‰¹å¾ vs æ·±å±‚è¯­ä¹‰ï¼‰æ‹¥æœ‰ä¸åŒçš„å¤„ç†é€»è¾‘ï¼ŒåŒæ—¶åœ¨å±€éƒ¨ä¿æŒå‚æ•°æ•ˆç‡ã€‚

------

##### 4. ç“¶é¢ˆå±‚ (Bottleneck Layer)

è¯¥ç­–ç•¥å¯¹åº”ç±» `BottleneckLayer`ã€‚ä¸ºäº†å‡å°‘è®¡ç®—é‡å’Œå‚æ•°é‡ï¼Œåœ¨æ‰§è¡Œæ˜‚è´µçš„æ“ä½œï¼ˆå¦‚å…¨è¿æ¥æˆ–æ³¨æ„åŠ›ï¼‰ä¹‹å‰ï¼Œå…ˆå°†ç»´åº¦æŠ•å½±åˆ°ä½ç»´ç©ºé—´ã€‚

è®¾è¾“å…¥ $x \in \mathbb{R}^{d_{in}}$ï¼Œç“¶é¢ˆæ¯”ç‡ä¸º $r$ã€‚

1. é™ç»´ (Project Down):

   $$x_{bot} = x W_{down} + b_{down}, \quad W_{down} \in \mathbb{R}^{d_{in} \times (d_{in}/r)}$$

2. æ ¸å¿ƒæ“ä½œ (Operation):

   $$h = \text{Operation}(x_{bot})$$

   æ³¨æ„ï¼šè¿™é‡Œçš„ Operation æ˜¯åœ¨ä½ç»´ç©ºé—´ $d_{in}/r$ è¿›è¡Œçš„ï¼Œå‚æ•°é‡å¤§å¹…å‡å°‘ã€‚

3. å‡ç»´ (Project Up):

   $$y = h W_{up} + b_{up}, \quad W_{up} \in \mathbb{R}^{(d_{in}/r) \times d_{in}}$$

4. æ®‹å·®ä¸å½’ä¸€åŒ– (Residual & Norm):

   $$Output = \text{LayerNorm}(x + y)$$

------

##### 5. æ·±åº¦å¯åˆ†ç¦»å±‚ (Depthwise Separable Layer)

è¯¥ç­–ç•¥å¯¹åº”ç±» `DepthwiseSeparableLayer`ã€‚å®ƒå°†æ ‡å‡†å·ç§¯åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼Œå¸¸ç”¨äºè½»é‡çº§ç½‘ç»œï¼ˆå¦‚ MobileNetï¼‰ã€‚

å‡è®¾è¾“å…¥ $X \in \mathbb{R}^{C \times L}$ï¼Œå·ç§¯æ ¸å¤§å°ä¸º $K$ã€‚

1. æ·±åº¦å·ç§¯ (Depthwise Conv): å¯¹æ¯ä¸ªé€šé“ç‹¬ç«‹è¿›è¡Œç©ºé—´æ»¤æ³¢ã€‚

   $$Y_{c, i} = \sum_{k} W_{c, k}^{depth} \cdot X_{c, i+k}$$

   å‚æ•°é‡: $C \times K$ã€‚

2. é€ç‚¹å·ç§¯ (Pointwise Conv): ä½¿ç”¨ $1 \times 1$ å·ç§¯æ··åˆé€šé“ä¿¡æ¯ã€‚

   $$Z_{j, i} = \sum_{c} W_{j, c}^{point} \cdot Y_{c, i}$$

   å‚æ•°é‡: $C_{out} \times C_{in} \times 1$ã€‚

**æ€»å‚æ•°é‡å¯¹æ¯”**:

- æ ‡å‡†å·ç§¯: $C_{out} \times C_{in} \times K$
- æ·±åº¦å¯åˆ†ç¦»: $C_{in} \times K + C_{out} \times C_{in}$
- å½“ $K$ è¾ƒå¤§æ—¶ï¼Œå‚æ•°å‡å°‘æ˜¾è‘—ã€‚

------

##### 6. å‚æ•°å‹ç¼©ç‡åˆ†æ

ä»£ç ä¸­çš„ `get_compression_ratio` è®¡ç®—å¦‚ä¸‹ï¼š

$$Ratio = \frac{\text{Baseline Parameters}}{\text{Compressed Parameters}} = \frac{L \times |\theta|_{base}}{|\Theta|_{shared}}$$

ä¾‹å¦‚ï¼Œå¯¹äº $L=12$ å±‚çš„ç½‘ç»œï¼š

- **Universal**: $|\Theta| = 1 \times |\theta| \Rightarrow Ratio = 12\times$
- **Alternating**: $|\Theta| = 2 \times |\theta| \Rightarrow Ratio = 6\times$
- **Block (Size=4)**: $|\Theta| = 3 \times |\theta| \Rightarrow Ratio = 4\times$

è¿™äº›æŠ€æœ¯ä½¿å¾—åœ¨æ˜¾å­˜å—é™çš„æƒ…å†µä¸‹ï¼ˆå¦‚ `model_compression.py` æ³¨é‡Šä¸­æåˆ°çš„ Stage 4ï¼‰ï¼Œå¯ä»¥è®­ç»ƒæ¯”å¸¸è§„ç½‘ç»œæ·±å¾—å¤šçš„æ¨¡å‹ã€‚

## Stage 5 æ›´æ–° (2026-01-15)

### ç³»ç»Ÿçº§ä¼˜åŒ–: åˆ†å¸ƒå¼è®­ç»ƒ

Stage 5 å®ç°äº†åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼Œå…è®¸è·¨å¤šGPUæ‰©å±•ã€‚

#### 1. Distributed Data Parallel (DDP) 

**åŠŸèƒ½**: æ ‡å‡†æ•°æ®å¹¶è¡Œè®­ç»ƒ
```python
class DistributedModelWrapper:
```

#### 2. Sequence Tensor Parallelism 

**åˆ›æ–°**: åœ¨åºåˆ—ç»´åº¦ä¸Šåˆ‡åˆ†
```python
class SequenceTensorParallel:
```

#### 3. Gradient Accumulation 

**åŠŸèƒ½**: å¤§æ‰¹é‡è®­ç»ƒæ”¯æŒ
```python
class GradientAccumulator:
```

**æ•ˆæœ**:

- å¤§æ‰¹é‡è®­ç»ƒæˆä¸ºå¯èƒ½
- æ›´ç¨³å®šçš„æ¢¯åº¦

#### 4. Stage 5 ç»¼åˆæ•ˆæœ

**åˆ†å¸ƒå¼è®­ç»ƒ**:
- å¤šGPU DDP
- Gradient Accumulation: å¤§æ‰¹é‡è®­ç»ƒ
## Stage 6 æ›´æ–°

### 1. mHC Loss

 `mhc_loss.py`æ¨¡å—å®ç°äº†ä¸€å¥—å— mHCï¼ˆMatrix Hyper-Columnï¼‰å¯å‘çš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨ç¨³å®šæ·±åº¦æ®‹å·®ç½‘ç»œä¸­çš„æ¢¯åº¦æµï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é•¿åºåˆ—è›‹ç™½è´¨ç»“æ„çš„ç”Ÿæˆã€‚

ä»¥ä¸‹æ˜¯å„ä¸ªæ ¸å¿ƒæŸå¤±å‡½æ•°çš„æ•°å­¦é€»è¾‘ã€‚

### ç¬¦å·å®šä¹‰

- $\mathbf{x}_{in}, \mathbf{x}_{out} \in \mathbb{R}^{B \times L \times C}$: å±‚çš„è¾“å…¥ä¸è¾“å‡ºå¼ é‡ã€‚
    
- $\mathbf{F}(\mathbf{x}) \in \mathbb{R}^{B \times L \times C}$: æ®‹å·®åˆ†æ”¯çš„è¾“å‡º (å³ $\mathbf{x}_{out} = \mathbf{x}_{in} + \mathbf{F}(\mathbf{x}_{in})$)ã€‚
    
- $\epsilon_{pred}, \epsilon_{target} \in \mathbb{R}^{B \times L \times 3}$: æ‰©æ•£æ¨¡å‹ä¸­çš„é¢„æµ‹å™ªå£°å’Œç›®æ ‡å™ªå£°ã€‚
    
- $M \in \{0, 1\}^{B \times L}$: åºåˆ—æ©ç ã€‚
    
- $\|\cdot\|_2$: Frobenius èŒƒæ•°æˆ– L2 èŒƒæ•°ï¼ˆåœ¨ masked åŒºåŸŸä¸Šè®¡ç®—ï¼‰ã€‚
    

---

#### 1. æ ¸å¿ƒ mHC æ­£åˆ™åŒ–æŸå¤± (Core mHC Regularization)

è¿™äº›æŸå¤±å‡½æ•°æ—¨åœ¨æ¨¡æ‹Ÿ mHC æ¶æ„ä¸­åŒéšæœºçŸ©é˜µï¼ˆDoubly Stochastic Matrixï¼‰å¸¦æ¥çš„è°±åŠå¾„ä¸º 1 çš„ç‰¹æ€§ï¼Œä»è€Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚

##### 1.1 æ®‹å·®å¹³è¡¡æŸå¤± (Residual Balance Loss)

è¯¥å‡½æ•°çº¦æŸæ®‹å·®åˆ†æ”¯ $\mathbf{F}(\mathbf{x})$ ç›¸å¯¹äºä¸»åˆ†æ”¯ $\mathbf{x}_{in}$ çš„è´¡çŒ®æ¯”ä¾‹ã€‚

å®šä¹‰æ®‹å·®æ¯”ç‡ $\rho$ï¼š

$$\rho = \frac{\|\mathbf{F}(\mathbf{x})\|_M^2}{\|\mathbf{x}_{in}\|_M^2 + \|\mathbf{F}(\mathbf{x})\|_M^2 + \delta}$$

æŸå¤±å‡½æ•°ä¸ºï¼š

$$\mathcal{L}_{balance} = (\rho - \gamma)^2$$

å…¶ä¸­ $\gamma$ æ˜¯ç›®æ ‡æ¯”ç‡ (target_ratioï¼Œé€šå¸¸ä¸º 0.5)ï¼Œ$\delta$ æ˜¯æ•°å€¼ç¨³å®šé¡¹ã€‚

##### 1.2 æ¢¯åº¦èŒƒæ•°ä¿æŒæŸå¤± (Gradient Norm Preservation Loss)

åŸºäºåŒéšæœºçŸ©é˜µè°±åŠå¾„ä¸º 1 çš„ç‰¹æ€§ï¼Œå±‚çš„è¾“å…¥è¾“å‡ºèŒƒæ•°åº”ä¿æŒç›¸å¯¹ç¨³å®šã€‚

å®šä¹‰èŒƒæ•°æ¯”ç‡ $r$ï¼š

$$r = \frac{\|\mathbf{x}_{out}\|_M}{\|\mathbf{x}_{in}\|_M + \delta}$$

å¼•å…¥å®¹å·® $\tau$ (ä»£ç ä¸­ä¸º 0.2)ï¼ŒæŸå¤±å‡½æ•°æƒ©ç½šè¶…å‡ºå®¹å·®çš„åå·®ï¼š

$$\mathcal{L}_{norm} = \text{ReLU}(|r - 1| - \tau)^2$$

##### 1.3 åŒéšæœºæƒ©ç½š (Doubly Stochastic Penalty)

å¦‚æœå­˜åœ¨æ˜¾å¼çš„æƒé‡çŸ©é˜µ $W$ï¼Œè¯¥æŸå¤±å¼ºåˆ¶å…¶ç»è¿‡æŒ‡æ•°å˜æ¢åæ¥è¿‘åŒéšæœºçŸ©é˜µï¼ˆè¡Œå’Œä¸åˆ—å’Œå‡ä¸º 1ï¼‰ã€‚

ä»¤ $\tilde{W} = \exp(W)$ï¼š

$$\mathcal{L}_{DS} = \frac{1}{N} \sum_i (\sum_j \tilde{W}_{ij} - 1)^2 + \frac{1}{N} \sum_j (\sum_i \tilde{W}_{ij} - 1)^2$$

---

#### 2. ç©ºé—´ä¸ç¨³å®šæ€§æ­£åˆ™åŒ– (Spatial & Stability Regularization)

##### 2.1 æ¢¯åº¦æµå¹³æ»‘æŸå¤± (Gradient Flow Loss)

çº¦æŸé¢„æµ‹çš„å¹³ç§»å‘é‡æˆ–å™ªå£°åœ¨ç©ºé—´ä¸Šçš„å¹³æ»‘æ€§ã€‚ä»¤æ®‹å·® $R = \mathbf{T}_{pred} - \mathbf{T}_{target}$ï¼š

$$\mathcal{L}_{flow} = \frac{\sum_{i=1}^{L-1} M_i M_{i+1} \|R_{i+1} - R_i\|_2^2}{\sum M_i M_{i+1}}$$

è¿™æƒ©ç½šäº†ç›¸é‚»æ®‹åŸºé¢„æµ‹è¯¯å·®çš„å‰§çƒˆæ³¢åŠ¨ã€‚

##### 2.2 ç‰¹å¾ç¨³å®šæ€§æŸå¤± (Representation Stability Loss)

ç®€å•çš„æ­£åˆ™åŒ–é¡¹ï¼Œé˜²æ­¢æ¿€æ´»å€¼è¿‡å¤§ï¼š

$$\mathcal{L}_{stability} = \frac{1}{N_{mask}} \sum_{b, l, c} (s_{b,l,c} \cdot M_{b,l})^2$$

---

#### 3. é•¿åºåˆ—è‡ªé€‚åº”æŸå¤± (Long Sequence Adaptive Losses)

é’ˆå¯¹é•¿åºåˆ— ($L > 1024$) ç´¯ç§¯è¯¯å·®å¤§ã€æ˜“æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜è®¾è®¡çš„ç‰¹å®šçº¦æŸã€‚

##### 3.1 åºåˆ—é•¿åº¦è‡ªé€‚åº”èŒƒæ•°æŸå¤± (Sequence Length Adaptive Norm Loss)

è¿™ä¸ 1.2 ç±»ä¼¼ï¼Œä½†å®¹å·® $\tau$ å’Œæƒé‡éšåºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´ã€‚

ä»¤é•¿åº¦å› å­ $\lambda = \sqrt{L / L_{base}}$ï¼š

$$\tau_{long} = \max(0.1, \frac{0.2}{\lambda})$$

$$\mathcal{L}_{adaptive} = \lambda \cdot \text{ReLU}\left( \left| \frac{\|\epsilon_{pred}\|}{\|\epsilon_{target}\|} - 1 \right| - \tau_{long} \right)^2$$

##### 3.2 æ¢¯åº¦å¹…åº¦è½¯è£å‰ªæŸå¤± (Gradient Magnitude Clipping Loss)

è½¯çº¦æŸé¢„æµ‹å™ªå£°çš„å¹…åº¦ä¸è¶…è¿‡é˜ˆå€¼ $\tau_{max}$ï¼š

$$\mathcal{L}_{clip} = \frac{1}{N_{mask}} \sum_{i} M_i \cdot \text{ReLU}(\|\epsilon_{pred, i}\|_2 - \tau_{max})^2$$

##### 3.3 å±€éƒ¨ä¸€è‡´æ€§æŸå¤± (Local Consistency Loss)

åœ¨çª—å£ $k$ å†…å¼ºåˆ¶é¢„æµ‹çš„å±€éƒ¨ä¸€è‡´æ€§ï¼Œè·ç¦»è¶Šè¿œæƒé‡è¶Šå°ï¼š

$$\mathcal{L}_{local} = \frac{1}{K} \sum_{k=1}^K \frac{1}{k} \sum_{i} \| \epsilon_{pred, i+k} - \epsilon_{pred, i} \|^2$$

##### 3.4 è°±èŒƒæ•°æ­£åˆ™åŒ– (Spectral Norm Regularization)

çº¦æŸæ•´ä¸ª Batch æˆ–åºåˆ—çš„èƒ½é‡æ¯”ï¼Œæ˜¯èŒƒæ•°ä¿æŒçš„å¦ä¸€ç§å½¢å¼ï¼š

$$r_{energy} = \frac{\sum \|\epsilon_{pred}\|^2}{\sum \|\epsilon_{target}\|^2}$$

$$\mathcal{L}_{spectral} = (r_{energy} - 1)^2$$

---

#### 4. é«˜çº§ mHC æ€»æŸå¤± (Advanced mHC Loss)

åœ¨ `compute_diffusion_loss` ä¸­ï¼Œæ€»æŸå¤±æ˜¯ä¸Šè¿°å„é¡¹çš„åŠ æƒå’Œã€‚å¯¹äºæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨å™ªå£°é¢„æµ‹ $\epsilon_{pred}$ å’Œç›®æ ‡ $\epsilon_{target}$ ä½œä¸ºç½‘ç»œè¾“å…¥è¾“å‡ºçš„ä»£ç†ï¼š

$$\mathcal{L}_{total} = \lambda_1 \mathcal{L}_{base\_reg} + \lambda_2 \mathcal{L}_{balance} + \lambda_3 \mathcal{L}_{norm} + \lambda_4 \mathcal{L}_{flow} + \dots$$

è¿™ç§ç»„åˆæœ‰æ•ˆåœ°åœ¨ä¸å¼•å…¥æ˜¾å¼ mHC æ¶æ„ï¼ˆå¦‚æ˜‚è´µçš„ Sinkhorn è¿­ä»£å±‚ï¼‰çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è½¯çº¦æŸæ¨¡æ‹Ÿäº† mHC çš„æ•°å€¼ç¨³å®šæ€§ä¼˜åŠ¿ã€‚
### 2.**æµå½¢çº¦æŸè¶…è¿æ¥ (mHC)** **é…å¯¹ç‰¹å¾ (Pair Features)** 
åŸºäº `mhc_pair_transform_net.py`ï¼Œè¯¥æ¨¡å—å°† **æµå½¢çº¦æŸè¶…è¿æ¥ (mHC)** æ¶æ„é€‚é…åˆ°äº† **é…å¯¹ç‰¹å¾ (Pair Features)** ä¸Šã€‚ä¸æ ‡å‡†çš„æ®‹å·®è¿æ¥ä¸åŒï¼ŒmHC åœ¨ä¸€ä¸ªæ‰©å±•çš„â€œè¶…ç©ºé—´â€ä¸­ç»´æŠ¤çŠ¶æ€ï¼Œé€šè¿‡åŠ¨æ€æŠ•å½±ä¸æ ‡å‡†å±‚äº¤äº’ã€‚

ä»¥ä¸‹æ˜¯æ ¸å¿ƒæ•°å­¦å…¬å¼è¯´æ˜ã€‚
### ç¬¦å·å®šä¹‰

- $L$: åºåˆ—é•¿åº¦ã€‚
    
- $C$: é…å¯¹ç‰¹å¾é€šé“æ•°ã€‚
    
- $n$: mHC æ‰©å±•ç‡ (`expansion_rate`, é€šå¸¸ä¸º 2 æˆ– 4)ã€‚
    
- $\mathbf{Z} \in \mathbb{R}^{L \times L \times C}$: æ ‡å‡†é…å¯¹å¼ é‡ã€‚
    
- $\mathcal{Z} \in \mathbb{R}^{L \times L \times n \times C}$: **è¶…é…å¯¹å¼ é‡ (Hyper Pair Tensor)**ã€‚
    
    - å¯¹äºæ¯å¯¹æ®‹åŸº $(i, j)$ï¼Œæˆ‘ä»¬ä¸å†åªç»´æŠ¤ä¸€ä¸ªå‘é‡ $z_{ij}$ï¼Œè€Œæ˜¯ç»´æŠ¤ä¸€ä¸ªåŒ…å« $n$ ä¸ªå‘é‡çš„å­ç©ºé—´ $\{z_{ij}^{(1)}, \dots, z_{ij}^{(n)}\}$ã€‚
        
---

#### 1. åŠ¨æ€æ˜ å°„çŸ©é˜µ (Dynamic Mapping Matrices)

å¯¹äºæ¯å¯¹ä½ç½® $(i, j)$ï¼Œç½‘ç»œæ ¹æ®å½“å‰çŠ¶æ€ $\mathcal{Z}_{ij}$ åŠ¨æ€è®¡ç®—ä¸‰ä¸ªçŸ©é˜µã€‚

é¦–å…ˆå°†çŠ¶æ€å±•å¹³å¹¶å½’ä¸€åŒ–ï¼š
$$x_{ij} = \text{Flatten}(\mathcal{Z}_{ij}) \in \mathbb{R}^{nC}$$
$$\bar{x}_{ij} = \text{RMSNorm}(x_{ij})$$
##### 1.1 é¢„æŠ•å½±å‘é‡ (Pre-projection) $H_{pre}$

å°†è¶…ç©ºé—´ $\mathbb{R}^{n \times C}$ å‹ç¼©åˆ°æ ‡å‡†ç©ºé—´ $\mathbb{R}^{C}$ çš„æƒé‡ã€‚

$$H_{pre, ij} = \sigma(\mathbf{W}_{pre} \bar{x}_{ij} + b_{pre}) \in \mathbb{R}^{1 \times n}$$

å…¶ä¸­ $\sigma$ æ˜¯ Sigmoid å‡½æ•°ã€‚

##### 1.2 åæŠ•å½±å‘é‡ (Post-projection) $H_{post}$

å°†æ ‡å‡†ç©ºé—´çš„æ›´æ–°é‡å¹¿æ’­å›è¶…ç©ºé—´çš„æƒé‡ã€‚

$$H_{post, ij} = 2 \cdot \sigma(\mathbf{W}_{post} \bar{x}_{ij} + b_{post}) \in \mathbb{R}^{n \times 1}$$

ç³»æ•° 2 å…è®¸æ¢¯åº¦çš„æ”¾å¤§æˆ–ç¼©å°ã€‚

##### 1.3 æ®‹å·®æ··åˆçŸ©é˜µ (Residual Mixing) $H_{res}$

åœ¨è¶…ç©ºé—´å†…éƒ¨æ··åˆä¿¡æ¯çš„åŒéšæœºçŸ©é˜µ (Doubly Stochastic Matrix)ã€‚

$$A_{ij} = \mathbf{W}_{res} \bar{x}_{ij} + b_{res} \in \mathbb{R}^{n \times n}$$
$$H_{res, ij} = \text{SinkhornKnopp}(A_{ij})$$
Sinkhorn è¿­ä»£ç¡®ä¿ $\sum_k H_{res, ik} = 1$ ä¸” $\sum_k H_{res, kj} = 1$ã€‚è¿™ä¿è¯äº†æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦çš„èŒƒæ•°ä¿æŒä¸å˜ï¼ˆè°±åŠå¾„ä¸º 1ï¼‰ã€‚

---

#### 2. mHC é…å¯¹å˜æ¢å±‚ (mHC Pair Transform Layer)

æ¯ä¸€å±‚çš„æ›´æ–°è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š**æ”¶ç¼© (Contract)** -> **å˜æ¢ (Transform)** -> **æ‰©å±•ä¸æ›´æ–° (Expand & Update)**ã€‚

##### 2.1 æ”¶ç¼© (Contraction)

å°†é«˜ç»´çš„è¶…é…å¯¹ç‰¹å¾æŠ•å½±åˆ°æ ‡å‡†ç©ºé—´ï¼Œä»¥ä¾¿è¾“å…¥åˆ°å¸¸è§„çš„ Evoformer æ¨¡å—ä¸­ã€‚

$$\tilde{Z}_{ij} = H_{pre, ij} \cdot \mathcal{Z}_{ij} = \sum_{k=1}^n H_{pre, ij}^{(k)} \cdot z_{ij}^{(k)}$$
ç»“æœ $\tilde{Z}_{ij} \in \mathbb{R}^C$ã€‚
##### 2.2 æ ‡å‡†å˜æ¢ (Standard Transform)

åœ¨æ”¶ç¼©åçš„ç‰¹å¾ä¸Šåº”ç”¨æ ‡å‡†çš„ä¸‰è§’æ›´æ–°ã€æ³¨æ„åŠ›æœºåˆ¶å’Œè¿‡æ¸¡å±‚ã€‚ä»¤ $F$ ä¸ºåŒ…å«ä»¥ä¸‹ç»„ä»¶çš„å¤åˆå‡½æ•°ï¼š

1. Triangle Multiplicative Update
    
2. Triangle Attention (Start & End)
    
3. Pair Transition
    

$$\Delta Z_{ij} = F(\tilde{Z}_{ij})$$

æ³¨æ„ï¼šè¿™ä¸€æ­¥çš„è®¡ç®—å¤æ‚åº¦ä»ä¸ºæ ‡å‡† Evoformer çš„å¤æ‚åº¦ã€‚
##### 2.3 æ‰©å±• (Expansion)

å°†è®¡ç®—å‡ºçš„æ›´æ–°é‡ $\Delta Z_{ij}$ æ˜ å°„å› $n$ ç»´è¶…ç©ºé—´ï¼š
$$\Delta \mathcal{Z}_{ij} = H_{post, ij} \otimes \Delta Z_{ij}$$
å…¶ä¸­ $\otimes$ è¡¨ç¤ºå¤–ç§¯ (Broadcasting)ï¼Œç»“æœç»´åº¦ä¸º $n \times C$ã€‚
##### 2.4 è¶…æ®‹å·®æ›´æ–° (Hyper-Residual Update)
æœ€ç»ˆçš„æ›´æ–°ç»“åˆäº†å†…éƒ¨æ··åˆ (Internal Mixing) å’Œæ–°çš„æ›´æ–°é‡ï¼š

$$\mathcal{Z}_{ij}^{(l+1)} = \underbrace{H_{res, ij} \cdot \mathcal{Z}_{ij}^{(l)}}_{\text{Stable Mixing}} + \underbrace{\Delta \mathcal{Z}_{ij}}_{\text{New Info}}$$

---

#### 3. è¾“å…¥è¾“å‡ºæ¥å£

##### 3.1 è¾“å…¥æ‰©å±• (Input Expansion)

å¦‚æœæ˜¯ç¬¬ä¸€å±‚ï¼Œå°†æ ‡å‡†è¾“å…¥å¤åˆ¶ $n$ æ¬¡ï¼š
$$\mathcal{Z}_{ij}^{(0, k)} = Z_{input, ij}, \quad \forall k \in [1, n]$$
##### 3.2 è¾“å‡ºæ”¶ç¼© (Output Contraction)

å¦‚æœæ˜¯æœ€åä¸€å±‚ï¼Œå¯¹è¶…ç»´åº¦å–å¹³å‡ä»¥æ¢å¤æ ‡å‡†è¾“å‡ºï¼š
$$Z_{output, ij} = \frac{1}{n} \sum_{k=1}^n z_{ij}^{(L, k)}$$
---

#### 4. å†…å­˜å¤æ‚åº¦è­¦å‘Š
- **æ ‡å‡†é…å¯¹ç‰¹å¾**: $O(L^2 \cdot C)$
- **mHC é…å¯¹ç‰¹å¾**: $O(L^2 \cdot n \cdot C)$

ç”±äº $L^2$ é¡¹çš„å­˜åœ¨ï¼Œå½“ $L$ å¾ˆå¤§ï¼ˆå¦‚ 1024ï¼‰ä¸” $n=4$ æ—¶ï¼Œæ˜¾å­˜æ¶ˆè€—ä¼šæ¿€å¢ã€‚
ä¾‹å¦‚ï¼Œå¯¹äº $L=1024, C=128, n=4$ (FP32)ï¼š
$$\text{Memory} \approx 1024^2 \times 4 \times 128 \times 4 \text{ bytes} \approx 2.1 \text{ GB}$$
è¿™ä»…æ˜¯ç‰¹å¾å›¾çš„å¤§å°ï¼Œä¸åŒ…æ‹¬æ¢¯åº¦å’Œä¸­é—´æ¿€æ´»å€¼ã€‚å› æ­¤å»ºè®®åœ¨é…å¯¹ç‰¹å¾ä¸Šä½¿ç”¨è¾ƒå°çš„ $n$ (å¦‚ $n=2$)ã€‚
### 3.mHC Structure Net

åŸºäº `mhc_structure_net.py`ï¼Œè¯¥æ¨¡å—å°† **mHC (Manifold-Constrained Hyper-Connections)** æ¶æ„åº”ç”¨äº **å•ä½“ç‰¹å¾ (Single Representation)** åŠå…¶æ ¸å¿ƒç»„ä»¶ **IPA (Invariant Point Attention)**ã€‚

ä»¥ä¸‹æ˜¯æ ¸å¿ƒæ•°å­¦å…¬å¼è¯´æ˜ã€‚
#### ç¬¦å·å®šä¹‰

- $L$: åºåˆ—é•¿åº¦ã€‚
    
- $C_s$: å•ä½“ç‰¹å¾ç»´åº¦ã€‚
    
- $n$: mHC æ‰©å±•ç‡ (é€šå¸¸ä¸º 4)ã€‚
    
- $\mathcal{S} \in \mathbb{R}^{L \times n \times C_s}$: **è¶…å•ä½“ç‰¹å¾ (Hyper Single Features)**ã€‚
    
- $T$: åˆšä½“å˜æ¢ (Rigid Transform)ã€‚
    

---

#### 1. åŠ¨æ€æ˜ å°„ (Dynamic Mappings)

å¯¹äºæ¯ä¸ªæ®‹åŸº $i$ï¼Œç½‘ç»œåŠ¨æ€è®¡ç®—æŠ•å½±çŸ©é˜µå’Œæ··åˆçŸ©é˜µã€‚
å±•å¹³å¹¶å½’ä¸€åŒ–ï¼š
$$x_i = \text{Flatten}(\mathcal{S}_i) \in \mathbb{R}^{nC_s}$$
$$\bar{x}_i = \text{RMSNorm}(x_i)$$

##### 1.1 é¢„æŠ•å½± (Pre-projection) $H_{pre}$
$$\mathbf{H}_{pre, i} = \sigma(\mathbf{W}_{pre} \bar{x}_i + b_{pre}) \in \mathbb{R}^{1 \times n}$$
##### 1.2 åæŠ•å½± (Post-projection) $H_{post}$
$$\mathbf{H}_{post, i} = 2 \cdot \sigma(\mathbf{W}_{post} \bar{x}_i + b_{post}) \in \mathbb{R}^{n \times 1}$$
##### 1.3 æ®‹å·®æ··åˆ (Residual Mixing) $H_{res}$
è¿™æ˜¯ä¸€ä¸ªé€šè¿‡ Sinkhorn-Knopp ç®—æ³•ç”Ÿæˆçš„åŒéšæœºçŸ©é˜µã€‚
$$\mathbf{A}_{i} = \mathbf{W}_{res} \bar{x}_i + b_{res} \in \mathbb{R}^{n \times n}$$
$$\mathbf{H}_{res, i} = \text{SinkhornKnopp}(\mathbf{A}_{i})$$
---

#### 2. mHC ç»“æ„å±‚ (mHC Structure Layer)

æ•°æ®æµéµå¾ª**æ”¶ç¼©-å¤„ç†-æ‰©å±•**çš„æ¨¡å¼ã€‚

##### 2.1 æ”¶ç¼©ä¸ IPA (Contraction & IPA)

é¦–å…ˆå°†è¶…ç‰¹å¾æ”¶ç¼©å›æ ‡å‡†ç»´åº¦ï¼Œä»¥ä¾¿è¾“å…¥åˆ°æ ‡å‡†çš„ IPA æ¨¡å—ä¸­ã€‚
$$s_{contracted, i} = \mathbf{H}_{pre, i} \cdot \mathcal{S}_i = \sum_{k=1}^n H_{pre, i}^{(k)} \cdot s_i^{(k)}$$
ç„¶åæ‰§è¡Œæ ‡å‡†çš„ç»“æ„æ›´æ–°æ“ä½œï¼ˆIPA + Transitionï¼‰ï¼š
$$\Delta s_{ipa} = \text{LayerNorm}(\text{Dropout}(\text{IPA}(s_{contracted}, p, T, \text{mask})))$$
$$\Delta s_{total} = \text{Transition}(\Delta s_{ipa})$$
##### 2.2 æ‰©å±•ä¸æ›´æ–° (Expansion & Update)

å°†æ ‡å‡†ç»´åº¦çš„æ›´æ–°é‡ $\Delta s_{total}$ å¹¿æ’­å› $n$ ç»´è¶…ç©ºé—´ï¼š
$$\Delta \mathcal{S}_i = \mathbf{H}_{post, i} \otimes \Delta s_{total}$$
ç»“åˆå†…éƒ¨æ··åˆå’Œæ–°ä¿¡æ¯è¿›è¡Œæœ€ç»ˆæ›´æ–°ï¼š
$$\mathcal{S}_i^{(l+1)} = \mathbf{H}_{res, i} \cdot \mathcal{S}_i^{(l)} + \Delta \mathcal{S}_i$$
##### 2.3 éª¨æ¶æ›´æ–° (Backbone Update)

æ¯ä¸€å±‚çš„æœ€åï¼Œåˆ©ç”¨æ”¶ç¼©åçš„ç‰¹å¾æ›´æ–°åˆšä½“å˜æ¢ $T$ï¼š

$$s_{for\_bb} = \text{Contract}(\mathcal{S}^{(l+1)})$$
$$T^{(l+1)} = T^{(l)} \circ \text{BackboneUpdate}(s_{for\_bb})$$
---

#### 3. æ•°å€¼ç¨³å®šæ€§ä¸æ¢¯åº¦æµ

mHC çš„æ ¸å¿ƒä»·å€¼åœ¨äº $\mathbf{H}_{res}$ çš„åŒéšæœºæ€§è´¨ã€‚æ ¹æ® Perron-Frobenius å®šç†ï¼ŒåŒéšæœºçŸ©é˜µçš„æœ€å¤§ç‰¹å¾å€¼ï¼ˆè°±åŠå¾„ï¼‰ä¸º 1ã€‚
$$\|\mathbf{H}_{res, i} \cdot \mathcal{S}_i\| \approx \|\mathcal{S}_i\|$$
è¿™æ„å‘³ç€åœ¨æ·±å±‚ç½‘ç»œï¼ˆå¦‚ AlphaFold2 çš„ 48 å±‚ Evoformer æˆ– 8 å±‚ Structure Moduleï¼‰ä¸­ï¼Œæ®‹å·®æµæ—¢ä¸ä¼šå‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸ï¼ˆExplosionï¼‰ï¼Œä¹Ÿä¸ä¼šå‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±ï¼ˆVanishingï¼‰ã€‚è¿™å¯¹äºé•¿åºåˆ—è®­ç»ƒè‡³å…³é‡è¦ï¼Œå› ä¸ºæ ‡å‡†æ®‹å·®è¿æ¥é€šå¸¸ä¾èµ– LayerNorm æ¥æ§åˆ¶å°ºåº¦ï¼Œè€Œ LayerNorm åœ¨ææ·±ç½‘ç»œä¸­å¯èƒ½å¤±æ•ˆã€‚
#### 4. å†…å­˜åˆ†æ

ç›¸æ¯” `mHCPairTransformNet`ï¼Œ`mHCStructureNet` çš„å†…å­˜å¼€é”€è¦å°å¾—å¤šï¼Œå› ä¸ºå®ƒæ˜¯ä½œç”¨åœ¨å•ä½“ç‰¹å¾ ($L \times C$) ä¸Šï¼Œè€Œä¸æ˜¯é…å¯¹ç‰¹å¾ ($L^2 \times C$) ä¸Šã€‚
- **å•ä½“ç‰¹å¾**: $O(L \cdot n \cdot C)$
- **é…å¯¹ç‰¹å¾**: $O(L^2 \cdot n \cdot C)$

å› æ­¤ï¼Œé€šå¸¸å¯ä»¥å®‰å…¨åœ°åœ¨ Structure Module ä¸­ä½¿ç”¨è¾ƒå¤§çš„æ‰©å±•ç‡ (å¦‚ $n=4$)ï¼Œè€Œä¸ä¼šå¯¼è‡´æ˜¾å­˜æº¢å‡ºã€‚
### 4.mHC Slash Structure Net
åŸºäº `mhc_flash_structure_net.py`ï¼Œè¯¥æ¨¡å—æ˜¯ **mHC (Manifold-Constrained Hyper-Connections)** ä¸ **Flash-IPA (Flash Invariant Point Attention)** çš„ç»“åˆä½“ã€‚
è¿™ç§è®¾è®¡æ—¨åœ¨åŒæ—¶è§£å†³é•¿åºåˆ—è›‹ç™½è´¨ç”Ÿæˆä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒç“¶é¢ˆï¼š
1. **è®­ç»ƒç¨³å®šæ€§**ï¼šé€šè¿‡ mHC çš„åŒéšæœºæ®‹å·®æµè§£å†³æ·±å±‚ç½‘ç»œå’Œé•¿åºåˆ—çš„æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±é—®é¢˜ã€‚
2. **æ˜¾å­˜æ•ˆç‡**ï¼šé€šè¿‡ Flash-IPA é¿å…æ˜¾å¼æ„å»º $L \times L$ çš„é…å¯¹æ³¨æ„åŠ›åç½®çŸ©é˜µã€‚

ä»¥ä¸‹æ˜¯æ ¸å¿ƒæ•°å­¦å…¬å¼è¯´æ˜ã€‚
#### ç¬¦å·å®šä¹‰

- $\mathcal{S} \in \mathbb{R}^{L \times n \times C_s}$: mHC æ‰©å±•åçš„è¶…å•ä½“ç‰¹å¾ (Hyper Single Representation)ã€‚
    
- $s \in \mathbb{R}^{L \times C_s}$: æ”¶ç¼©åçš„æ ‡å‡†å•ä½“ç‰¹å¾ã€‚
    
- $Z_{fac1} \in \mathbb{R}^{L \times R \times C_z}$: å› å­åŒ–çš„é…å¯¹ç‰¹å¾ï¼ˆå·¦å› å­ï¼‰ã€‚
    
- $Z_{fac2} \in \mathbb{R}^{L \times H \times R \times \frac{C_z}{4}}$: å› å­åŒ–çš„é…å¯¹ç‰¹å¾ï¼ˆå³å› å­ï¼Œé’ˆå¯¹å¤šå¤´ä¼˜åŒ–ï¼‰ã€‚
    
- $T$: åˆšä½“å˜æ¢ (åŒ…å«æ—‹è½¬ $R$ å’Œå¹³ç§» $\vec{t}$)ã€‚
    

---

#### 1. mHC åŠ¨æ€æŠ•å½±ä¸æ··åˆ (mHC Dynamic Projection & Mixing)

ä¸ä¹‹å‰çš„ mHC æ¨¡å—ç›¸åŒï¼Œé¦–å…ˆè®¡ç®—åŠ¨æ€æ˜ å°„çŸ©é˜µï¼Œç”¨äºåœ¨è¶…ç©ºé—´å’Œæ ‡å‡†ç©ºé—´ä¹‹é—´è½¬æ¢ã€‚
$$x_i = \text{Flatten}(\mathcal{S}_i), \quad \bar{x}_i = \text{RMSNorm}(x_i)$$
- **æ”¶ç¼©çŸ©é˜µ**: $\mathbf{H}_{pre, i} = \sigma(\mathbf{W}_{pre} \bar{x}_i + b_{pre}) \in \mathbb{R}^{1 \times n}$
- **æ‰©å±•çŸ©é˜µ**: $\mathbf{H}_{post, i} = 2\sigma(\mathbf{W}_{post} \bar{x}_i + b_{post}) \in \mathbb{R}^{n \times 1}$
- **æ®‹å·®çŸ©é˜µ**: $\mathbf{H}_{res, i} = \text{Sinkhorn}(\mathbf{W}_{res} \bar{x}_i + b_{res}) \in \mathbb{R}^{n \times n}$

---

#### 2. Flash-IPA æ ¸å¿ƒè®¡ç®—

Flash-IPA çš„æ ¸å¿ƒåœ¨äº**ä¸æ˜¾å¼æ„å»º $L \times L$ çš„ Pair Bias çŸ©é˜µ**ã€‚
##### 2.1 æ”¶ç¼© (Contraction)
å°†è¶…ç‰¹å¾æŠ•å½±åˆ°æ ‡å‡†ç»´åº¦ï¼Œä½œä¸º Flash-IPA çš„ Query/Key/Value è¾“å…¥æºï¼š
$$s_{in, i} = \mathbf{H}_{pre, i} \cdot \mathcal{S}_i$$
##### 2.2 å› å­åŒ–åç½®æ³¨æ„åŠ› (Factorized Bias Attention)

æ ‡å‡† IPA çš„æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—åŒ…å«ä¸€ä¸ªé…å¯¹åç½®é¡¹ $b_{ij} = w^T z_{ij}$ã€‚

åœ¨ Flash-IPA ä¸­ï¼Œè¿™ä¸ªåç½®é¡¹é€šè¿‡ä½ç§©å› å­é‡å»ºï¼š
$$b_{ij} \approx \text{Linear}(Z_{fac1, i} \odot Z_{fac2, j})$$
å®Œæ•´çš„æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ä¸ºï¼š
$$A_{ij} = \frac{q_i k_j^T}{\sqrt{d}} + b_{ij} + \text{GeometryTerm}(T_i, T_j)$$
Flash-IPA åˆ©ç”¨ Flash Attention ç®—å­ï¼ˆå¦‚ FlashAttention-2 æˆ– 3ï¼‰åœ¨è®¡ç®— $A_{ij}$ åç«‹å³è®¡ç®— Softmax å’Œ Value èšåˆï¼Œè€Œä¸å°†å®Œæ•´çš„ $A_{ij}$ å†™å…¥æ˜¾å­˜ã€‚
$$s_{ipa} = \text{FlashAttention}(Q, K, V, \text{BiasFactors})$$
---

#### 3. ç»“æ„å±‚æ›´æ–°æµ (Structure Layer Update Flow)

##### 3.1 IPA ä¸è¿‡æ¸¡ (IPA & Transition)
$$\Delta s = \text{Transition}(\text{LayerNorm}(\text{Dropout}(s_{ipa})))$$
æ³¨æ„ï¼šè¿™é‡Œçš„ $\Delta s \in \mathbb{R}^{L \times C_s}$ æ˜¯åœ¨æ ‡å‡†ç»´åº¦ä¸Šè®¡ç®—çš„ã€‚
##### 3.2 æ‰©å±•ä¸è¶…æ®‹å·® (Expansion & Hyper-Residual)

å°†æ›´æ–°é‡å¹¿æ’­å›è¶…ç©ºé—´ï¼Œå¹¶ä¸å†å²çŠ¶æ€æ··åˆï¼š
$$\mathcal{S}_i^{(l+1)} = \underbrace{\mathbf{H}_{res, i} \cdot \mathcal{S}_i^{(l)}}_{\text{Stable History}} + \underbrace{\mathbf{H}_{post, i} \otimes \Delta s_i}_{\text{New Info}}$$
è¿™ç§æœºåˆ¶ç¡®ä¿äº†å³ä½¿åœ¨ 1000+ å±‚çš„æ·±åº¦æˆ– 2048+ çš„åºåˆ—é•¿åº¦ä¸‹ï¼Œæ¢¯åº¦çš„ä¼ æ’­ä¾ç„¶ç”±è°±åŠå¾„ä¸º 1 çš„ $\mathbf{H}_{res}$ ä¸»å¯¼ï¼Œæå…¶ç¨³å®šã€‚
##### 3.3 éª¨æ¶æ›´æ–° (Backbone Update)
åˆšä½“å˜æ¢çš„æ›´æ–°ä¾èµ–äº IPA çš„è¾“å‡ºï¼ˆæœªç» mHC æ‰©å±•ï¼‰ï¼š
$$T^{(l+1)} = T^{(l)} \circ \text{BackboneUpdate}(s_{ipa})$$
---

#### 4. æ˜¾å­˜ä¸å¤æ‚åº¦ä¼˜åŠ¿

è¯¥æ¨¡å—å®ç°äº†åŒé‡ä¼˜åŒ–ï¼š

1. **å‚æ•°ä¸çŠ¶æ€æ•ˆç‡ (mHC)**:
    - é€šè¿‡ $n$ å€æ‰©å±•æ®‹å·®æµï¼Œä½†ä»…ä½¿ç”¨ $O(n)$ çš„å‚æ•°ç”ŸæˆæŠ•å½±çŸ©é˜µï¼Œé¿å…äº†ç›´æ¥å¢åŠ ç½‘ç»œå®½åº¦å¸¦æ¥çš„ $O(C^2)$ å‚æ•°å¢é•¿ã€‚
2. **æ³¨æ„åŠ›æ˜¾å­˜æ•ˆç‡ (Flash-IPA)**:
    - **æ ‡å‡† IPA**: æ˜¾å­˜ $O(L^2 \cdot H)$ (ç”¨äºå­˜å‚¨ logits å’Œé…å¯¹åç½®)ã€‚
    - **Flash-IPA**: æ˜¾å­˜ $O(L \cdot H)$ (çº¿æ€§å¤æ‚åº¦ï¼Œåˆ©ç”¨åˆ†å—è®¡ç®—)ã€‚

ç»“åˆåï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿä»¥çº¿æ€§æ˜¾å­˜å¤æ‚åº¦è®­ç»ƒè¶…é•¿åºåˆ—ï¼ŒåŒæ—¶ä¿æŒææ·±çš„æ®‹å·®ç½‘ç»œæ”¶æ•›æ€§ã€‚
## ğŸ“¦ æ–‡ä»¶æ¸…å•
### Stage 4-5 æ–°å¢æ–‡ä»¶ (4 ä¸ªæ ¸å¿ƒæ¨¡å—) ** Stage 4-5**

1. **`genie/model/axial_attention.py`** (600+ è¡Œ) ** Stage 4**
    - `AxialAttention`: è½´å‘æ³¨æ„åŠ› (è¡Œ+åˆ—åˆ†è§£)
    - `FactorizedAxialAttention`: å› å­åŒ–è½´å‘æ³¨æ„åŠ›
2. **`genie/training/gradient_checkpointing.py`** (400+ è¡Œ) **Stage 4**
    - `CheckpointConfig`: æ£€æŸ¥ç‚¹é…ç½®
    - `AdaptiveCheckpointManager`: è‡ªé€‚åº”æ£€æŸ¥ç‚¹ç®¡ç†
    - `CheckpointedSequential`: æ£€æŸ¥ç‚¹åºåˆ—æ¨¡å—
3. **`genie/model/model_compression.py`** (500+ è¡Œ) **Stage 4**
    - `CompressedStructureNet`: å‹ç¼©ç»“æ„ç½‘ç»œ
    - `SharedLayerModule`: å…±äº«å±‚æ¨¡å—
    - `AlternatingSharedLayers`: äº¤æ›¿å…±äº«å±‚
4. **`genie/training/distributed_training.py`** (500+ è¡Œ) **Stage 5**
    - `DistributedModelWrapper`: DDPå°è£…
    - `SequenceTensorParallel`: åºåˆ—å¼ é‡å¹¶è¡Œ
    - `GradientAccumulator`: æ¢¯åº¦ç´¯ç§¯
5. **`test_stage4_5.py`** (500+ è¡Œ) **Stage 4-5**
    - 6 ä¸ªç»¼åˆæµ‹è¯•
    - Stage 4-5 é›†æˆæµ‹è¯•

### Stage 3 V2 æ–°å¢æ–‡ä»¶ (2 ä¸ªæ ¸å¿ƒæ¨¡å—) **Stage 3 V2**

1. **`genie/model/sparse_pairs.py`** (500+ è¡Œ) **Stage 3 V2**
    - `SparseKNNPairSelector`: ç¨€ç– k-NN å¯¹é€‰æ‹©å™¨
    - ä¸‰ç§é€‰æ‹©ç­–ç•¥: coordinate / sequence / hybrid
    - æ”¯æŒè¶…é•¿åºåˆ— 
2. **`test_stage3_v2.py`** (400+ è¡Œ) **Stage 3 V2**
    - 4 ä¸ªç»¼åˆæµ‹è¯•
    - Ultra-long memory scaling

### Stage 3 æ–°å¢æ–‡ä»¶ (4 ä¸ªæ ¸å¿ƒæ¨¡å—) **Stage 3**

1. **`genie/training/progressive_training.py`** (400+ è¡Œ) **Stage 3**
    - `ProgressiveTrainingScheduler`: æ¸è¿›å¼è®­ç»ƒè°ƒåº¦å™¨
    - `ChunkedLossComputation`: åˆ†å—æŸå¤±è®¡ç®—
    - æ”¯æŒ linear/cosine/exponential å¢é•¿æ›²çº¿
    - FAPE å’Œ dRMSD æŸå¤±æ”¯æŒ
2. **`genie/training/mixed_precision.py`** (300+ è¡Œ) **Stage 3**
    - `MixedPrecisionTrainer`: æ··åˆç²¾åº¦è®­ç»ƒç®¡ç†å™¨
    - `SelectiveMixedPrecision`: é€‰æ‹©æ€§ç²¾åº¦æ§åˆ¶
    - FP16/BF16 æ”¯æŒ + åŠ¨æ€æŸå¤±ç¼©æ”¾
    - **æ”¶ç›Š**: 50% å†…å­˜èŠ‚çœ + 2-3x è®­ç»ƒåŠ é€Ÿ
3. **`genie/training/stage3_trainer.py`** (400+ è¡Œ) **Stage 3**
    - `Stage3TrainingManager`: ç»¼åˆè®­ç»ƒç®¡ç†å™¨
    - é›†æˆæ‰€æœ‰ Stage 3 ä¼˜åŒ–
    - ç»Ÿä¸€è®­ç»ƒæ¥å£
    - Checkpoint æ”¯æŒ
4. **`test_stage3_optimizations.py`** (400+ è¡Œ) **Stage 3**
    - 5 ä¸ªç»¼åˆæµ‹è¯•
    - Performance comparison

### Stage 2 æ–°å¢æ–‡ä»¶ (3 ä¸ªæ ¸å¿ƒæ¨¡å—)

1. **`genie/model/factorized_triangle_ops.py`** (500+ è¡Œ) **Stage 2**
   - `FactorizedTriangleMultiplicativeUpdate`: å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°
   - `FactorizedTriangleMultiplicationOutgoing`: Outgoing å˜ä½“
   - `FactorizedTriangleMultiplicationIncoming`: Incoming å˜ä½“
   - `ChunkedTriangleAttention`: åˆ†å—ä¸‰è§’æ³¨æ„åŠ›
   - `ChunkedTriangleAttentionStartingNode`: è¡Œæ–¹å‘æ³¨æ„åŠ›
   - `ChunkedTriangleAttentionEndingNode`: åˆ—æ–¹å‘æ³¨æ„åŠ›
2. **`genie/model/factorized_pair_transform.py`** (300+ è¡Œ) **Stage 2**
    - `FactorizedPairTransformLayer`: å•å±‚ pair è½¬æ¢
    - `FactorizedPairTransformNet`: å¤šå±‚ pair è½¬æ¢ç½‘ç»œ
    - å®Œæ•´çš„ Evoformer-style processing
    - æ‰€æœ‰æ“ä½œéƒ½åœ¨å› å­åŒ–è¡¨ç¤ºä¸Šè¿›è¡Œ
3. **`test_stage2_optimizations.py`** (400+ è¡Œ) **Stage 2**
    - 5 ä¸ªç»¼åˆæµ‹è¯•
    - å†…å­˜ç¼©æ”¾åˆ†æ
    - Stage 1 vs Stage 2 å¯¹æ¯”
    - **`test_stage2_quick.py`**: å¿«é€Ÿé›†æˆæµ‹è¯•

### æ ¸å¿ƒå®ç° (Stage 1 + V2 æ–‡ä»¶)

1. **`genie/model/factorized_pair_features.py`** (560 è¡Œ)
   - `FactorizedPairFeatureNet`: ä¸»è¦ç±»
   - `FactorizedRelPos`: å› å­åŒ–ä½ç½®ç¼–ç 
   - `FactorizedTemplate`: å› å­åŒ–æ¨¡æ¿ç‰¹å¾
   - `AdaptiveFactorizationRank`: åŠ¨æ€ rank è°ƒæ•´

2. **`genie/model/long_sequence_denoiser.py`** (400+ è¡Œ)
   - `LongSequenceDenoiser`: é›†æˆæ‰€æœ‰ä¼˜åŒ–
   - è‡ªåŠ¨é…ç½®å’Œå†…å­˜ä¼°ç®—
   - å®Œæ•´çš„æµ‹è¯•å‡½æ•°

3. **`genie/utils/adaptive_config.py`** (500+ è¡Œ)
   - `AdaptiveMHCConfig`: mHC é…ç½®
   - `DynamicBatchSize`: æ‰¹æ¬¡å¤§å°è®¡ç®—
   - `AdaptiveFactorizationRank`: Rank è®¡ç®—
   - `MemoryEstimator`: å†…å­˜ä¼°ç®—å·¥å…·

### æ–‡æ¡£ (4 ä¸ªæ–‡ä»¶)

1. **`EVALUATION_AND_IMPROVEMENTS.md`** (2000+ è¡Œ)
   - å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
   - 5 é˜¶æ®µä¼˜åŒ–è·¯çº¿å›¾
   - è¯¦ç»†çš„æŠ€æœ¯åˆ†æ
   - ä»£ç ç¤ºä¾‹å’ŒåŸºå‡†æµ‹è¯•
2. **`mhc_code_review_fixes.md`** (ä¹‹å‰åˆ›å»º)
   - Bug ä¿®å¤æ€»ç»“
   - Skip Connection è¯¦ç»†åˆ†æ
   - Sinkhorn ä¼˜åŒ–è¯´æ˜

## ğŸ“ åˆ›æ–°ç‚¹

### 1. End-to-End Factorization
**åˆ›æ–°**: å®Œå…¨é¿å… pair tensor å®ä¾‹åŒ–
```
ä¼ ç»Ÿ: s â†’ p[LÂ²] â†’ factorize â†’ factors[LÃ—rank]
     (éœ€è¦ 537 MB)          (éœ€è¦ 1 MB)

åˆ›æ–°: s â†’ factors[LÃ—rank] (ç›´æ¥ç”Ÿæˆ)
     (ä»…éœ€ 1 MB, èŠ‚çœ 537x!)
```

### 2. Adaptive Architecture
**åˆ›æ–°**: åºåˆ—é•¿åº¦æ„ŸçŸ¥çš„æ¨¡å‹é…ç½®
- çŸ­åºåˆ—: é«˜å®¹é‡ (è´¨é‡ä¼˜å…ˆ)
- é•¿åºåˆ—: ä½å®¹é‡ (æ•ˆç‡ä¼˜å…ˆ)
- åŠ¨æ€å¹³è¡¡: è‡ªåŠ¨è°ƒæ•´

### 3. Memory-First Design
**åˆ›æ–°**: ä»¥å†…å­˜ä¸ºç¬¬ä¸€çº¦æŸï¼ˆæœ€å¼€å§‹æ˜¯æ²¡æœ‰é«˜æ€§èƒ½gpuï¼Œæ‰€ä»¥åªèƒ½æ”¹ï¼‰

- æ¯ä¸ªä¼˜åŒ–éƒ½æœ‰å†…å­˜ä¼°ç®—
- é…ç½®è‡ªåŠ¨æ£€æŸ¥å’Œè­¦å‘Š
- æä¾›è¯¦ç»†çš„å†…å­˜åˆ†æå·¥å…·

## ğŸ’¡ å…³é”®

### æŠ€æœ¯äº’è¡¥
- Genie: æ ¸å¿ƒæ¶æ„
- Flash-IPA: å†…å­˜æ•ˆç‡
- AlphaFold2 Evoformer: å› å­åŒ–ä¸‰è§’æ“ä½œ (Stage 2)
- Curriculum Learning: æ¸è¿›å¼è®­ç»ƒ (Stage 3)
- Mixed Precision: è®­ç»ƒåŠ é€Ÿ (Stage 3)
- Sparse Attention: k-NN ç¨€ç–å¯¹é€‰æ‹© (Stage 3 V2)
- Axial Attention: è®¡ç®—æ•ˆç‡ä¼˜åŒ– (Stage 4) 
- Universal Transformer: æ¨¡å‹å‹ç¼© (Stage 4) 
- Distributed Training: å¤šGPUæ‰©å±• (Stage 5) 

---