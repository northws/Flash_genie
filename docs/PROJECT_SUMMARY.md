# ğŸ¯ Genie é•¿åºåˆ—æ‰©å±•é¡¹ç›®æ€»ç»“

> [!CAUTION]
>
> GitHubç½‘é¡µä¸Š**éƒ¨åˆ†æ•°å­¦å…¬å¼æ— æ³•æ­£å¸¸æ˜¾ç¤º**ï¼Œå¦‚éœ€æŸ¥çœ‹åªèƒ½åˆ°æœ¬åœ°markdownç¼–è¾‘å™¨ä¸­æŸ¥çœ‹

## ğŸ“Š é¡¹ç›®ç»“æœ

### å½“å‰å®ç°

| ç»´åº¦ | è¯´æ˜ |
|------|------|
| **Flash-IPA é›†æˆ(Stage 1)** | âœ… å·²å®ç° |
| **mHC é›†æˆ(Stage 1)** | âœ… å·²å®ç° |
| **Pair Features(Stage 1)** | âœ… å·²å®ç°ï¼Œ V2ä¿®å¤æ•°å­¦é—®é¢˜ï¼Œä¿ç•™å‡ ä½•ä¿¡æ¯ |
| **Triangle Ops (Stage 2)** | âœ… å› å­åŒ–ä¸‰è§’æ“ä½œ |
| **Training (Stage 3)** | âœ… æ¸è¿›å¼è®­ç»ƒ+æ··åˆç²¾åº¦ |
| **Sparse Pairs (Stage 3 V2)** | âœ… k-NNç¨€ç–å¯¹ |
| **Axial Attention (Stage 4)** | âœ… è®¡ç®—æ•ˆç‡ä¼˜åŒ– |
| **Model Compression (Stage 4)** | âœ… å‚æ•°å…±äº« |
| **Distributed Training (Stage 5)** | âœ… å¤šGPUæ”¯æŒ |
| **è®­ç»ƒç¨³å®šæ€§** | âœ… mHC + Progressive Training |
| **æ–‡æ¡£å®Œæ•´æ€§** | âœ… å®Œæ•´çš„æ–‡æ¡£å’Œæµ‹è¯• |

## v1 + v1.2: Factorized Pair Features

 `factorized_pair_features.py` æ¨¡å—æ ¸å¿ƒåŠŸèƒ½çš„æ•°å­¦é€»è¾‘æè¿°å¦‚ä¸‹

è¯¥æ¨¡å—çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°† $O(L^2)$ çš„é…å¯¹å¼ é‡ $\mathbf{P}$ åˆ†è§£ä¸ºä¸¤ä¸ªç§©ä¸º $R$ çš„ä½ç§©å› å­ $\mathbf{F}_L$ å’Œ $\mathbf{F}_R$ï¼Œä»è€Œå°†å†…å­˜å¤æ‚åº¦ä» $O(L^2 \cdot C)$ é™ä½åˆ° $O(L \cdot R \cdot C)$ã€‚

### 1. æ ¸å¿ƒåˆ†è§£å…¬å¼ (Core Factorization)

å®Œæ•´çš„é…å¯¹ç‰¹å¾ $\mathbf{p}_{ij} \in \mathbb{R}^C$ è¢«è¿‘ä¼¼ä¸ºä¸¤ä¸ªå› å­å¼ é‡çš„æ”¶ç¼©ã€‚

è®¾ï¼š

- $L$ ä¸ºåºåˆ—é•¿åº¦
- $R$ ä¸ºåˆ†è§£çš„ç§© (Rank)
- $C$ ä¸ºç‰¹å¾ç»´åº¦

é‡æ„å…¬å¼ï¼ˆå¯¹åº”ä»£ç ä¸­çš„ `reconstruct_pair` å’Œ `forward` çš„é€»è¾‘ï¼‰ï¼š

$$\mathbf{p}_{ij} = \sum_{r=1}^{R} \left( \mathbf{f}_{L, i, r} \odot \mathbf{f}_{R, j, r} \right)$$

å…¶ä¸­ï¼š

- $\mathbf{f}_{L, i, r} \in \mathbb{R}^C$ æ˜¯å·¦å› å­åœ¨ä½ç½® $i$ã€ç§© $r$ çš„ç‰¹å¾ã€‚
- $\mathbf{f}_{R, j, r} \in \mathbb{R}^C$ æ˜¯å³å› å­åœ¨ä½ç½® $j$ã€ç§© $r$ çš„ç‰¹å¾ã€‚
- $\odot$ è¡¨ç¤ºæ²¿ç‰¹å¾ç»´åº¦ $C$ çš„é€å…ƒç´ ä¹˜æ³• (Hadamard product)ã€‚

------

### 2. å› å­ç”Ÿæˆè¿‡ç¨‹ (Factor Generation)

å› å­ $\mathbf{F}_L$ å’Œ $\mathbf{F}_R$ ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šå•åºåˆ—æŠ•å½±ã€ç›¸å¯¹ä½ç½®ç¼–ç å’Œæ¨¡æ¿ç‰¹å¾ã€‚

$$\mathbf{f}_{L, i, r} = \mathbf{f}_{L, i, r}^{(s)} + \mathbf{f}_{L, i, r}^{(\text{rel})} + \mathbf{f}_{L, i, r}^{(\text{tmpl})}$$

$$\mathbf{f}_{R, j, r} = \mathbf{f}_{R, j, r}^{(s)} + \mathbf{f}_{R, j, r}^{(\text{rel})} + \mathbf{f}_{R, j, r}^{(\text{tmpl})}$$

#### 2.1 å•åºåˆ—ç‰¹å¾æŠ•å½± (Single Feature Projection)

ç»™å®šå•åºåˆ—ç‰¹å¾ $\mathbf{s}_i \in \mathbb{R}^{C_s}$ï¼š

$$\mathbf{f}_{L, i, r}^{(s)} = \mathbf{W}_{L}^{(s)} \mathbf{s}_i, \quad \mathbf{f}_{R, j, r}^{(s)} = \mathbf{W}_{R}^{(s)} \mathbf{s}_j$$

#### 2.2 å› å­åŒ–åå¯¹ç§°ç›¸å¯¹ä½ç½®ç¼–ç  (Factorized Antisymmetric RelPos)

ä»£ç ç±»ï¼š`FactorizedRelPos`

ä¸ºäº†åœ¨å› å­åŒ–å½¢å¼ä¸­ä¿ç•™ç›¸å¯¹ä½ç½® $i-j$ çš„åå¯¹ç§°æ€§è´¨ï¼ˆå› ä¸º $i-j \neq j-i$ï¼‰ï¼Œå¼•å…¥äº†åå¯¹ç§°åç½® $\mathbf{b}_{rel}$ã€‚

1. **ä½ç½®åµŒå…¥ç»„åˆ**:

   $$\mathbf{h}_i = \left[ \text{Emb}_{\text{abs}}(i) \, ; \, \text{Emb}_{\text{bin}}\left( \text{clamp}(i - \frac{L}{2}) \right) \right]$$

2. **ç”Ÿæˆå› å­**:

   $$\mathbf{f}_{L, i, r}^{(\text{rel})} = \mathbf{W}_{L}^{(\text{pos})} \mathbf{h}_i + \mathbf{b}_{\text{rel}, r}$$

   $$\mathbf{f}_{R, j, r}^{(\text{rel})} = \mathbf{W}_{R}^{(\text{pos})} \mathbf{h}_j - \mathbf{b}_{\text{rel}, r}$$

   *æ³¨ï¼šå½“è®¡ç®—äº¤å‰é¡¹æ—¶ï¼Œ$(+b)(\dots) + (\dots)(-b)$ çš„ç»“æ„æœ‰åŠ©äºæ‰“ç ´åå¯¹ç§°æ€§ã€‚*

#### 2.3 å› å­åŒ–æ¨¡æ¿ç‰¹å¾ (Factorized Template - SVD Style)

ä»£ç ç±»ï¼š`FactorizedTemplate`

ç»™å®šæ¨¡æ¿å¼ é‡ $\mathbf{T} \in \mathbb{R}^{L \times L \times C_t}$ï¼Œä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–èšåˆè¡Œå’Œåˆ—ä¿¡æ¯ã€‚

1. **ç‰¹å¾æå–**:

   - å¯¹è§’çº¿: $\mathbf{v}_{\text{diag}, i} = \mathbf{T}_{ii}$
   - è¡Œèšåˆ: $\mathbf{v}_{\text{row}, i} = \sum_k \text{Softmax}(\mathbf{w}_q^T \mathbf{T}_{ik}) \cdot \mathbf{T}_{ik}$
   - åˆ—èšåˆ: $\mathbf{v}_{\text{col}, i} = \sum_k \text{Softmax}(\mathbf{w}_q^T \mathbf{T}_{ki}) \cdot \mathbf{T}_{ki}$

2. **éå¯¹ç§°ç»„åˆ (Asymmetric Combination)**:

   $$\mathbf{u}_{L, i} = \text{LayerNorm}([\mathbf{v}_{\text{diag}, i}, \mathbf{v}_{\text{row}, i}, \mathbf{v}_{\text{col}, i}])$$

   $$\mathbf{u}_{R, j} = \text{LayerNorm}([\mathbf{v}_{\text{diag}, j}, \mathbf{v}_{\text{col}, j}, \mathbf{v}_{\text{row}, j}]) \quad (\text{æ³¨æ„è¡Œåˆ—é¡ºåºäº¤æ¢})$$

3. **SVDé£æ ¼æŠ•å½±**:

   $$\mathbf{f}_{L, i, r}^{(\text{tmpl})} = (\mathbf{W}_U \mathbf{u}_{L, i}) \cdot \sigma_r$$

   $$\mathbf{f}_{R, j, r}^{(\text{tmpl})} = \mathbf{W}_V \mathbf{u}_{R, j}$$

   å…¶ä¸­ $\sigma_r$ æ˜¯å¯å­¦ä¹ çš„å¥‡å¼‚å€¼æ ‡é‡ï¼Œç”¨äºåŠ æƒä¸åŒç§©çš„é‡è¦æ€§ã€‚

------

### 3. å› å­åŒ–é…å¯¹ç²¾ç‚¼ (Factorized Pair Refinement)

ä»£ç ç±»ï¼š`FactorizedPairRefinementLayer`

è¯¥æ¨¡å—æ¨¡æ‹Ÿä¸‰è§’æ›´æ–°ï¼ˆTriangular Updateï¼‰ï¼Œä½†ä¸æ„å»º $L^2$ çŸ©é˜µï¼Œè€Œæ˜¯é€šè¿‡å› å­é—´çš„äº¤å‰æ³¨æ„åŠ›å®ç°ã€‚

å¯¹äºæ¯ä¸€å±‚ $l$ å’Œæ¯ä¸€ä¸ªç§© $r$ï¼š

$$\mathbf{\hat{f}}_{L, i, r} = \text{LayerNorm}(\mathbf{f}_{L, i, r})$$

$$\mathbf{\hat{f}}_{R, j, r} = \text{LayerNorm}(\mathbf{f}_{R, j, r})$$

**äº¤å‰æ³¨æ„åŠ› (Cross-Factor Interaction):**

$$\mathbf{z}_{L, i, r} = \text{Attention}(Q=\mathbf{\hat{f}}_{L, \cdot, r}, K=\mathbf{\hat{f}}_{R, \cdot, r}, V=\mathbf{\hat{f}}_{R, \cdot, r})_i$$

$$\mathbf{z}_{R, j, r} = \text{Attention}(Q=\mathbf{\hat{f}}_{R, \cdot, r}, K=\mathbf{\hat{f}}_{L, \cdot, r}, V=\mathbf{\hat{f}}_{L, \cdot, r})_j$$

**é—¨æ§æ›´æ–° (Gated Update):**

$$\mathbf{f}_{L, i, r} \leftarrow \mathbf{f}_{L, i, r} + \sigma(\mathbf{W}_{g1}[\mathbf{f}_{L, i, r}; \mathbf{z}_{L, i, r}]) \odot \mathbf{z}_{L, i, r}$$

æœ€åï¼Œé€šè¿‡ç§©æ··åˆå±‚ (Rank Mixing) å…è®¸ä¸åŒç§©ä¹‹é—´äº¤æ¢ä¿¡æ¯ï¼š

$$\mathbf{F}_L \leftarrow \text{Linear}_{\text{rank}}(\mathbf{F}_L), \quad \mathbf{F}_R \leftarrow \text{Linear}_{\text{rank}}(\mathbf{F}_R)$$

## Stage 2 æ›´æ–° (2026-01-11)

### Triangle Operations ä¼˜åŒ–

Stage 2 å®ç°äº†ç±»ä¼¼AlphaFold2 Evoformer çš„ Triangle Operationsï¼Œä½†æ­¤å¤„é€šè¿‡å› å­åŒ–ï¼Œé¿å… O(LÂ³) å†…å­˜å¼€é”€ã€‚

#### **é—®é¢˜**: åŸå§‹ä¸‰è§’ä¹˜æ³•æ›´æ–°éœ€è¦ O(LÂ³) å†…å­˜ï¼Œä¸‰è§’æ³¨æ„åŠ›éœ€è¦ O(LÂ²) æ³¨æ„åŠ›çŸ©é˜µ

 `factorized_triangle_ops.py` æ¨¡å—å®ç°çš„å› å­åŒ–ä¸‰è§’æ“ä½œçš„æ•°å­¦é€»è¾‘æè¿°å¦‚ä¸‹ï¼Œè¯¥æ¨¡å—ä¸»è¦é’ˆå¯¹ AlphaFold2 ä¸­å†…å­˜æ¶ˆè€—å·¨å¤§çš„â€œä¸‰è§’æ›´æ–°â€å’Œâ€œä¸‰è§’æ³¨æ„åŠ›â€è¿›è¡Œäº†ä½ç§©å› å­åŒ–ï¼ˆFactorizedï¼‰å’Œåˆ†å—ï¼ˆChunkedï¼‰ä¼˜åŒ–ã€‚

### ç¬¦å·å®šä¹‰

- $L$: åºåˆ—é•¿åº¦ (Sequence Length)
- $R$: ç§© (Rank)
- $C$: é€šé“æ•° (Channels/Feature dimension)
- $H$: æ³¨æ„åŠ›å¤´æ•° (Number of Heads)
- $d_h$: æ¯ä¸ªå¤´çš„ç»´åº¦ ($C_{hidden} / H$)
- $\sigma$: Sigmoid æ¿€æ´»å‡½æ•°
- $\text{LN}$: Layer Normalization

------

### 1. å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (Factorized Triangle Multiplicative Update)

è¯¥æ¨¡å—å¯¹åº”ç±» `FactorizedTriangleMultiplicativeUpdate`ã€‚å®ƒå°†åŸæœ¬ $O(L^3)$ çš„å¤æ‚åº¦é™ä½ä¸º $O(L^2 \times R)$ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š

åŸå§‹çš„ä¸‰è§’ä¹˜æ³•æ›´æ–°è®¡ç®—å…¬å¼ä¸ºï¼š

$$z_{ij} \leftarrow \sum_{k} a_{ik} \odot b_{kj}$$

ä»£ç ä¸­é€šè¿‡ç»´æŠ¤ä½ç§©å› å­ $Z_{left}, Z_{right} \in \mathbb{R}^{L \times R \times C}$ æ¥é¿å…æ„å»ºå®Œæ•´çš„ $L \times L$ å¼ é‡ã€‚

#### 1.1 é¢„å¤„ç†ä¸æŠ•å½±

é¦–å…ˆå¯¹è¾“å…¥å› å­è¿›è¡Œå½’ä¸€åŒ–å’Œé—¨æ§çº¿æ€§æŠ•å½±ï¼š

$$\begin{aligned} Z'_{left} &= \text{LN}(Z_{left}) \\ Z'_{right} &= \text{LN}(Z_{right}) \end{aligned}$$

ç”Ÿæˆä¸­é—´å˜é‡ $A$ å’Œ $B$ï¼ˆåŒ…å«é—¨æ§æœºåˆ¶ï¼‰ï¼š

$$\begin{aligned} A_{left} &= \text{Linear}_{a}(Z'_{left}) \odot \sigma(\text{Linear}_{g\_a}(Z'_{left})) \\ B_{right} &= \text{Linear}_{b}(Z'_{right}) \odot \sigma(\text{Linear}_{g\_b}(Z'_{right})) \end{aligned}$$

#### 1.2 è·¨ç§©æ··åˆ (Rank Mixing)

ä¸ºäº†å…è®¸ä¸åŒç§©ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ï¼Œå¯¹ç§©ç»´åº¦ $R$ è¿›è¡Œçº¿æ€§å˜æ¢ï¼š

$$\begin{aligned} \tilde{A} &= A_{left} W_{mix\_a}^T \\ \tilde{B} &= B_{right} W_{mix\_b}^T \end{aligned}$$

å…¶ä¸­ $W_{mix} \in \mathbb{R}^{R \times R}$ã€‚

#### 1.3 å› å­åŒ–èšåˆ (Factorized Aggregation)

è¿™æ˜¯ä¼˜åŒ–çš„æ ¸å¿ƒã€‚ä»¥ **å‡ºè¾¹ (Outgoing)** ä¸ºä¾‹ï¼Œä»£ç è¿‘ä¼¼è®¡ç®—äº†æ‰€æœ‰ä¸­é—´èŠ‚ç‚¹ $k$ çš„èšåˆä¿¡æ¯ï¼š

$$\bar{B} = \frac{1}{L} \sum_{k=1}^{L} \tilde{B}_{k}$$

è¿™é‡Œ $\bar{B} \in \mathbb{R}^{1 \times R \times C_{hidden}}$ æ˜¯å¯¹åºåˆ—ç»´åº¦çš„å‡å€¼èšåˆã€‚

æ›´æ–°å·¦å› å­çš„å…¬å¼ä¸ºï¼š

$$U_{left} = \tilde{A} \odot \bar{B}$$

(æ³¨ï¼šå¯¹äºå…¥è¾¹ Incomingï¼Œæ“ä½œæ˜¯å¯¹ç§°çš„ï¼šèšåˆ $\tilde{A}$ å¹¶æ›´æ–° $\tilde{B}$)ã€‚

#### 1.4 è¾“å‡ºæŠ•å½±

æœ€ç»ˆè¾“å‡ºç»è¿‡å½’ä¸€åŒ–ã€æŠ•å½±ã€é—¨æ§å’Œ Dropoutï¼š

$$\begin{aligned} O_{left} &= \text{Linear}_{out}(\text{LN}(U_{left})) \\ Gate_{left} &= \sigma(\text{Linear}_{gate}(Z_{left})) \\ Z_{out} &= \text{Dropout}(O_{left} \odot Gate_{left}) \end{aligned}$$

------

### 2. åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (Chunked Triangle Attention)

è¯¥æ¨¡å—å¯¹åº”ç±» `ChunkedTriangleAttention`ã€‚å®ƒè§£å†³äº†æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶ä¸­ $O(L^2)$ çš„æ˜¾å­˜å ç”¨é—®é¢˜ã€‚

#### 2.1 ç‰¹å¾èšåˆ (Feature Aggregation)

ç”±äºè¾“å…¥æ˜¯å› å­åŒ–çš„å½¢å¼ï¼Œé¦–å…ˆå°†å…¶èšåˆä¸ºä¼ªé…å¯¹ç‰¹å¾ï¼ˆPseudo-pair featuresï¼‰ä»¥è®¡ç®— Query, Key, Valueï¼š

$$Z_{pair} = \text{LN}\left(\sum_{r=1}^{R} Z_{left, r} + \sum_{r=1}^{R} Z_{right, r}\right)$$

å…¶ä¸­ $Z_{pair} \in \mathbb{R}^{L \times C}$ã€‚

#### 2.2 Q, K, V æŠ•å½±

$$\begin{aligned} Q &= Z_{pair} W_Q \\ K &= Z_{pair} W_K \\ V &= Z_{pair} W_V \end{aligned}$$

è¿™ä¸‰ä¸ªçŸ©é˜µè¢«é‡å¡‘ä¸º $\mathbb{R}^{L \times H \times d_h}$ã€‚

#### 2.3 åç½®è®¡ç®— (Bias Calculation)

ä¸‰è§’æ³¨æ„åŠ›é€šå¸¸åŒ…å«ä»é…å¯¹è¡¨ç¤ºä¸­å¯¼å‡ºçš„åç½®é¡¹ $b_{ij}$ã€‚ä»£ç ä¸­é€šè¿‡å·¦å› å­å¯¼å‡ºï¼š

$$\text{BiasFeatures} = \sum_{r=1}^{R} Z_{left, r}$$

$$B_{bias} = \text{Linear}_{bias}(\text{BiasFeatures})$$

$B_{bias}$ å¹¿æ’­åå½¢çŠ¶ä¸º $\mathbb{R}^{H \times L \times L}$ã€‚

#### 2.4 åˆ†å—æ³¨æ„åŠ›æœºåˆ¶ (Chunked Attention Mechanism)

ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæ³¨æ„åŠ›åˆ†æ•°ä¸ä¸€æ¬¡æ€§è®¡ç®—å®Œæ•´çš„ $L \times L$ çŸ©é˜µï¼Œè€Œæ˜¯æ²¿ Query çš„åºåˆ—ç»´åº¦åˆ‡åˆ†ä¸ºå¤§å°ä¸º $S$ ($chunk\_size$) çš„å—ã€‚

å¯¹äºç¬¬ $m$ ä¸ªå—ï¼ˆç´¢å¼•èŒƒå›´ $[i, i+S]$ï¼‰ï¼š

$$\begin{aligned} \text{Scores}^{(m)} &= \frac{Q_{[i:i+S]} K^T}{\sqrt{d_h}} + B_{bias[i:i+S]} \\ \text{Attn}^{(m)} &= \text{Softmax}(\text{Scores}^{(m)}) \\ O^{(m)} &= \text{Attn}^{(m)} V \end{aligned}$$

æœ€ç»ˆå°†æ‰€æœ‰å—çš„è¾“å‡ºæ‹¼æ¥ï¼š

$$O = \text{Concat}(O^{(1)}, O^{(2)}, \dots)$$

#### 2.5 è¾“å‡ºåˆ†å‘

æ³¨æ„åŠ›è¾“å‡ºç»è¿‡æŠ•å½±å’Œé—¨æ§åï¼Œé‡æ–°åˆ†é…å›å› å­åŒ–è¡¨ç¤ºï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå‡åˆ†ç»™ç§©ç»´åº¦ï¼‰ï¼š



$$\begin{aligned} Y &= \text{Linear}_{out}(O) \odot \sigma(\text{Linear}_{gate}(Z_{pair})) \\ Z_{left}^{new} &= \frac{1}{R} \cdot Y \quad (\text{Broadcast over rank}) \end{aligned}$$


### 2. Factorized Pair Transform Network 

 `factorized_pair_transform.py` å®ç°äº†ä¸€ä¸ª**å› å­åŒ–é…å¯¹å˜æ¢ç½‘ç»œ (Factorized Pair Transform Network)** ï¼Œä»¥ä¸‹æ˜¯**å› å­åŒ–é…å¯¹å˜æ¢ç½‘ç»œ (Factorized Pair Transform Network)** çš„æ•°å­¦é€»è¾‘ã€‚

è¯¥æ¨¡å—çš„ä¸»è¦åŠŸèƒ½æ˜¯å°† AlphaFold2 ä¸­çš„ `PairTransformNet`ï¼ˆåŒ…å«ä¸‰è§’æ›´æ–°å’Œä¸‰è§’æ³¨æ„åŠ›ï¼‰é€‚é…ä¸ºå› å­åŒ–å½¢å¼ã€‚æ ¸å¿ƒè®¾è®¡æ¨¡å¼æ˜¯**â€œè®¡ç®—å› å­æ›´æ–° $\to$ èšåˆä¸ºå…¨å±€æ›´æ–° $\to$ å‡åŒ€åˆ†é…å›å› å­â€**ï¼Œè¿™ç§æœºåˆ¶åœ¨ä¿æŒä½ç§©ç»“æ„çš„åŒæ—¶ï¼Œä¿ƒè¿›äº†ä¸åŒç§©ä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€‚

------

### 1. æ€»ä½“æ¶æ„ (General Architecture)

ç½‘ç»œç”± $N$ ä¸ªå †å çš„ **å› å­åŒ–é…å¯¹å˜æ¢å±‚ (FactorizedPairTransformLayer)** ç»„æˆã€‚

**è¾“å…¥/è¾“å‡ºçŠ¶æ€**:

- $\mathbf{F}_L^{(l)} \in \mathbb{R}^{L \times R \times C}$: ç¬¬ $l$ å±‚çš„å·¦å› å­å¼ é‡ã€‚
- $\mathbf{F}_R^{(l)} \in \mathbb{R}^{L \times R \times C}$: ç¬¬ $l$ å±‚çš„å³å› å­å¼ é‡ã€‚
- $R$: å› å­åŒ–ç§© (Rank)ã€‚

æ¯ä¸€å±‚çš„å¤„ç†æµç¨‹åŒ…å« 5 ä¸ªé¡ºåºå­æ¨¡å—ï¼š

1. å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (å‡ºè¾¹)
2. å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (å…¥è¾¹)
3. åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (èµ·å§‹èŠ‚ç‚¹)
4. åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (ç»“æŸèŠ‚ç‚¹)
5. é…å¯¹è¿‡æ¸¡ (Pair Transition)

------

### 2. é€šç”¨æ›´æ–°æœºåˆ¶ (The Update-Aggregate-Distribute Mechanism)

ä»£ç ä¸­æ‰€æœ‰å­æ¨¡å—ï¼ˆé™¤äº† Pair Transitionï¼‰éƒ½éµå¾ªç›¸åŒçš„æ®‹å·®è¿æ¥æ¨¡å¼ã€‚ä¸ºäº†æ•°å­¦ä¸Šçš„ç®€æ´ï¼Œå®šä¹‰ä¸€ä¸ªé€šç”¨ç®—å­ $\text{ApplyBlock}$ï¼š

**è®¾å­æ¨¡å—æ“ä½œä¸º $\Phi(\mathbf{F}_L, \mathbf{F}_R)$ï¼Œè¿”å›æ›´æ–°é‡ $(\Delta \mathbf{F}_L, \Delta \mathbf{F}_R)$ã€‚**

**èšåˆä¸åˆ†é…æ­¥éª¤**:

1. **æ‰§è¡Œæ“ä½œ**:

   $$(\Delta \mathbf{F}_L, \Delta \mathbf{F}_R) = \Phi(\mathbf{F}_L, \mathbf{F}_R)$$

2. ç§©èšåˆ (Rank Aggregation):

   å°†å·¦ã€å³å› å­çš„æ›´æ–°é‡æ²¿ç§©ç»´åº¦æ±‚å’Œï¼Œå¾—åˆ°å…¨å±€åºåˆ—æ›´æ–°é‡ $\Delta \mathbf{z} \in \mathbb{R}^{L \times C}$ï¼š

   $$\Delta \mathbf{z} = \sum_{r=1}^{R} \Delta \mathbf{F}_{L, \cdot, r, \cdot} + \sum_{r=1}^{R} \Delta \mathbf{F}_{R, \cdot, r, \cdot}$$

3. **Dropout**:

   $$\Delta \mathbf{z}' = \text{Dropout}(\Delta \mathbf{z})$$

4. å‡åŒ€åˆ†é…æ®‹å·® (Distribute Residual):

   å°†èšåˆåçš„æ›´æ–°é‡å‡åŒ€åˆ†é…å›æ‰€æœ‰ç§©ï¼Œå®ç°ä¿¡æ¯åŒæ­¥ï¼š

   $$\mathbf{F}_{L, \cdot, r, \cdot} \leftarrow \mathbf{F}_{L, \cdot, r, \cdot} + \frac{1}{R} \Delta \mathbf{z}'$$

   $$\mathbf{F}_{R, \cdot, r, \cdot} \leftarrow \mathbf{F}_{R, \cdot, r, \cdot} + \frac{1}{R} \Delta \mathbf{z}'$$

------

### 3. å­æ¨¡å—é€»è¾‘è¯¦è§£

#### 3.1 å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–° (Factorized Triangle Multiplicative Update)

åˆ©ç”¨å‰ä¸€ä¸ªæ–‡ä»¶å®šä¹‰çš„ `FactorizedTriangleMultiplication` æ¨¡å—ã€‚

**å‡ºè¾¹æ›´æ–° (Outgoing):**

$$\Phi_{\text{mul\_out}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

- æ³¨ï¼šåœ¨ `FactorizedTriangleMultiplicationOutgoing` ä¸­ï¼Œé€šå¸¸ $\Delta \mathbf{F}_R = \mathbf{0}$ï¼Œå› æ­¤èšåˆæ­¥éª¤ä¸»è¦æ”¶é›† $\Delta \mathbf{F}_L$ çš„ä¿¡æ¯ã€‚

**å…¥è¾¹æ›´æ–° (Incoming):**

$$\Phi_{\text{mul\_in}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

#### 3.2 åˆ†å—ä¸‰è§’æ³¨æ„åŠ› (Chunked Triangle Attention)

åˆ©ç”¨å‰ä¸€ä¸ªæ–‡ä»¶å®šä¹‰çš„ `ChunkedTriangleAttention` æ¨¡å—ã€‚

**èµ·å§‹èŠ‚ç‚¹æ³¨æ„åŠ› (Starting Node):**

$$\Phi_{\text{att\_start}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

- æ­¤å¤„ä½¿ç”¨ **Row-wise Dropout**ã€‚

**ç»“æŸèŠ‚ç‚¹æ³¨æ„åŠ› (Ending Node):**

$$\Phi_{\text{att\_end}}(\mathbf{F}_L, \mathbf{F}_R) \implies \text{ApplyBlock}$$

- æ­¤å¤„ä½¿ç”¨ **Column-wise Dropout**ã€‚

#### 3.3 å› å­åŒ–é…å¯¹è¿‡æ¸¡ (Factorized Pair Transition)

æ ‡å‡†çš„ Pair Transition æ˜¯ä¸€ä¸ª MLPï¼Œä½œç”¨äº $L \times L \times C$ å¼ é‡çš„æ¯ä¸ªå…ƒç´ ã€‚ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæ­¤å®ç°å¯¹èšåˆåçš„ $L \times C$ ç‰¹å¾è¿›è¡Œæ“ä½œã€‚

1. **ç‰¹å¾èšåˆ**:

   $$\mathbf{z}_{\text{agg}} = \sum_{r=1}^R \mathbf{F}_{L, \cdot, r, \cdot} + \sum_{r=1}^R \mathbf{F}_{R, \cdot, r, \cdot}$$

2. è¿‡æ¸¡å±‚ (Transition):

   åº”ç”¨ä¸¤å±‚ MLP (Linear $\to$ ReLU $\to$ Linear)ï¼š

   $$\mathbf{z}_{\text{trans}} = \text{Linear}_2(\text{ReLU}(\text{Linear}_1(\text{LayerNorm}(\mathbf{z}_{\text{agg}}))))$$

   *æ³¨ï¼šä»£ç å¤ç”¨äº†æ ‡å‡†çš„ `PairTransition` æ¨¡å—ï¼Œä½†è¾“å…¥è¢« reshape ä¸ºæ¨¡æ‹Ÿå½¢çŠ¶ä»¥é€‚åº”æ¥å£ï¼Œå®é™…è®¡ç®—æ˜¯é€ä½ç½® (position-wise) çš„ã€‚*

3. **æ®‹å·®åˆ†é…**:

   $$\mathbf{F}_{L, \cdot, r, \cdot} \leftarrow \mathbf{F}_{L, \cdot, r, \cdot} + \frac{1}{R} \mathbf{z}_{\text{trans}}$$

   $$\mathbf{F}_{R, \cdot, r, \cdot} \leftarrow \mathbf{F}_{R, \cdot, r, \cdot} + \frac{1}{R} \mathbf{z}_{\text{trans}}$$

### 4. æ©ç å¤„ç† (Masking)

åœ¨æ¯ä¸€å±‚çš„æœ€åï¼Œåº”ç”¨åºåˆ—æ©ç ä»¥ç¡®ä¿æ— æ•ˆä½ç½®ä¿æŒä¸º 0ï¼š

$$\mathbf{F}_{L, i, r} \leftarrow \mathbf{F}_{L, i, r} \cdot m_i$$

$$\mathbf{F}_{R, i, r} \leftarrow \mathbf{F}_{R, i, r} \cdot m_i$$

å…¶ä¸­ $m_i \in \{0, 1\}$ æ˜¯åºåˆ—æ©ç ã€‚

### æ€»ç»“ï¼šå†…å­˜å¤æ‚åº¦å¯¹æ¯”

å¯¹äºå±‚æ•° $N$ï¼Œåºåˆ—é•¿åº¦ $L$ï¼Œé€šé“æ•° $C$ï¼Œç§© $R$ï¼š

- **æ ‡å‡† PairTransform**: $O(N \cdot L^2 \cdot C)$
- **å› å­åŒ– PairTransform**: $O(N \cdot L \cdot R \cdot C)$

## Stage 3 æ›´æ–° (2026-01-12)

### è®­ç»ƒæ•ˆç‡ä¼˜åŒ–

Stage 3 å®ç°äº†å®Œæ•´çš„è®­ç»ƒä¼˜åŒ–æµç¨‹ï¼Œå¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦å’Œç¨³å®šæ€§ã€‚

 `progressive_training.py`æ¨¡å—ä¸»è¦å®ç°äº†ä¸¤ä¸ªå…³é”®çš„è®­ç»ƒä¼˜åŒ–æœºåˆ¶ï¼š**æ¸è¿›å¼è®­ç»ƒè°ƒåº¦ï¼ˆCurriculum Learningï¼‰ï¼Œåˆ†å—æŸå¤±è®¡ç®—ï¼ˆChunked Loss Computationï¼‰**ã€‚

ä»¥ä¸‹æ˜¯å„ä¸ªæ¨¡å—å¯¹åº”çš„æ•°å­¦å…¬å¼è¯´æ˜ã€‚

### ç¬¦å·å®šä¹‰

- $t$: å½“å‰è®­ç»ƒæ­¥æ•° (Current Step)
- $L_{curr}$: å½“å‰è®­ç»ƒä½¿ç”¨çš„åºåˆ—é•¿åº¦
- $L_{min}, L_{max}$: æœ€å°å’Œæœ€å¤§åºåˆ—é•¿åº¦
- $T_{warmup}$: é¢„çƒ­æ­¥æ•°
- $T_{growth}$: å¢é•¿é˜¶æ®µæ­¥æ•°
- $\mathbf{x}_i, \hat{\mathbf{x}}_i \in \mathbb{R}^3$: ç¬¬ $i$ ä¸ªæ®‹åŸºçš„çœŸå®åæ ‡å’Œé¢„æµ‹åæ ‡
- $M_{ij} \in \{0, 1\}$: æ©ç çŸ©é˜µï¼Œå½“æ®‹åŸº $i$ å’Œ $j$ éƒ½å­˜åœ¨æ—¶ä¸º 1

------

### 1. æ¸è¿›å¼è®­ç»ƒè°ƒåº¦å™¨ (Progressive Training Scheduler)

è¯¥æ¨¡å—é€šè¿‡è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰åŠ¨æ€è°ƒæ•´åºåˆ—è£å‰ªé•¿åº¦ï¼Œä½¿æ¨¡å‹å…ˆå­¦ä¹ çŸ­åºåˆ—çš„å±€éƒ¨ç‰¹å¾ï¼Œå†é€æ­¥è¿‡æ¸¡åˆ°é•¿åºåˆ—çš„å…¨å±€ç‰¹å¾ã€‚

#### 1.1 è¿›åº¦è®¡ç®— (Progress Calculation)

å®šä¹‰å½’ä¸€åŒ–çš„å¢é•¿è¿›åº¦ $p \in [0, 1]$ï¼š

$$p = \text{clamp}\left(\frac{t - T_{warmup}}{T_{growth}}, 0, 1\right)$$

#### 1.2 å¢é•¿ç­–ç•¥ (Growth Schedule)

å¯¹åº” `growth_schedule` å‚æ•°ï¼Œæ’å€¼ç³»æ•° $\alpha$ çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

- çº¿æ€§ (Linear):

  $$\alpha = p$$

- ä½™å¼¦ (Cosine):

  $$\alpha = \frac{1 - \cos(\pi \cdot p)}{2}$$

- æŒ‡æ•° (Exponential):

  $$\alpha = p^2$$

#### 1.3 å½“å‰åºåˆ—é•¿åº¦ (Current Max Length)

$$L_{curr}(t) = \begin{cases} L_{min} & \text{if } t < T_{warmup} \\ \lfloor L_{min} + \alpha \cdot (L_{max} - L_{min}) \rfloor & \text{if } T_{warmup} \le t < T_{total} \\ L_{max} & \text{otherwise} \end{cases}$$

------

### 2. åˆ†å—æŸå¤±è®¡ç®— (Chunked Loss Computation)

ä¸ºäº†é¿å…æ„å»ºå½¢çŠ¶ä¸º $B \times L \times L$ çš„å®Œæ•´è·ç¦»çŸ©é˜µï¼ˆæ˜¾å­˜æ¶ˆè€— $O(L^2)$ï¼‰ï¼Œä»£ç å°† $L$ ç»´åº¦åˆ‡åˆ†ä¸ºå¤§å°ä¸º $S$ çš„å—ï¼ˆChunkï¼‰ï¼Œå°†ç©ºé—´å¤æ‚åº¦é™ä½ä¸º $O(S \cdot L)$ã€‚

#### 2.1 è·ç¦»è®¡ç®—åŸºç¡€

å¯¹äºä»»æ„ä¸¤ç‚¹ $i, j$ï¼Œå…¶æ¬§å‡ é‡Œå¾—è·ç¦»ä¸ºï¼š

$$d_{ij} = \|\mathbf{x}_i - \mathbf{x}_j\|_2, \quad \hat{d}_{ij} = \|\hat{\mathbf{x}}_i - \hat{\mathbf{x}}_j\|_2$$

åœ¨åˆ†å—æ¨¡å¼ä¸‹ï¼Œå¤–å±‚å¾ªç¯éå†å—ç´¢å¼• $k$ï¼Œå†…å±‚è®¡ç®—å—å†…æ®‹åŸº $i \in [kS, (k+1)S]$ ä¸å…¨åºåˆ—æ®‹åŸº $j \in [1, L]$ çš„è·ç¦»ã€‚

#### 2.2 æŸå¤± 

> [!WARNING]
>
> *è™½ç„¶ä»£ç å‡½æ•°åä¸º `compute_fape_loss_chunked`ï¼Œä½†å…¶å®æˆ‘ç°åœ¨çš„é€»è¾‘ä»ç„¶æ˜¯åŸºäº**æ ‡é‡è·ç¦»çŸ©é˜µçš„è¯¯å·®**ï¼ˆDistance Deviationï¼‰ï¼Œè€Œé AlphaFold2 åŸæ–‡ä¸­åŸºäºå±€éƒ¨åæ ‡ç³»çš„ Frame Aligned Point Errorï¼ˆ**å¯èƒ½æœªæ¥ä¼šè€ƒè™‘å¼•å…¥**ï¼‰ã€‚ä»¥ä¸‹å…¬å¼å¯¹åº”ä»£ç çš„å®é™…é€»è¾‘ã€‚*

å¯¹äºæ¯ä¸ªå—ï¼Œè®¡ç®—è·ç¦»è¯¯å·®å¹¶è¿›è¡Œæˆªæ–­ï¼ˆClampingï¼‰ï¼š

$$E_{ij} = \min\left( \left| \hat{d}_{ij} - d_{ij} \right|, \tau \right)$$

å…¶ä¸­ $\tau$ æ˜¯æˆªæ–­é˜ˆå€¼ (clamp_distanceï¼Œé»˜è®¤ 10.0)ã€‚

æ€»æŸå¤±ä¸ºåŠ æƒå¹³å‡ï¼š

$$\mathcal{L}_{dist} = \frac{\sum_{k} \sum_{i \in \text{chunk}_k} \sum_{j=1}^L M_{ij} E_{ij}}{\sum_{i,j} M_{ij} + \epsilon}$$

#### 2.3 dRMSD æŸå¤± (åˆ†å—ç‰ˆ)

è·ç¦»å‡æ–¹æ ¹åå·®ï¼ˆDistance RMSDï¼‰ç”¨äºè¡¡é‡å†…éƒ¨å‡ ä½•ç»“æ„çš„ç›¸ä¼¼æ€§ï¼Œä¸ä¾èµ–äºå…¨å±€å åŠ ã€‚

$$SquaredError_{ij} = (\hat{d}_{ij} - d_{ij})^2$$

åˆ†å—ç´¯ç§¯è®¡ç®—åï¼š

$$\mathcal{L}_{dRMSD} = \sqrt{ \frac{\sum_{k} \sum_{i \in \text{chunk}_k} \sum_{j=1}^L M_{ij} (\hat{d}_{ij} - d_{ij})^2}{\sum_{i,j} M_{ij} + \epsilon} }$$

### 3. å†…å­˜ä¸æ•ˆç‡åˆ†æ

é€šè¿‡åˆ†å—ï¼Œè·ç¦»çŸ©é˜µçš„æ˜¾å­˜å ç”¨ä»äºŒæ¬¡æ–¹é™ä½ä¸ºçº¿æ€§ï¼š

$$Memory_{standard} \propto B \cdot L^2$$

$$Memory_{chunked} \propto B \cdot S \cdot L$$

å½“ $L=1024, S=64$ æ—¶ï¼Œå†…å­˜å ç”¨å‡å°‘çº¦ **16 å€**ï¼Œè¿™å…è®¸åœ¨æœ‰é™æ˜¾å­˜çš„ GPU ä¸Šè®­ç»ƒæ›´é•¿çš„è›‹ç™½è´¨åºåˆ—ã€‚

## Stage 3 V2 æ›´æ–° (2026-01-13)

Stage 3 V2 é€šè¿‡Sparse k-NN Pairsï¼Œæ”¯æŒæ›´é•¿åºåˆ—

 `sparse_pairs.py`æ¨¡å—å®ç°äº†ä¸€ç§**ç¨€ç– $k$-æœ€è¿‘é‚» ($k$-NN) é…å¯¹é€‰æ‹©æœºåˆ¶**ã€‚å…¶æ ¸å¿ƒç›®çš„æ˜¯å°†é•¿åºåˆ—è›‹ç™½è´¨å»ºæ¨¡ä¸­çš„é…å¯¹ç‰¹å¾ä»ç¨ å¯†çš„ $O(L^2)$ é™ä½åˆ°ç¨€ç–çš„ $O(L \cdot k)$ã€‚

ä»¥ä¸‹æ˜¯å„ä¸ªé€‰æ‹©ç­–ç•¥çš„æ•°å­¦é€»è¾‘ï¼š

### ç¬¦å·å®šä¹‰

- $L$: åºåˆ—é•¿åº¦
- $k$: æ¯ä¸ªæ®‹åŸºé€‰æ‹©çš„é‚»å±…æ•°é‡
- $\mathbf{x}_i \in \mathbb{R}^3$: ç¬¬ $i$ ä¸ªæ®‹åŸºçš„ç©ºé—´åæ ‡ (é€šå¸¸æ˜¯ $C_\alpha$ æˆ– $C_\beta$)
- $\mathcal{N}_i$: ç¬¬ $i$ ä¸ªæ®‹åŸºçš„é‚»å±…ç´¢å¼•é›†åˆ
- $w$: å±€éƒ¨çª—å£å¤§å° (`local_window`)

------

### 1. åŸºäºåæ ‡çš„é€‰æ‹© (Coordinate-based Selection)

è¿™æ˜¯é€šè¿‡è®¡ç®— 3D ç©ºé—´ä¸­çš„æ¬§å‡ é‡Œå¾—è·ç¦»æ¥å¯»æ‰¾æœ€è¿‘çš„é‚»å±…ã€‚

#### 1.1 è·ç¦»çŸ©é˜µè®¡ç®—

é¦–å…ˆè®¡ç®—æ‰€æœ‰æ®‹åŸºå¯¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦» $D \in \mathbb{R}^{L \times L}$ï¼š

$$d_{ij} = \|\mathbf{x}_i - \mathbf{x}_j\|_2 = \sqrt{\sum_{d=1}^3 (x_{i,d} - x_{j,d})^2}$$

#### 1.2 æ©ç å¤„ç† (Masking)

ä¸ºäº†å¤„ç† padding æˆ–ç¼ºå¤±çš„æ®‹åŸºï¼Œåº”ç”¨æ©ç  $M \in \{0, 1\}^L$ï¼š

$$d'_{ij} = \begin{cases} d_{ij} & \text{if } M_i \cdot M_j = 1 \\ \infty & \text{if } M_i \cdot M_j = 0 \end{cases}$$

#### 1.3 Top-k é€‰æ‹©

å¯¹äºæ¯ä¸ªæ®‹åŸº $i$ï¼Œé€‰æ‹©è·ç¦»æœ€å°çš„ $k$ ä¸ªç´¢å¼•ï¼š

$$\mathcal{N}_i^{coord} = \underset{j}{\text{argtopk}}(d'_{i, :}, k, \text{largest=False})$$

è¿™å¯¹åº”ä»£ç ä¸­çš„ `torch.topk(dist, k, largest=False)`ã€‚

------

### 2. åŸºäºåºåˆ—çš„é€‰æ‹© (Sequence-based Selection)

è¿™ç§æ–¹æ³•ä»…åŸºäºæ®‹åŸºåœ¨æ°¨åŸºé…¸åºåˆ—ä¸­çš„ç´¢å¼•è·ç¦» $|i-j|$ æ¥é€‰æ‹©é‚»å±…ï¼Œæ•æ‰ä¸€çº§åºåˆ—ä¸Šçš„å±€éƒ¨æ€§ã€‚

å¯¹äºæ®‹åŸº $i$ï¼Œé€‰æ‹©å…¶å‰ $k/2$ ä¸ªå’Œå $k/2$ ä¸ªæ®‹åŸºï¼š

$$\mathcal{N}_i^{seq} = \{j \mid \max(0, i - \lfloor \frac{k}{2} \rfloor) \le j \le \min(L-1, i + \lfloor \frac{k}{2} \rfloor), j \neq i \}$$

å¦‚æœè¾¹ç•Œå¤„çš„é‚»å±…æ•°é‡ä¸è¶³ $k$ï¼Œä»£ç é€»è¾‘ä¼šç”¨ $i$ è‡ªèº«è¿›è¡Œå¡«å……ä»¥ä¿æŒå¼ é‡å½¢çŠ¶å›ºå®šã€‚

------

### 3. æ··åˆç­–ç•¥ (Hybrid Strategy)

æ··åˆç­–ç•¥ç»“åˆäº†ç©ºé—´å‡ ä½•ä¿¡æ¯å’Œåºåˆ—å±€éƒ¨ä¿¡æ¯ã€‚å®ƒå°† $k$ åˆ†é…ä¸ºä¸¤éƒ¨åˆ†ï¼š$k_{coord} = \lfloor k/2 \rfloor$ å’Œ $k_{seq} = k - k_{coord}$ã€‚

$$\mathcal{N}_i^{hybrid} = \mathcal{N}_i^{coord}(k_{coord}) \cup \mathcal{N}_i^{seq}(k_{seq})$$

æœ€ç»ˆçš„é‚»å±…å¼ é‡æ˜¯ä¸¤è€…çš„æ‹¼æ¥ï¼ˆConcatenationï¼‰ï¼š

$$\text{Indices}_{hybrid} = \text{Concat}(\text{Indices}_{coord}, \text{Indices}_{seq})$$

------

### 4. å¼ºåˆ¶å±€éƒ¨é…å¯¹ (Mandatory Local Pairs)

ä¸ºäº†ä¿è¯æ¨¡å‹å§‹ç»ˆèƒ½çœ‹åˆ°å±€éƒ¨çš„äºŒçº§ç»“æ„ä¿¡æ¯ï¼Œä»£ç æä¾›äº†ä¸€ä¸ªé€‰é¡¹ `include_all_local`ï¼Œå¼ºåˆ¶åŒ…å«çª—å£ $w$ å†…çš„æ‰€æœ‰æ®‹åŸºã€‚

å±€éƒ¨ç´¢å¼•é›†åˆï¼š

$$\mathcal{N}_i^{local} = \{j \mid |i - j| \le w \}$$

æœ€ç»ˆé›†åˆä¸º $k$-NN é›†åˆä¸å±€éƒ¨é›†åˆçš„å¹¶é›†ï¼š

$$\mathcal{N}_i^{final} = \mathcal{N}_i^{knn} \cup \mathcal{N}_i^{local}$$

ç”±äºå¹¶é›†æ“ä½œä¼šå¯¼è‡´æ¯ä¸ªæ®‹åŸºçš„é‚»å±…æ•°é‡ä¸ä¸€è‡´ï¼Œä»£ç å®ç°ä¸­é€šå¸¸ä¼šå–æœ€å¤§é•¿åº¦å¹¶è¿›è¡Œ Paddingã€‚

------

### 5. å¤æ‚åº¦ä¸å†…å­˜åˆ†æ

é€šè¿‡è¿™ç§ç¨€ç–åŒ–ï¼Œå†…å­˜æ¶ˆè€—ä»åºåˆ—é•¿åº¦çš„å¹³æ–¹çº§é™ä½åˆ°çº¿æ€§çº§ï¼š

- å¯†é›†é…å¯¹ (Dense):

  $$\text{Memory} \propto O(L^2 \cdot C)$$

  å¯¹äº $L=4096$ï¼Œè¿™æ˜¯ä¸å¯è¡Œçš„ã€‚

- ç¨€ç–é…å¯¹ (Sparse):

  $$\text{Memory} \propto O(L \cdot k \cdot C)$$

  å¯¹äº $L=4096, k=32$ï¼Œå†…å­˜å‡å°‘çº¦ 120å€ã€‚

## Stage 4 æ›´æ–° (2026-01-14)

Stage 4 å®ç°äº†è®¡ç®—æ•ˆç‡ä¼˜åŒ–å’Œæ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚

### 1. Axial Attention 

 `axial_attention.py`ï¼Œæ¨¡å—å®ç°äº†**è½´å‘æ³¨æ„åŠ› (Axial Attention)**ï¼Œæ—¨åœ¨å°† AlphaFold2 ä¸­è®¡ç®—å¤æ‚åº¦ä¸º $O(L^3)$ çš„ä¸‰è§’æ³¨æ„åŠ›é™ä½ä¸º $O(L^2)$ã€‚æ­¤å¤–ï¼Œè¿˜å®ç°äº†ä¸€ä¸ªç»“åˆäº†ä½ç§©å› å­åŒ–çš„ç‰ˆæœ¬ã€‚

ä»¥ä¸‹æ˜¯æ ¸å¿ƒæ¨¡å—çš„æ•°å­¦å…¬å¼è¯´æ˜ã€‚

#### ç¬¦å·å®šä¹‰

- $X \in \mathbb{R}^{B \times L \times L \times C}$: è¾“å…¥çš„é…å¯¹å¼ é‡ (Batch, Row, Column, Channel)
- $L$: åºåˆ—é•¿åº¦
- $C$: é€šé“ç»´åº¦
- $H$: æ³¨æ„åŠ›å¤´æ•°
- $\mathcal{A}$: æ³¨æ„åŠ›å‡½æ•° (Attention)
- $\sigma$: Sigmoid æ¿€æ´»å‡½æ•°

------

#### 1. è½´å‘æ³¨æ„åŠ› (Axial Attention)

è½´å‘æ³¨æ„åŠ›å°†å…¨äºŒç»´æ³¨æ„åŠ›åˆ†è§£ä¸ºä¸¤ä¸ªé¡ºåºæ‰§è¡Œçš„ä¸€ç»´æ³¨æ„åŠ›ï¼š**è¡Œæ³¨æ„åŠ› (Row Attention)** å’Œ **åˆ—æ³¨æ„åŠ› (Column Attention)**ã€‚

##### 1.1 æ³¨æ„åŠ›é€šç”¨å½¢å¼

å¯¹äºè¾“å…¥åºåˆ— $Y$ï¼ˆå½¢çŠ¶ä¸º $N \times S \times C$ï¼‰ï¼Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—å¦‚ä¸‹ï¼š

$$\begin{aligned} Q &= Y W_Q, \quad K = Y W_K, \quad V = Y W_V \\ \text{Scores} &= \frac{Q K^T}{\sqrt{d_k}} \\ \text{Attn} &= \text{Softmax}(\text{Scores} + M) \\ O &= \text{Attn} \cdot V \end{aligned}$$

æœ€ç»ˆè¾“å‡ºç»è¿‡çº¿æ€§æŠ•å½±å’Œé—¨æ§ï¼š

$$Y_{out} = (O W_O) \odot \sigma(Y W_G)$$

##### 1.2 è¡Œæ³¨æ„åŠ› (Row-wise Attention)

å¯¹æ¯ä¸€è¡Œ $i$ï¼Œåœ¨æ‰€æœ‰åˆ— $j \in [1, L]$ ä¸Šè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚

å°† $X$ è§†ä¸º $B \cdot L$ ä¸ªé•¿åº¦ä¸º $L$ çš„åºåˆ—ï¼š

$$X_{row\_view} \in \mathbb{R}^{(B \cdot L) \times L \times C}$$

è®¡ç®—ï¼š

$$X^{(1)} = X + \text{Attention}_{row}(X_{row\_view})$$

å…¶ä¸­æ³¨æ„åŠ›å‘ç”Ÿåœ¨ç¬¬ 3 ç»´åº¦ï¼ˆåˆ—ç´¢å¼• $j$ï¼‰ä¸Šã€‚

##### 1.3 åˆ—æ³¨æ„åŠ› (Column-wise Attention)

å¯¹æ¯ä¸€åˆ— $j$ï¼Œåœ¨æ‰€æœ‰è¡Œ $i \in [1, L]$ ä¸Šè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚

é¦–å…ˆè½¬ç½®è¾“å…¥ï¼Œäº¤æ¢è¡Œå’Œåˆ—ç»´åº¦ï¼š

$$X_{transposed} = (X^{(1)})^T \in \mathbb{R}^{B \times L \times L \times C} \quad (\text{dim } 1 \leftrightarrow 2)$$

å°†è½¬ç½®åçš„å¼ é‡è§†ä¸º $B \cdot L$ ä¸ªé•¿åº¦ä¸º $L$ çš„åºåˆ—ï¼š

$$X^{(2)} = X^{(1)} + \left( \text{Attention}_{col}(X_{transposed}) \right)^T$$

å…¶ä¸­æ³¨æ„åŠ›å‘ç”Ÿåœ¨ç¬¬ 2 ç»´åº¦ï¼ˆè¡Œç´¢å¼• $i$ï¼‰ä¸Šã€‚

------

#### 2. å› å­åŒ–è½´å‘æ³¨æ„åŠ› (Factorized Axial Attention)

è¯¥æ¨¡å—å°è¯•åœ¨å› å­åŒ–è¡¨ç¤º $Z_{left}, Z_{right} \in \mathbb{R}^{L \times R \times C}$ ä¸Šåº”ç”¨è½´å‘æ³¨æ„åŠ›ï¼Œé€šè¿‡æ„å»ºâ€œä¼ªé…å¯¹â€æ¥è®¡ç®—äº¤äº’ï¼Œç„¶åæŠ•å½±å›å› å­ã€‚

å¯¹äºæ¯ä¸ªç§© $r \in [1, R]$ï¼š

##### 2.1 ä¼ªé…å¯¹æ„å»º (Pseudo-Pair Construction)

é€šè¿‡å¹¿æ’­åŠ æ³•æ„å»ºä¸´æ—¶çš„ $L \times L$ ç‰¹å¾ï¼ˆä»£ç ä¸­æ˜¾å¼æ‰©å±•äº†ç»´åº¦ï¼Œå®é™…åº”ç”¨ä¸­é€šå¸¸ä½¿ç”¨åˆ†å—ä»¥èŠ‚çœå†…å­˜ï¼Œæ­¤å¤„å±•ç¤ºé€»è¾‘å…¬å¼ï¼‰ï¼š

$$P^{(r)}_{ij} = Z_{left, i, r} + Z_{right, j, r}$$

å…¶ä¸­ $P^{(r)} \in \mathbb{R}^{L \times L \times C}$ã€‚

##### 2.2 è½´å‘æ³¨æ„åŠ›åº”ç”¨

ä»£ç ä¸­ä»…åº”ç”¨äº†è¡Œæ³¨æ„åŠ›ï¼ˆRow Attentionï¼‰ä½œä¸ºæ¼”ç¤ºæˆ–ç‰¹å®šå˜ä½“ï¼š

$$U^{(r)} = \text{Attention}_{row}(P^{(r)})$$

$U^{(r)}$ åŒ…å«äº†æ›´æ–°åçš„é…å¯¹ä¿¡æ¯ã€‚

##### 2.3 æŠ•å½±å›å› å­ (Projection Back to Factors)

é€šè¿‡å¹³å‡æ± åŒ–å°† $L \times L$ ä¿¡æ¯å‹ç¼©å› $L$ ç»´åº¦ï¼š

æ›´æ–°å·¦å› å­ (å¯¹åˆ—æ±‚å‡å€¼):

$$Z'_{left, i, r} = \frac{1}{L} \sum_{j=1}^L U^{(r)}_{ij}$$

æ›´æ–°å³å› å­ (å¯¹è¡Œæ±‚å‡å€¼):

$$Z'_{right, j, r} = \frac{1}{L} \sum_{i=1}^L U^{(r)}_{ij}$$

##### 2.4 æœ€ç»ˆåˆå¹¶

$$Z_{left}^{new} = \text{Concat}([Z'_{left, \cdot, 1}, \dots, Z'_{left, \cdot, R}], \text{dim}=rank)$$

$$Z_{right}^{new} = \text{Concat}([Z'_{right, \cdot, 1}, \dots, Z'_{right, \cdot, R}], \text{dim}=rank)$$

------

#### 3. å¤æ‚åº¦å¯¹æ¯”

| **æ–¹æ³•**             | **æ˜¾å­˜å¤æ‚åº¦**             | **è®¡ç®—å¤æ‚åº¦**                |
| -------------------- | -------------------------- | ----------------------------- |
| **æ ‡å‡†ä¸‰è§’æ³¨æ„åŠ›**   | $O(L^2)$                   | $O(L^3)$                      |
| **è½´å‘æ³¨æ„åŠ›**       | $O(L^2)$ (åˆ†å—å¯è¾¾ $O(L)$) | $O(L^2)$                      |
| **å› å­åŒ–è½´å‘æ³¨æ„åŠ›** | $O(L \cdot R)$             | $O(L^2 \cdot R)$ (å«é‡å»ºè¿‡ç¨‹) |

å¯¹äº $L=2048$ï¼Œè½´å‘æ³¨æ„åŠ›å°†è®¡ç®—é‡å‡å°‘äº†çº¦ 24 å€ã€‚

### 2. Advanced Gradient Checkpointing 

è‡ªé€‚åº”æ¢¯åº¦æ£€æŸ¥ç‚¹ç­–ç•¥
```python
class AdaptiveCheckpointManager:
    """
    æ ¹æ®åºåˆ—é•¿åº¦å’Œå¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´æ£€æŸ¥ç‚¹ç­–ç•¥

    çŸ­åºåˆ— + å……è¶³å†…å­˜: ä¸æ£€æŸ¥ç‚¹ (é€Ÿåº¦ä¼˜å…ˆ)
    ä¸­ç­‰åºåˆ—: é€‰æ‹©æ€§æ£€æŸ¥ç‚¹ (å¹³è¡¡)
    é•¿åºåˆ— + ç´§å¼ å†…å­˜: å…¨æ£€æŸ¥ç‚¹ (å†…å­˜ä¼˜å…ˆ)
    """
    def get_adaptive_config(seq_len, available_memory_gb):
        if seq_len < 256 and available_memory_gb > 10:
            return CheckpointConfig(enabled=False)  # æ— éœ€æ£€æŸ¥ç‚¹
        elif seq_len < 512:
            return CheckpointConfig(
                checkpoint_triangles=True,  # åªæ£€æŸ¥ç‚¹ä¸‰è§’æ“ä½œ
            )
        else:
            return CheckpointConfig(
                checkpoint_structure=True,  # æ£€æŸ¥ç‚¹æ‰€æœ‰
                checkpoint_pairs=True,
                checkpoint_triangles=True,
            )
```

### 3. Model Compression 

å±‚å‚æ•°å…±äº« (Universal Transformer)é£æ ¼

 `model_compression.py`æ¨¡å—å®ç°äº†ä¸€ç³»åˆ—**å‚æ•°é«˜æ•ˆçš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ (Parameter-Efficient Model Compression)**ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**å±‚å…±äº« (Layer Sharing)** å’Œ **ç“¶é¢ˆç»“æ„ (Bottleneck Architectures)** æ¥åœ¨å‡å°‘å‚æ•°é‡çš„åŒæ—¶ä¿æŒç½‘ç»œçš„æ·±åº¦å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

ä»¥ä¸‹æ˜¯å„ä¸ªæ ¸å¿ƒç»„ä»¶çš„æ•°å­¦é€»è¾‘ã€‚

#### ç¬¦å·å®šä¹‰

- $x^{(l)}$: ç¬¬ $l$ å±‚çš„è¾“å…¥å¼ é‡ã€‚
- $f_\theta(\cdot)$: å‚æ•°ä¸º $\theta$ çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆé€šå¸¸åŒ…å« Attention, MLP, Norm ç­‰ï¼‰ã€‚
- $L$: ç½‘ç»œçš„æ€»å±‚æ•°ã€‚
- $C$: è¾“å…¥ç‰¹å¾ç»´åº¦ (Channels)ã€‚
- $C_{bot}$: ç“¶é¢ˆå±‚ç»´åº¦ ($C / r$)ã€‚

------

##### 1. é€šç”¨å±‚å…±äº« (Universal Layer Sharing)

è¯¥ç­–ç•¥å¯¹åº”ç±» `SharedLayerModule`ã€‚è¿™æ˜¯ **Universal Transformer** çš„æ ¸å¿ƒæœºåˆ¶ã€‚ç½‘ç»œåœ¨æ·±åº¦æ–¹å‘ä¸Šåº”ç”¨é€’å½’ï¼ˆRecurrenceï¼‰ï¼Œå³æ¯ä¸€å±‚å¤ç”¨å®Œå…¨ç›¸åŒçš„å‚æ•° $\theta$ã€‚

å¯¹äº $l = 1, \dots, L$ï¼š

$$x^{(l)} = f_{\theta}(x^{(l-1)})$$

- **å‚æ•°å¤æ‚åº¦**: $O(1 \times |\theta|)$ï¼Œä¸ç½‘ç»œæ€»æ·±åº¦ $L$ æ— å…³ã€‚
- **ç‰©ç†æ„ä¹‰**: å¯ä»¥çœ‹ä½œæ˜¯åœ¨æ—¶é—´æ­¥ä¸Šå±•å¼€çš„ RNNï¼Œä½†åœ¨ç©ºé—´ï¼ˆæ·±åº¦ï¼‰ç»´åº¦ä¸Šæ“ä½œï¼Œæ—¨åœ¨å¯»æ‰¾ä¸åŠ¨ç‚¹æˆ–é€æ­¥ç»†åŒ–è¡¨ç¤ºã€‚

------

##### 2. äº¤æ›¿å±‚å…±äº« (Alternating Layer Sharing)

è¯¥ç­–ç•¥å¯¹åº”ç±» `AlternatingSharedLayers`ã€‚è¿™æ˜¯ **ALBERT (A Lite BERT)** ä¸­ä½¿ç”¨çš„ä¸€ç§å˜ä½“ã€‚å®ƒå°†å‚æ•°åˆ†ä¸ºä¸¤ç»„ï¼šå¥‡æ•°å±‚å‚æ•° $\theta_{odd}$ å’Œå¶æ•°å±‚å‚æ•° $\theta_{even}$ã€‚

å¯¹äº $l = 0, \dots, L-1$ï¼š

$$x^{(l+1)} = \begin{cases} f_{\theta_{even}}(x^{(l)}) & \text{if } l \equiv 0 \pmod 2 \\ f_{\theta_{odd}}(x^{(l)}) & \text{if } l \equiv 1 \pmod 2 \end{cases}$$

- **å‚æ•°å¤æ‚åº¦**: $O(2 \times |\theta|)$ã€‚
- **å‹ç¼©ç‡**: ç›¸æ¯”æ ‡å‡†ç½‘ç»œï¼Œå‹ç¼©ç‡çº¦ä¸º $L/2$ã€‚

------

##### 3. åˆ†å—å±‚å…±äº« (Block-wise Layer Sharing)

è¯¥ç­–ç•¥å¯¹åº”ç±» `BlockSharedLayers`ã€‚ç½‘ç»œè¢«åˆ’åˆ†ä¸º $K$ ä¸ªå— (Block)ï¼Œæ¯ä¸ªå—åŒ…å« $M$ ä¸ªå­å±‚ ($L = K \times M$)ã€‚å—ä¸å—ä¹‹é—´å‚æ•°ä¸åŒï¼Œä½†å—å†…çš„ $M$ æ¬¡è¿­ä»£å…±äº«å‚æ•°ã€‚

ä»¤ $\theta_k$ ä¸ºç¬¬ $k$ ä¸ªå—çš„å‚æ•°ã€‚å¯¹äºç¬¬ $k$ ä¸ªå—ä¸­çš„ç¬¬ $m$ æ¬¡è¿­ä»£ï¼š

$$x^{(k, m+1)} = f_{\theta_k}(x^{(k, m)}), \quad m \in [0, M-1]$$

- **å‚æ•°å¤æ‚åº¦**: $O(K \times |\theta|)$ï¼Œå…¶ä¸­ $K \ll L$ã€‚
- å…è®¸ç½‘ç»œåœ¨ä¸åŒé˜¶æ®µï¼ˆå¦‚æµ…å±‚ç‰¹å¾ vs æ·±å±‚è¯­ä¹‰ï¼‰æ‹¥æœ‰ä¸åŒçš„å¤„ç†é€»è¾‘ï¼ŒåŒæ—¶åœ¨å±€éƒ¨ä¿æŒå‚æ•°æ•ˆç‡ã€‚

------

##### 4. ç“¶é¢ˆå±‚ (Bottleneck Layer)

è¯¥ç­–ç•¥å¯¹åº”ç±» `BottleneckLayer`ã€‚ä¸ºäº†å‡å°‘è®¡ç®—é‡å’Œå‚æ•°é‡ï¼Œåœ¨æ‰§è¡Œæ˜‚è´µçš„æ“ä½œï¼ˆå¦‚å…¨è¿æ¥æˆ–æ³¨æ„åŠ›ï¼‰ä¹‹å‰ï¼Œå…ˆå°†ç»´åº¦æŠ•å½±åˆ°ä½ç»´ç©ºé—´ã€‚

è®¾è¾“å…¥ $x \in \mathbb{R}^{d_{in}}$ï¼Œç“¶é¢ˆæ¯”ç‡ä¸º $r$ã€‚

1. é™ç»´ (Project Down):

   $$x_{bot} = x W_{down} + b_{down}, \quad W_{down} \in \mathbb{R}^{d_{in} \times (d_{in}/r)}$$

2. æ ¸å¿ƒæ“ä½œ (Operation):

   $$h = \text{Operation}(x_{bot})$$

   æ³¨æ„ï¼šè¿™é‡Œçš„ Operation æ˜¯åœ¨ä½ç»´ç©ºé—´ $d_{in}/r$ è¿›è¡Œçš„ï¼Œå‚æ•°é‡å¤§å¹…å‡å°‘ã€‚

3. å‡ç»´ (Project Up):

   $$y = h W_{up} + b_{up}, \quad W_{up} \in \mathbb{R}^{(d_{in}/r) \times d_{in}}$$

4. æ®‹å·®ä¸å½’ä¸€åŒ– (Residual & Norm):

   $$Output = \text{LayerNorm}(x + y)$$

------

##### 5. æ·±åº¦å¯åˆ†ç¦»å±‚ (Depthwise Separable Layer)

è¯¥ç­–ç•¥å¯¹åº”ç±» `DepthwiseSeparableLayer`ã€‚å®ƒå°†æ ‡å‡†å·ç§¯åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼Œå¸¸ç”¨äºè½»é‡çº§ç½‘ç»œï¼ˆå¦‚ MobileNetï¼‰ã€‚

å‡è®¾è¾“å…¥ $X \in \mathbb{R}^{C \times L}$ï¼Œå·ç§¯æ ¸å¤§å°ä¸º $K$ã€‚

1. æ·±åº¦å·ç§¯ (Depthwise Conv): å¯¹æ¯ä¸ªé€šé“ç‹¬ç«‹è¿›è¡Œç©ºé—´æ»¤æ³¢ã€‚

   $$Y_{c, i} = \sum_{k} W_{c, k}^{depth} \cdot X_{c, i+k}$$

   å‚æ•°é‡: $C \times K$ã€‚

2. é€ç‚¹å·ç§¯ (Pointwise Conv): ä½¿ç”¨ $1 \times 1$ å·ç§¯æ··åˆé€šé“ä¿¡æ¯ã€‚

   $$Z_{j, i} = \sum_{c} W_{j, c}^{point} \cdot Y_{c, i}$$

   å‚æ•°é‡: $C_{out} \times C_{in} \times 1$ã€‚

**æ€»å‚æ•°é‡å¯¹æ¯”**:

- æ ‡å‡†å·ç§¯: $C_{out} \times C_{in} \times K$
- æ·±åº¦å¯åˆ†ç¦»: $C_{in} \times K + C_{out} \times C_{in}$
- å½“ $K$ è¾ƒå¤§æ—¶ï¼Œå‚æ•°å‡å°‘æ˜¾è‘—ã€‚

------

##### 6. å‚æ•°å‹ç¼©ç‡åˆ†æ

ä»£ç ä¸­çš„ `get_compression_ratio` è®¡ç®—å¦‚ä¸‹ï¼š

$$Ratio = \frac{\text{Baseline Parameters}}{\text{Compressed Parameters}} = \frac{L \times |\theta|_{base}}{|\Theta|_{shared}}$$

ä¾‹å¦‚ï¼Œå¯¹äº $L=12$ å±‚çš„ç½‘ç»œï¼š

- **Universal**: $|\Theta| = 1 \times |\theta| \Rightarrow Ratio = 12\times$
- **Alternating**: $|\Theta| = 2 \times |\theta| \Rightarrow Ratio = 6\times$
- **Block (Size=4)**: $|\Theta| = 3 \times |\theta| \Rightarrow Ratio = 4\times$

è¿™äº›æŠ€æœ¯ä½¿å¾—åœ¨æ˜¾å­˜å—é™çš„æƒ…å†µä¸‹ï¼ˆå¦‚ `model_compression.py` æ³¨é‡Šä¸­æåˆ°çš„ Stage 4ï¼‰ï¼Œå¯ä»¥è®­ç»ƒæ¯”å¸¸è§„ç½‘ç»œæ·±å¾—å¤šçš„æ¨¡å‹ã€‚

## Stage 5 æ›´æ–° (2026-01-15)

### ç³»ç»Ÿçº§ä¼˜åŒ–: åˆ†å¸ƒå¼è®­ç»ƒ

Stage 5 å®ç°äº†åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼Œå…è®¸è·¨å¤šGPUæ‰©å±•ã€‚

#### 1. Distributed Data Parallel (DDP) 

**åŠŸèƒ½**: æ ‡å‡†æ•°æ®å¹¶è¡Œè®­ç»ƒ
```python
class DistributedModelWrapper:
```

#### 2. Sequence Tensor Parallelism 

**åˆ›æ–°**: åœ¨åºåˆ—ç»´åº¦ä¸Šåˆ‡åˆ†
```python
class SequenceTensorParallel:
```

#### 3. Gradient Accumulation 

**åŠŸèƒ½**: å¤§æ‰¹é‡è®­ç»ƒæ”¯æŒ
```python
class GradientAccumulator:
```

**æ•ˆæœ**:

- å¤§æ‰¹é‡è®­ç»ƒæˆä¸ºå¯èƒ½
- æ›´ç¨³å®šçš„æ¢¯åº¦

#### 4. Stage 5 ç»¼åˆæ•ˆæœ

**åˆ†å¸ƒå¼è®­ç»ƒ**:
- å¤šGPU DDP
- Gradient Accumulation: å¤§æ‰¹é‡è®­ç»ƒ

---

## ğŸ“¦ æ–‡ä»¶æ¸…å•

### Stage 4-5 æ–°å¢æ–‡ä»¶ (4 ä¸ªæ ¸å¿ƒæ¨¡å—) ** Stage 4-5**

18. **`genie/model/axial_attention.py`** (600+ è¡Œ) ** Stage 4**
    - `AxialAttention`: è½´å‘æ³¨æ„åŠ› (è¡Œ+åˆ—åˆ†è§£)
    - `FactorizedAxialAttention`: å› å­åŒ–è½´å‘æ³¨æ„åŠ›
19. **`genie/training/gradient_checkpointing.py`** (400+ è¡Œ) **Stage 4**
    - `CheckpointConfig`: æ£€æŸ¥ç‚¹é…ç½®
    - `AdaptiveCheckpointManager`: è‡ªé€‚åº”æ£€æŸ¥ç‚¹ç®¡ç†
    - `CheckpointedSequential`: æ£€æŸ¥ç‚¹åºåˆ—æ¨¡å—
20. **`genie/model/model_compression.py`** (500+ è¡Œ) **Stage 4**
    - `CompressedStructureNet`: å‹ç¼©ç»“æ„ç½‘ç»œ
    - `SharedLayerModule`: å…±äº«å±‚æ¨¡å—
    - `AlternatingSharedLayers`: äº¤æ›¿å…±äº«å±‚
21. **`genie/training/distributed_training.py`** (500+ è¡Œ) **Stage 5**
    - `DistributedModelWrapper`: DDPå°è£…
    - `SequenceTensorParallel`: åºåˆ—å¼ é‡å¹¶è¡Œ
    - `GradientAccumulator`: æ¢¯åº¦ç´¯ç§¯
22. **`test_stage4_5.py`** (500+ è¡Œ) **Stage 4-5**
    - 6 ä¸ªç»¼åˆæµ‹è¯•
    - Stage 4-5 é›†æˆæµ‹è¯•

### Stage 3 V2 æ–°å¢æ–‡ä»¶ (2 ä¸ªæ ¸å¿ƒæ¨¡å—) **Stage 3 V2**

16. **`genie/model/sparse_pairs.py`** (500+ è¡Œ) **Stage 3 V2**
    - `SparseKNNPairSelector`: ç¨€ç– k-NN å¯¹é€‰æ‹©å™¨
    - ä¸‰ç§é€‰æ‹©ç­–ç•¥: coordinate / sequence / hybrid
    - æ”¯æŒè¶…é•¿åºåˆ— 
17. **`test_stage3_v2.py`** (400+ è¡Œ) **Stage 3 V2**
    - 4 ä¸ªç»¼åˆæµ‹è¯•
    - Ultra-long memory scaling

### Stage 3 æ–°å¢æ–‡ä»¶ (4 ä¸ªæ ¸å¿ƒæ¨¡å—) **Stage 3**

12. **`genie/training/progressive_training.py`** (400+ è¡Œ) **Stage 3**
    - `ProgressiveTrainingScheduler`: æ¸è¿›å¼è®­ç»ƒè°ƒåº¦å™¨
    - `ChunkedLossComputation`: åˆ†å—æŸå¤±è®¡ç®—
    - æ”¯æŒ linear/cosine/exponential å¢é•¿æ›²çº¿
    - FAPE å’Œ dRMSD æŸå¤±æ”¯æŒ
13. **`genie/training/mixed_precision.py`** (300+ è¡Œ) **Stage 3**
    - `MixedPrecisionTrainer`: æ··åˆç²¾åº¦è®­ç»ƒç®¡ç†å™¨
    - `SelectiveMixedPrecision`: é€‰æ‹©æ€§ç²¾åº¦æ§åˆ¶
    - FP16/BF16 æ”¯æŒ + åŠ¨æ€æŸå¤±ç¼©æ”¾
    - **æ”¶ç›Š**: 50% å†…å­˜èŠ‚çœ + 2-3x è®­ç»ƒåŠ é€Ÿ
14. **`genie/training/stage3_trainer.py`** (400+ è¡Œ) **Stage 3**
    - `Stage3TrainingManager`: ç»¼åˆè®­ç»ƒç®¡ç†å™¨
    - é›†æˆæ‰€æœ‰ Stage 3 ä¼˜åŒ–
    - ç»Ÿä¸€è®­ç»ƒæ¥å£
    - Checkpoint æ”¯æŒ
15. **`test_stage3_optimizations.py`** (400+ è¡Œ) **Stage 3**
    - 5 ä¸ªç»¼åˆæµ‹è¯•
    - Performance comparison

### Stage 2 æ–°å¢æ–‡ä»¶ (3 ä¸ªæ ¸å¿ƒæ¨¡å—)

9. **`genie/model/factorized_triangle_ops.py`** (500+ è¡Œ) **Stage 2**
   - `FactorizedTriangleMultiplicativeUpdate`: å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°
   - `FactorizedTriangleMultiplicationOutgoing`: Outgoing å˜ä½“
   - `FactorizedTriangleMultiplicationIncoming`: Incoming å˜ä½“
   - `ChunkedTriangleAttention`: åˆ†å—ä¸‰è§’æ³¨æ„åŠ›
   - `ChunkedTriangleAttentionStartingNode`: è¡Œæ–¹å‘æ³¨æ„åŠ›
   - `ChunkedTriangleAttentionEndingNode`: åˆ—æ–¹å‘æ³¨æ„åŠ›
10. **`genie/model/factorized_pair_transform.py`** (300+ è¡Œ) **Stage 2**
    - `FactorizedPairTransformLayer`: å•å±‚ pair è½¬æ¢
    - `FactorizedPairTransformNet`: å¤šå±‚ pair è½¬æ¢ç½‘ç»œ
    - å®Œæ•´çš„ Evoformer-style processing
    - æ‰€æœ‰æ“ä½œéƒ½åœ¨å› å­åŒ–è¡¨ç¤ºä¸Šè¿›è¡Œ
11. **`test_stage2_optimizations.py`** (400+ è¡Œ) **Stage 2**
    - 5 ä¸ªç»¼åˆæµ‹è¯•
    - å†…å­˜ç¼©æ”¾åˆ†æ
    - Stage 1 vs Stage 2 å¯¹æ¯”
    - **`test_stage2_quick.py`**: å¿«é€Ÿé›†æˆæµ‹è¯•

### æ ¸å¿ƒå®ç° (Stage 1 + V2 æ–‡ä»¶)

1. **`genie/model/factorized_pair_features.py`** (560 è¡Œ)
   - `FactorizedPairFeatureNet`: ä¸»è¦ç±»
   - `FactorizedRelPos`: å› å­åŒ–ä½ç½®ç¼–ç 
   - `FactorizedTemplate`: å› å­åŒ–æ¨¡æ¿ç‰¹å¾
   - `AdaptiveFactorizationRank`: åŠ¨æ€ rank è°ƒæ•´

2. **`genie/model/long_sequence_denoiser.py`** (400+ è¡Œ)
   - `LongSequenceDenoiser`: é›†æˆæ‰€æœ‰ä¼˜åŒ–
   - è‡ªåŠ¨é…ç½®å’Œå†…å­˜ä¼°ç®—
   - å®Œæ•´çš„æµ‹è¯•å‡½æ•°

3. **`genie/utils/adaptive_config.py`** (500+ è¡Œ)
   - `AdaptiveMHCConfig`: mHC é…ç½®
   - `DynamicBatchSize`: æ‰¹æ¬¡å¤§å°è®¡ç®—
   - `AdaptiveFactorizationRank`: Rank è®¡ç®—
   - `MemoryEstimator`: å†…å­˜ä¼°ç®—å·¥å…·

### æ–‡æ¡£ (4 ä¸ªæ–‡ä»¶)

4. **`EVALUATION_AND_IMPROVEMENTS.md`** (2000+ è¡Œ)
   - å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
   - 5 é˜¶æ®µä¼˜åŒ–è·¯çº¿å›¾
   - è¯¦ç»†çš„æŠ€æœ¯åˆ†æ
   - ä»£ç ç¤ºä¾‹å’ŒåŸºå‡†æµ‹è¯•
6. **`mhc_code_review_fixes.md`** (ä¹‹å‰åˆ›å»º)
   - Bug ä¿®å¤æ€»ç»“
   - Skip Connection è¯¦ç»†åˆ†æ
   - Sinkhorn ä¼˜åŒ–è¯´æ˜

## ğŸ“ åˆ›æ–°ç‚¹

### 1. End-to-End Factorization
**åˆ›æ–°**: å®Œå…¨é¿å… pair tensor å®ä¾‹åŒ–
```
ä¼ ç»Ÿ: s â†’ p[LÂ²] â†’ factorize â†’ factors[LÃ—rank]
     (éœ€è¦ 537 MB)          (éœ€è¦ 1 MB)

åˆ›æ–°: s â†’ factors[LÃ—rank] (ç›´æ¥ç”Ÿæˆ)
     (ä»…éœ€ 1 MB, èŠ‚çœ 537x!)
```

### 2. Adaptive Architecture
**åˆ›æ–°**: åºåˆ—é•¿åº¦æ„ŸçŸ¥çš„æ¨¡å‹é…ç½®
- çŸ­åºåˆ—: é«˜å®¹é‡ (è´¨é‡ä¼˜å…ˆ)
- é•¿åºåˆ—: ä½å®¹é‡ (æ•ˆç‡ä¼˜å…ˆ)
- åŠ¨æ€å¹³è¡¡: è‡ªåŠ¨è°ƒæ•´

### 3. Memory-First Design
**åˆ›æ–°**: ä»¥å†…å­˜ä¸ºç¬¬ä¸€çº¦æŸï¼ˆæœ€å¼€å§‹æ˜¯æ²¡æœ‰é«˜æ€§èƒ½gpuï¼Œæ‰€ä»¥åªèƒ½æ”¹ï¼‰

- æ¯ä¸ªä¼˜åŒ–éƒ½æœ‰å†…å­˜ä¼°ç®—
- é…ç½®è‡ªåŠ¨æ£€æŸ¥å’Œè­¦å‘Š
- æä¾›è¯¦ç»†çš„å†…å­˜åˆ†æå·¥å…·

## ğŸ’¡ å…³é”®

### æŠ€æœ¯äº’è¡¥
- Genie: æ ¸å¿ƒæ¶æ„
- Flash-IPA: å†…å­˜æ•ˆç‡
- AlphaFold2 Evoformer: å› å­åŒ–ä¸‰è§’æ“ä½œ (Stage 2)
- Curriculum Learning: æ¸è¿›å¼è®­ç»ƒ (Stage 3)
- Mixed Precision: è®­ç»ƒåŠ é€Ÿ (Stage 3)
- Sparse Attention: k-NN ç¨€ç–å¯¹é€‰æ‹© (Stage 3 V2)
- Axial Attention: è®¡ç®—æ•ˆç‡ä¼˜åŒ– (Stage 4) 
- Universal Transformer: æ¨¡å‹å‹ç¼© (Stage 4) 
- Distributed Training: å¤šGPUæ‰©å±• (Stage 5) 

---