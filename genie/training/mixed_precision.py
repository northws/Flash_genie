"""
æ··åˆç²¾åº¦è®­ç»ƒå·¥å…· (Stage 3)

æ­¤æ¨¡å—ä¸ºå†…å­˜é«˜æ•ˆçš„é•¿åºåˆ—è›‹ç™½è´¨ç»“æ„å»ºæ¨¡æä¾›æ··åˆç²¾åº¦ (FP16/BF16) è®­ç»ƒæ”¯æŒã€‚

æ ¸å¿ƒä¼˜åŠ¿:
- å†…å­˜å‡å°‘ 50% (FP16 vs FP32)
- è®­ç»ƒåŠ é€Ÿ 2-3x (åœ¨å…·æœ‰ Tensor Cores çš„ç°ä»£ GPU ä¸Š)
- é€‚å½“çš„æŸå¤±ç¼©æ”¾ä¸‹ç²¾åº¦æŸå¤±æå°

åŸºäº:
- PyTorch è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)
- NVIDIA Apex (å¯é€‰ï¼Œç”¨äºé«˜çº§åŠŸèƒ½)
- AlphaFold2 è®­ç»ƒç­–ç•¥

å®ç°:
- ä½¿ç”¨ torch.cuda.amp è¿›è¡Œè‡ªåŠ¨æ··åˆç²¾åº¦
- åŠ¨æ€æŸå¤±ç¼©æ”¾ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
- å¯¹å…³é”®è®¡ç®—é€‰æ‹©æ€§ä½¿ç”¨ FP32

ä½œè€…: Stage 3 å®ç° (2026-01-13)
"""

import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
from typing import Optional, Dict, Any, List
import contextlib


class MixedPrecisionTrainer:
    """
    å¸¦è‡ªåŠ¨æŸå¤±ç¼©æ”¾çš„æ··åˆç²¾åº¦è®­ç»ƒç®¡ç†å™¨ã€‚

    ç‰¹æ€§:
    1. ä½¿ç”¨ torch.cuda.amp çš„è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)
    2. åŠ¨æ€æŸå¤±ç¼©æ”¾ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
    3. æ··åˆç²¾åº¦ä¸‹çš„æ¢¯åº¦è£å‰ª
    4. è®­ç»ƒç»Ÿè®¡è·Ÿè¸ª

    ç”¨æ³•:
        trainer = MixedPrecisionTrainer(enabled=True, dtype=torch.float16)

        # è®­ç»ƒå¾ªç¯
        with trainer.autocast():
            loss = model(inputs)

        trainer.backward(loss)
        trainer.step(optimizer)
        trainer.update()
    """

    def __init__(
        self,
        enabled: bool = True,
        dtype: torch.dtype = torch.float16,  # æˆ– torch.bfloat16
        init_scale: float = 65536.0,
        growth_factor: float = 2.0,
        backoff_factor: float = 0.5,
        growth_interval: int = 2000,
        enabled_ops: Optional[List[str]] = None,  # åœ¨ FP16 ä¸­è¿è¡Œçš„è¿ç®—
        disabled_ops: Optional[List[str]] = None,  # ä¿æŒåœ¨ FP32 çš„è¿ç®—
    ):
        """
        åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒå™¨ã€‚

        Args:
            enabled: å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            dtype: æ··åˆç²¾åº¦çš„æ•°æ®ç±»å‹ (float16 æˆ– bfloat16)
            init_scale: åˆå§‹æŸå¤±ç¼©æ”¾å› å­
            growth_factor: ç¼©æ”¾å› å­ä¹˜æ•°
            backoff_factor: æº¢å‡ºæ—¶ç¼©æ”¾å› å­é™¤æ•°
            growth_interval: å¢é•¿ç¼©æ”¾å‰çš„æ­¥æ•°
            enabled_ops: ä»¥è¾ƒä½ç²¾åº¦è¿è¡Œçš„è¿ç®—
            disabled_ops: ä¿æŒåœ¨ FP32 çš„è¿ç®— (å¦‚ layer_norm, softmax)
        """
        self.enabled = enabled and torch.cuda.is_available()
        self.dtype = dtype

        if self.enabled:
            self.scaler = GradScaler(
                init_scale=init_scale,
                growth_factor=growth_factor,
                backoff_factor=backoff_factor,
                growth_interval=growth_interval,
                enabled=True,
            )
        else:
            self.scaler = None

        # åœ¨ FP16/BF16 ä¸ FP32 ä¸­è¿è¡Œçš„è¿ç®—
        self.enabled_ops = enabled_ops
        self.disabled_ops = disabled_ops or [
            "layer_norm",
            "softmax",
            "batch_norm",
            "group_norm",
        ]

        # è®­ç»ƒç»Ÿè®¡
        self.stats = {
            "scale": init_scale if self.enabled else 1.0,
            "overflows": 0,
            "step": 0,
        }

    @contextlib.contextmanager
    def autocast(self):
        """
        è‡ªåŠ¨æ··åˆç²¾åº¦çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

        ç”¨æ³•:
            with trainer.autocast():
                output = model(input)
        """
        if self.enabled:
            with autocast(dtype=self.dtype):
                yield
        else:
            yield

    def backward(self, loss: torch.Tensor):
        """
        å¸¦æŸå¤±ç¼©æ”¾çš„åå‘ä¼ æ’­ã€‚

        Args:
            loss: æŸå¤±å¼ é‡
        """
        if self.enabled:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()

    def step(self, optimizer: torch.optim.Optimizer):
        """
        å¸¦æ¢¯åº¦åç¼©æ”¾å’Œè£å‰ªçš„ä¼˜åŒ–å™¨æ­¥éª¤ã€‚

        Args:
            optimizer: PyTorch ä¼˜åŒ–å™¨
        """
        if self.enabled:
            # åç¼©æ”¾æ¢¯åº¦
            self.scaler.unscale_(optimizer)

            # æ¢¯åº¦è£å‰ª (å¯é€‰ï¼Œåœ¨åç¼©æ”¾ç©ºé—´ä¸­è¿›è¡Œ)
            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.scaler.step(optimizer)
        else:
            optimizer.step()

    def update(self):
        """æ›´æ–°æŸå¤±ç¼©æ”¾å™¨ (åœ¨ optimizer.step åè°ƒç”¨)ã€‚"""
        if self.enabled:
            old_scale = self.scaler.get_scale()
            self.scaler.update()
            new_scale = self.scaler.get_scale()

            # è·Ÿè¸ªæº¢å‡º
            if new_scale < old_scale:
                self.stats["overflows"] += 1

            self.stats["scale"] = new_scale
            self.stats["step"] += 1

    def state_dict(self) -> Dict[str, Any]:
        """è·å–ç”¨äºæ£€æŸ¥ç‚¹çš„çŠ¶æ€å­—å…¸ã€‚"""
        if self.enabled:
            return {
                "scaler": self.scaler.state_dict(),
                "stats": self.stats,
            }
        else:
            return {"stats": self.stats}

    def load_state_dict(self, state_dict: Dict[str, Any]):
        """ä»æ£€æŸ¥ç‚¹åŠ è½½çŠ¶æ€å­—å…¸ã€‚"""
        if self.enabled and "scaler" in state_dict:
            self.scaler.load_state_dict(state_dict["scaler"])
        if "stats" in state_dict:
            self.stats = state_dict["stats"]

    def get_scale(self) -> float:
        """è·å–å½“å‰æŸå¤±ç¼©æ”¾å› å­ã€‚"""
        if self.enabled:
            return self.scaler.get_scale()
        else:
            return 1.0

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ã€‚"""
        return self.stats.copy()

    def __repr__(self):
        if self.enabled:
            return (
                f"MixedPrecisionTrainer("
                f"enabled=True, "
                f"dtype={self.dtype}, "
                f"scale={self.get_scale():.0f}, "
                f"overflows={self.stats['overflows']})"
            )
        else:
            return "MixedPrecisionTrainer(enabled=False)"


class SelectiveMixedPrecision:
    """
    å…³é”®è¿ç®—çš„é€‰æ‹©æ€§æ··åˆç²¾åº¦ã€‚

    æŸäº›è¿ç®—å¯¹æ•°å€¼æ•æ„Ÿï¼Œåº”ä¿æŒåœ¨ FP32:
    - LayerNorm, BatchNorm (ç»Ÿè®¡)
    - Softmax (æ³¨æ„åŠ›)
    - æŸå¤±è®¡ç®—
    - ç¨³å®šæ€§æ¢¯åº¦è®¡ç®—

   æ­¤ç±»ä¸ºé€‰æ‹©æ€§ç²¾åº¦æä¾›è£…é¥°å™¨å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚
    """

    @staticmethod
    @contextlib.contextmanager
    def fp32_context():
        """å¼ºåˆ¶ä¸Šä¸‹æ–‡å†…çš„è¿ç®—ä¸º FP32ã€‚"""
        with autocast(enabled=False):
            yield

    @staticmethod
    def fp32_forward(module: nn.Module):
        """
        ä»¥ FP32 è¿è¡Œæ¨¡å—å‰å‘ä¼ æ’­çš„è£…é¥°å™¨ã€‚

        ç”¨æ³•:
            @SelectiveMixedPrecision.fp32_forward
            class MyLayerNorm(nn.LayerNorm):
                pass
        """
        original_forward = module.forward

        def forward_fp32(*args, **kwargs):
            with autocast(enabled=False):
                # å°†è¾“å…¥è½¬æ¢ä¸º FP32
                args = [arg.float() if torch.is_tensor(arg) else arg for arg in args]
                kwargs = {
                    k: v.float() if torch.is_tensor(v) else v for k, v in kwargs.items()
                }
                return original_forward(*args, **kwargs)

        module.forward = forward_fp32
        return module


def test_mixed_precision():
    """æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒå·¥å…·ã€‚"""
    print("=" * 80)
    print("æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒ")
    print("=" * 80)
    print()

    # å¦‚æœ CUDA å¯ç”¨åˆ™æµ‹è¯•
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡æ··åˆç²¾åº¦æµ‹è¯•")
        print("   (æ··åˆç²¾åº¦éœ€è¦ CUDA)")
        return

    # æµ‹è¯• 1: åŸºæœ¬è‡ªåŠ¨æ··åˆç²¾åº¦
    print("æµ‹è¯• 1: è‡ªåŠ¨æ··åˆç²¾åº¦")
    print("-" * 80)

    trainer = MixedPrecisionTrainer(enabled=True, dtype=torch.float16)

    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Linear(128, 128).cuda()
    optimizer = torch.optim.Adam(model.parameters())

    x = torch.randn(2, 128).cuda()
    y = torch.randn(2, 128).cuda()

    # åœ¨æ··åˆç²¾åº¦ä¸‹å‰å‘ä¼ æ’­
    with trainer.autocast():
        output = model(x)
        loss = nn.functional.mse_loss(output, y)

    print(f"  è¾“å…¥æ•°æ®ç±»å‹: {x.dtype}")
    print(f"  è¾“å‡ºæ•°æ®ç±»å‹ (åœ¨ autocast ä¸­): {output.dtype}")
    print(f"  æŸå¤±æ•°æ®ç±»å‹: {loss.dtype}")
    print(f"  âœ… Autocast å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 2: å¸¦ç¼©æ”¾çš„åå‘ä¼ æ’­
    print("æµ‹è¯• 2: å¸¦æŸå¤±ç¼©æ”¾çš„åå‘ä¼ æ’­")
    print("-" * 80)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    trainer.backward(loss)

    # æ£€æŸ¥æ¢¯åº¦
    has_grads = any(p.grad is not None for p in model.parameters())
    print(f"  æœ‰æ¢¯åº¦: {has_grads}")
    print(f"  æŸå¤±ç¼©æ”¾: {trainer.get_scale():.0f}")

    # æ­¥éª¤
    trainer.step(optimizer)
    trainer.update()

    stats = trainer.get_stats()
    print(f"  è®­ç»ƒæ­¥éª¤: {stats['step']}")
    print(f"  æº¢å‡ºæ¬¡æ•°: {stats['overflows']}")
    print(f"  âœ… åå‘ä¼ æ’­å’Œæ­¥éª¤å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 3: å†…å­˜å¯¹æ¯”
    print("æµ‹è¯• 3: å†…å­˜å¯¹æ¯”")
    print("-" * 80)

    torch.cuda.reset_peak_memory_stats()

    # FP32 æ¨¡å‹
    model_fp32 = nn.Linear(1024, 1024).cuda()
    x_fp32 = torch.randn(8, 1024).cuda()
    mem_fp32 = torch.cuda.max_memory_allocated() / (1024 ** 2)

    torch.cuda.reset_peak_memory_stats()

    # FP16 æ¨¡å‹
    model_fp16 = nn.Linear(1024, 1024).cuda().half()
    x_fp16 = torch.randn(8, 1024).cuda().half()
    mem_fp16 = torch.cuda.max_memory_allocated() / (1024 ** 2)

    print(f"  FP32 å†…å­˜: {mem_fp32:.2f} MB")
    print(f"  FP16 å†…å­˜: {mem_fp16:.2f} MB")
    print(f"  å†…å­˜å‡å°‘: {mem_fp32 / mem_fp16:.2f}x")
    print(f"  âœ… å†…å­˜å‡å°‘å·²ç¡®è®¤!")
    print()

    print("=" * 80)
    print("ğŸ‰ æ‰€æœ‰æ··åˆç²¾åº¦æµ‹è¯•é€šè¿‡!")
    print("=" * 80)


if __name__ == "__main__":
    test_mixed_precision()
