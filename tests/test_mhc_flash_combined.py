#!/usr/bin/env python3
"""
æµ‹è¯• mHC + Flash-IPA ç»„åˆæ¨¡å¼çš„å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½

è¿è¡Œæ­¤è„šæœ¬ä»¥éªŒè¯æ–°æ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…å’Œå·¥ä½œã€‚
"""

import sys
import torch

def test_imports():
    """æµ‹è¯•æ‰€æœ‰å¿…éœ€çš„å¯¼å…¥"""
    print("=" * 60)
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    print("=" * 60)
    
    try:
        from genie.model.mhc_flash_structure_net import mHCFlashStructureLayer, mHCFlashStructureNet
        print("âœ… mHCFlashStructureNet å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ mHCFlashStructureNet å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from genie.model.mhc_flash_denoiser import mHCFlashDenoiser
        print("âœ… mHCFlashDenoiser å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ mHCFlashDenoiser å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from genie.diffusion.diffusion import Diffusion
        print("âœ… Diffusion (æ›´æ–°ç‰ˆ) å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Diffusion å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True


def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•åŸºæœ¬åŠŸèƒ½...")
    print("=" * 60)
    
    try:
        from genie.model.mhc_flash_structure_net import mHCFlashStructureLayer
        from genie.flash_ipa.ipa import IPAConfig
        
        # åˆ›å»ºä¸€ä¸ªå°å‹æµ‹è¯•å±‚
        layer = mHCFlashStructureLayer(
            c_s=64,
            c_p=64,
            c_hidden_ipa=16,
            n_head=4,
            n_qk_point=2,
            n_v_point=4,
            ipa_dropout=0.1,
            n_structure_transition_layer=1,
            structure_transition_dropout=0.1,
            max_n_res=128,
            z_factor_rank=2,
            k_neighbors=8,
            mhc_expansion_rate=4,
            mhc_sinkhorn_iters=10,
            mhc_alpha_init=0.01,
            use_grad_checkpoint=False,
            use_flash_attn_3=False,
            is_first_layer=True,
            is_last_layer=False,
        )
        print("âœ… mHCFlashStructureLayer å®ä¾‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥å‚æ•°æ•°é‡
        num_params = sum(p.numel() for p in layer.parameters())
        print(f"   å‚æ•°æ•°é‡: {num_params:,}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_config_parsing():
    """æµ‹è¯•é…ç½®æ–‡ä»¶è§£æ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•é…ç½®è§£æ...")
    print("=" * 60)
    
    try:
        from genie.config import Config
        import os
        
        # æ£€æŸ¥ç¤ºä¾‹é…ç½®æ˜¯å¦å­˜åœ¨
        config_path = "runs/config_mhc_flash_combined.txt"
        if os.path.exists(config_path):
            config = Config(config_path)
            
            # éªŒè¯å…³é”®é…ç½®
            use_mhc = config.training.get('use_mhc_mode', False)
            use_flash = config.training.get('use_flash_mode', False)
            
            print(f"   use_mhc_mode: {use_mhc}")
            print(f"   use_flash_mode: {use_flash}")
            
            if use_mhc and use_flash:
                print("âœ… é…ç½®è§£ææˆåŠŸ - mHC + Flash-IPA åŒæ—¶å¯ç”¨")
                return True
            else:
                print("âš ï¸  é…ç½®æ–‡ä»¶æœªåŒæ—¶å¯ç”¨ä¸¤ç§æ¨¡å¼")
                return True
        else:
            print(f"âš ï¸  ç¤ºä¾‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return True
            
    except Exception as e:
        print(f"âŒ é…ç½®è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_flash_attention():
    """æ£€æŸ¥ Flash Attention å¯ç”¨æ€§"""
    print("\n" + "=" * 60)
    print("æ£€æŸ¥ Flash Attention å¯ç”¨æ€§...")
    print("=" * 60)
    
    try:
        from flash_attn import flash_attn_func
        print("âœ… Flash Attention 2 å¯ç”¨")
        
        try:
            from flash_attn import flash_attn_func_fa3
            print("âœ… Flash Attention 3 å¯ç”¨ (Hopper GPU)")
        except:
            print("â„¹ï¸  Flash Attention 3 ä¸å¯ç”¨ (éœ€è¦ Hopper GPU)")
            
        return True
    except ImportError:
        print("âŒ Flash Attention æœªå®‰è£…")
        print("   å®‰è£…å‘½ä»¤: pip install flash-attn --no-build-isolation")
        return False


def main():
    print("\n" + "=" * 60)
    print("mHC + Flash-IPA ç»„åˆæ¨¡å¼æµ‹è¯•")
    print("=" * 60)
    
    print(f"\nPython ç‰ˆæœ¬: {sys.version}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    results.append(("æ¨¡å—å¯¼å…¥", test_imports()))
    results.append(("åŸºæœ¬åŠŸèƒ½", test_basic_functionality()))
    results.append(("é…ç½®è§£æ", test_config_parsing()))
    results.append(("Flash Attention", check_flash_attention()))
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    all_passed = all(result[1] for result in results)
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥ä½¿ç”¨ mHC + Flash-IPA ç»„åˆæ¨¡å¼ã€‚")
        print("\nå¿«é€Ÿå¼€å§‹:")
        print("  python -m genie.train runs/config_mhc_flash_combined.txt")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())
