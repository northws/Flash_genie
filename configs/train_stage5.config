# =============================================================================
# Stage 5 Configuration - Distributed Training
# =============================================================================
# Stage: 1 + 2 + 3 + V2 + 4 + 5 (Full Stack)
# Max Length: L=16384-24576+
# Memory: ~32GB per GPU (8 GPUs)
# Speed Improvement: 8x (8 GPUs with DDP)
# Use Case: Extreme length sequences with multi-GPU setup
# =============================================================================

# Basic Training Settings
name stage5_distributed_training
seed 42
batchSize 1
gradientAccumulation 8
maximumNumResidues 16384

# Model Architecture
singleFeatureDimension 128
pairFeatureDimension 96
structureEncoderDepth 3
structureEncoderHeads 4

# *** STAGE 1: Factorized Pairs ***
useFactorizedPairs True
zFactorRank 2
pairFactorDim 64

# *** STAGE 2: Factorized Triangle Ops ***
useFactorizedTriangleOps True
triangleUpdateRank 4
triangleAttentionChunkSize 256
numTriangleLayers 4

# *** STAGE 3: Training Optimizations ***
useProgressiveTraining True
progressiveStartLength 2048
progressiveEndLength 16384
progressiveNumStages 4
progressiveEpochsPerStage 50

useChunkedLoss True
lossChunkSize 1024

useMixedPrecision True
precision 16

# *** STAGE 3 V2: Sparse k-NN ***
useSparseKNN True
kNeighbors 32
knnStrategy hybrid
knnUpdateFrequency 10

# *** STAGE 4: Advanced Optimizations ***
useAxialAttention True
axialRowChunkSize 256
axialColChunkSize 256

useGradientCheckpointing True
checkpointingStrategy adaptive
checkpointEveryNLayers 1

useModelCompression True
compressionStrategy universal
numSharedLayers 2
compressionRatio 4

# *** STAGE 5 KEY FEATURES: Distributed Training ***
useDistributed True
distributedBackend nccl         # NCCL for multi-GPU
distributedStrategy ddp         # DistributedDataParallel

# Sequence Parallelism: Split sequence dimension
useSequenceParallelism True
sequenceParallelDim 0          # Split along sequence dimension
numSequenceShards 4            # Split into 4 shards

# Gradient Accumulation for large effective batch
effectiveBatchSize 32          # 8 GPUs × 1 batch × 4 accumulation

# Communication Optimization
gradientCompressionRatio 0.01   # Compress gradients for efficiency
useGradientBucketing True       # Bucket gradients for all-reduce

# Learning Rate & Optimization
learningRate 3e-4               # Slightly higher for multi-GPU
warmupEpochs 150                # Longer warmup for stability
numEpochs 500
gradientClipVal 1.0
weightDecay 0.01

# Loss Settings
lossWeights {
    "translation": 1.0,
    "rotation": 1.0,
    "backbone": 1.0,
    "chi": 0.5,
    "mhc": 0.1
}

useMHCLoss True
mhcNumScales 4                  # More scales for longer sequences
mhcWeights [0.4, 0.3, 0.2, 0.1]

# Diffusion Settings
diffusionTimesteps 1000
diffusionSchedule cosine
noiseSchedule exponential

# Hardware Settings
numWorkers 8
accelerator gpu
devices 8                       # 8 GPUs

# Monitoring
logEveryNSteps 50
saveEveryNEpochs 10
validateEveryNEpochs 5

# ===== Launch with torchrun =====
# torchrun --nproc_per_node=8 train/train_ultralong.py \
#     --config configs/train_stage5.config
