"""
Long-Sequence Optimized Denoiser (Stage 2)

This module implements an optimized version of mHCFlashDenoiser with
all Stage 1, V2, and Stage 2 optimizations for efficient long-sequence training:

Stage 1 & V2:
1. Factorized Pair Features (512x memory reduction)
2. Adaptive mHC Configuration (dynamic expansion rates)
3. Dynamic Batch Sizing (constant memory usage)
4. Adaptive Factorization Rank (quality-memory tradeoff)
5. Factorized Pair Refinement (V2 - lightweight pair updates)

Stage 2:
6. Factorized Triangle Multiplicative Updates (O(L¬≥) ‚Üí O(L¬≤ √ó rank))
7. Chunked Triangle Attention (memory-bounded)
8. Factorized Pair Transform Network (optional full pair processing)

Capabilities:
- Stage 1: Single GPU (24GB): 512-768 residues
- Stage 2: Single GPU (24GB): 768-1024 residues
- Stage 2: 8 GPUs (192GB): 1536-2048 residues

Stage 2 Improvements (2026-01-13):
- Added FactorizedTriangleMultiplicativeUpdate (268x memory reduction)
- Added ChunkedTriangleAttention (16x memory reduction)
- Added FactorizedPairTransformNet (optional intensive pair processing)

Based on:
- Genie: https://arxiv.org/abs/2301.12485
- Flash-IPA: https://arxiv.org/abs/2505.11580
- mHC: https://arxiv.org/abs/2512.24880
"""

import torch
from torch import nn

from genie.model.single_feature_net import SingleFeatureNet
from genie.model.factorized_pair_features import (
    FactorizedPairFeatureNet,
    AdaptiveFactorizationRank,
    FactorizedPairRefinement
)
from genie.model.factorized_pair_transform import FactorizedPairTransformNet  # Stage 2
from genie.model.mhc_flash_structure_net import mHCFlashStructureNet
from genie.utils.adaptive_config import AdaptiveMHCConfig, DynamicBatchSize, MemoryEstimator


class LongSequenceDenoiser(nn.Module):
    """
    Optimized denoiser for long protein sequences (512-1024+ residues).

    Key Optimizations (Stage 1 + V2 + Stage 2):
    1. **Factorized Pair Features**: O(L¬≤) ‚Üí O(L√órank) memory
    2. **Adaptive mHC**: Dynamic expansion rates based on length
    3. **Flash-IPA**: Memory-efficient attention
    4. **Dynamic Config**: Automatically adjusts to sequence length
    5. **Pair Refinement (V2)**: Lightweight factorized pair updates
    6. **Factorized Triangle Ops (Stage 2)**: O(L¬≥) ‚Üí O(L¬≤ √ó rank) memory
    7. **Factorized Pair Transform (Stage 2)**: Optional intensive pair processing

    Architecture:
        Input ‚Üí SingleFeatureNet ‚Üí FactorizedPairFeatureNet
        ‚Üí [FactorizedPairRefinement] ‚Üí [FactorizedPairTransformNet (Stage 2)]
        ‚Üí mHCFlashStructureNet ‚Üí Output

    Memory Comparison (L=1024, batch=1):
        Standard mHCFlashDenoiser:  ~2.4 GB
        LongSequenceDenoiser (Stage 1): ~400 MB  (6x reduction!)
        LongSequenceDenoiser (Stage 2): ~450 MB  (5.3x reduction, more expressive!)
    """

    def __init__(
        self,
        c_s: int,
        c_p: int,
        n_timestep: int,
        c_pos_emb: int,
        c_timestep_emb: int,
        relpos_k: int,
        template_type: str,
        n_structure_layer: int,
        n_structure_block: int,
        c_hidden_ipa: int,
        n_head_ipa: int,
        n_qk_point: int,
        n_v_point: int,
        ipa_dropout: float,
        n_structure_transition_layer: int,
        structure_transition_dropout: float,
        max_n_res: int,
        z_factor_rank: int = None,  # Auto-compute if None
        k_neighbors: int = 10,
        use_adaptive_config: bool = True,  # Enable adaptive optimization
        force_mhc_expansion: int = None,  # Override adaptive expansion
        use_grad_checkpoint: bool = True,  # Recommended for long sequences
        use_flash_attn_3: bool = True,
        # V2 parameters
        use_pair_refinement: bool = None,  # V2: Auto-detect if None
        pair_refinement_layers: int = None,  # V2: Auto-detect if None
        pair_refinement_dropout: float = 0.1,  # V2: Dropout for pair refinement
        gpu_memory_gb: float = 24.0,  # V2: GPU memory for adaptive config
        # Stage 2 parameters
        use_pair_transform: bool = False,  # Stage 2: Enable factorized pair transform
        n_pair_transform_layer: int = 2,  # Stage 2: Number of pair transform layers
        pair_transform_include_mul: bool = True,  # Stage 2: Include triangle mult updates
        pair_transform_include_att: bool = True,  # Stage 2: Include triangle attention
        pair_transform_chunk_size: int = 64,  # Stage 2: Chunk size for attention
    ):
        super().__init__()

        self.max_n_res = max_n_res
        self.use_adaptive_config = use_adaptive_config

        # Compute adaptive factorization rank if not specified
        if z_factor_rank is None:
            z_factor_rank = AdaptiveFactorizationRank.compute_rank(max_n_res)

        self.z_factor_rank = z_factor_rank

        # Get adaptive mHC configuration
        if use_adaptive_config:
            mhc_config = AdaptiveMHCConfig.get_config(max_n_res, gpu_memory_gb)
            mhc_expansion_rate = force_mhc_expansion or mhc_config['structure_expansion']
            mhc_sinkhorn_iters = mhc_config['sinkhorn_iters']

            # V2: Auto-detect pair refinement settings
            if use_pair_refinement is None:
                use_pair_refinement = mhc_config.get('use_pair_refinement', False)
            if pair_refinement_layers is None:
                pair_refinement_layers = mhc_config.get('pair_refinement_layers', 0)
        else:
            mhc_expansion_rate = force_mhc_expansion or 4
            mhc_sinkhorn_iters = 20
            if use_pair_refinement is None:
                use_pair_refinement = True
            if pair_refinement_layers is None:
                pair_refinement_layers = 2

        self.use_pair_refinement = use_pair_refinement
        self.use_pair_transform = use_pair_transform  # Stage 2

        print(f"============================================================")
        print(f"LongSequenceDenoiser Stage 2: Stage 1 + V2 + Stage 2 Optimizations")
        print(f"============================================================")
        print(f"  max_n_res: {max_n_res}")
        print(f"  z_factor_rank: {z_factor_rank} (factorization rank)")
        print(f"  mhc_expansion_rate: {mhc_expansion_rate}")
        print(f"  mhc_sinkhorn_iters: {mhc_sinkhorn_iters}")
        print(f"  use_adaptive_config: {use_adaptive_config}")
        print(f"  use_grad_checkpoint: {use_grad_checkpoint}")
        print(f"  use_pair_refinement: {use_pair_refinement}")
        if use_pair_refinement:
            print(f"  pair_refinement_layers: {pair_refinement_layers}")
        print(f"  use_pair_transform (Stage 2): {use_pair_transform}")  # Stage 2
        if use_pair_transform:
            print(f"  n_pair_transform_layer: {n_pair_transform_layer}")
            print(f"  pair_transform_include_mul: {pair_transform_include_mul}")
            print(f"  pair_transform_include_att: {pair_transform_include_att}")
            print(f"  pair_transform_chunk_size: {pair_transform_chunk_size}")

        if use_adaptive_config:
            print(f"")
            print(f"Adaptive Configuration:")
            AdaptiveMHCConfig.print_config(max_n_res, gpu_memory_gb)
            print(f"")
            DynamicBatchSize.print_batch_config(max_n_res, base_batch=32, base_len=128)

        # Memory estimate
        print(f"")
        print(f"Memory Estimate (per sample):")
        batch = DynamicBatchSize.compute_batch_size(max_n_res, base_batch=32, base_len=128)
        mem = MemoryEstimator.estimate_total_memory(
            max_n_res, batch, c_s=c_s, c_p=c_p,
            use_factorization=True,
            use_mhc=True,
            mhc_expansion=mhc_expansion_rate
        )
        print(f"  Single features: {mem['single']:.1f} MB")
        print(f"  Pair features: {mem['pair']:.1f} MB (factorized)")
        print(f"  Structure module: {mem['structure']:.1f} MB")
        print(f"  Total (with gradients): {mem['total']:.1f} MB")
        print(f"  Recommended batch size: {batch}")
        print(f"============================================================")

        # Single feature network (same as standard)
        self.single_feature_net = SingleFeatureNet(
            c_s,
            n_timestep,
            c_pos_emb,
            c_timestep_emb
        )

        # Factorized pair feature network (KEY OPTIMIZATION!)
        self.pair_feature_net = FactorizedPairFeatureNet(
            c_s=c_s,
            c_p=c_p,
            rank=z_factor_rank,
            relpos_k=relpos_k,
            template_type=template_type
        )

        # V2: Factorized pair refinement (optional lightweight pair updates)
        if use_pair_refinement and pair_refinement_layers > 0:
            self.pair_refinement = FactorizedPairRefinement(
                c_p=c_p,
                rank=z_factor_rank,
                n_layers=pair_refinement_layers,
                dropout=pair_refinement_dropout
            )
        else:
            self.pair_refinement = None

        # Stage 2: Factorized pair transform network (optional intensive pair processing)
        if use_pair_transform:
            self.pair_transform = FactorizedPairTransformNet(
                c_p=c_p,
                rank=z_factor_rank,
                n_pair_transform_layer=n_pair_transform_layer,
                include_mul_update=pair_transform_include_mul,
                include_tri_att=pair_transform_include_att,
                c_hidden_mul=c_p,
                c_hidden_tri_att=c_p,
                n_head_tri=4,
                tri_dropout=0.25,
                pair_transition_n=4,
                tri_att_chunk_size=pair_transform_chunk_size,
                use_grad_checkpoint=use_grad_checkpoint
            )
        else:
            self.pair_transform = None

        # mHC Flash Structure Network with adaptive configuration
        self.structure_net = mHCFlashStructureNet(
            c_s=c_s,
            c_p=c_p,
            c_hidden_ipa=c_hidden_ipa,
            n_head=n_head_ipa,
            n_qk_point=n_qk_point,
            n_v_point=n_v_point,
            ipa_dropout=ipa_dropout,
            n_structure_transition_layer=n_structure_transition_layer,
            structure_transition_dropout=structure_transition_dropout,
            n_structure_layer=n_structure_layer,
            n_structure_block=n_structure_block,
            max_n_res=max_n_res,
            z_factor_rank=z_factor_rank,
            k_neighbors=k_neighbors,
            mhc_expansion_rate=mhc_expansion_rate,
            mhc_sinkhorn_iters=mhc_sinkhorn_iters,
            mhc_alpha_init=0.01,
            use_grad_checkpoint=use_grad_checkpoint,
            use_flash_attn_3=use_flash_attn_3,
        )

    def forward(self, ts, timesteps, mask):
        """
        Forward pass through long-sequence optimized denoiser (Stage 2).

        Args:
            ts: Noisy frames (rigid transforms)
            timesteps: Diffusion timesteps [B]
            mask: Sequence mask [B, L]

        Returns:
            ts: Denoised frames (updated rigid transforms)
        """
        # Generate single features
        s = self.single_feature_net(ts, timesteps, mask)

        # Generate FACTORIZED pair features (KEY: no full L¬≤ materialization!)
        # Output: (factor_1, factor_2) instead of full pair tensor
        z_factor_1, z_factor_2 = self.pair_feature_net(s, ts, mask)

        # V2: Optional pair refinement (lightweight factorized updates)
        if self.pair_refinement is not None:
            z_factor_1, z_factor_2 = self.pair_refinement(z_factor_1, z_factor_2, mask)

        # Stage 2: Optional pair transform (intensive factorized pair processing)
        if self.pair_transform is not None:
            z_factor_1, z_factor_2 = self.pair_transform(z_factor_1, z_factor_2, mask)

        # Structure network (Flash-IPA with factorized pairs)
        # Directly uses factors without reconstructing full pair
        s_final, t_final = self.structure_net(s, z_factor_1, z_factor_2, ts, mask)

        return t_final

    @staticmethod
    def from_config(config):
        """
        Create LongSequenceDenoiser from config object.

        Args:
            config: Config object from genie.config

        Returns:
            LongSequenceDenoiser instance
        """
        return LongSequenceDenoiser(
            c_s=config.model['c_s'],
            c_p=config.model['c_p'],
            n_timestep=config.diffusion['n_timestep'],
            c_pos_emb=config.model['c_pos_emb'],
            c_timestep_emb=config.model['c_timestep_emb'],
            relpos_k=config.model['relpos_k'],
            template_type=config.model['template_type'],
            n_structure_layer=config.model['n_structure_layer'],
            n_structure_block=config.model['n_structure_block'],
            c_hidden_ipa=config.model['c_hidden_ipa'],
            n_head_ipa=config.model['n_head_ipa'],
            n_qk_point=config.model['n_qk_point'],
            n_v_point=config.model['n_v_point'],
            ipa_dropout=config.model['ipa_dropout'],
            n_structure_transition_layer=config.model['n_structure_transition_layer'],
            structure_transition_dropout=config.model['structure_transition_dropout'],
            max_n_res=config.model['max_n_res'],
            z_factor_rank=config.model.get('z_factor_rank', None),
            k_neighbors=config.model.get('k_neighbors', 10),
            use_adaptive_config=config.model.get('use_adaptive_config', True),
            force_mhc_expansion=config.model.get('force_mhc_expansion', None),
            use_grad_checkpoint=config.model.get('use_grad_checkpoint', True),
            use_flash_attn_3=config.model.get('use_flash_attn_3', True),
            # V2 parameters
            use_pair_refinement=config.model.get('use_pair_refinement', None),
            pair_refinement_layers=config.model.get('pair_refinement_layers', None),
            pair_refinement_dropout=config.model.get('pair_refinement_dropout', 0.1),
            gpu_memory_gb=config.model.get('gpu_memory_gb', 24.0),
            # Stage 2 parameters
            use_pair_transform=config.model.get('use_pair_transform', False),
            n_pair_transform_layer=config.model.get('n_pair_transform_layer', 2),
            pair_transform_include_mul=config.model.get('pair_transform_include_mul', True),
            pair_transform_include_att=config.model.get('pair_transform_include_att', True),
            pair_transform_chunk_size=config.model.get('pair_transform_chunk_size', 64),
        )


def test_long_sequence_denoiser():
    """
    Test LongSequenceDenoiser Stage 2 with different sequence lengths.
    """
    print("=" * 80)
    print("Testing LongSequenceDenoiser Stage 2")
    print("=" * 80)
    print()

    # Test configurations
    test_configs = [
        {'L': 128, 'batch': 4, 'expected_rank': 8},
        {'L': 256, 'batch': 2, 'expected_rank': 8},
        {'L': 512, 'batch': 1, 'expected_rank': 4},
    ]

    for config in test_configs:
        L = config['L']
        B = config['batch']

        print(f"Testing L={L}, batch={B}")
        print("-" * 80)

        # Create model
        model = LongSequenceDenoiser(
            c_s=64,
            c_p=64,
            n_timestep=100,
            c_pos_emb=64,
            c_timestep_emb=64,
            relpos_k=32,
            template_type='v1',
            n_structure_layer=2,
            n_structure_block=1,
            c_hidden_ipa=16,
            n_head_ipa=4,
            n_qk_point=2,
            n_v_point=4,
            ipa_dropout=0.1,
            n_structure_transition_layer=1,
            structure_transition_dropout=0.1,
            max_n_res=L,
            use_adaptive_config=True,
            use_grad_checkpoint=False,  # Disable for testing
            use_flash_attn_3=False,
            # V2: Test pair refinement
            use_pair_refinement=True,
            pair_refinement_layers=1,
            # Stage 2: Test pair transform
            use_pair_transform=(L >= 256),  # Only for longer sequences
            n_pair_transform_layer=1,
        )

        # Test input
        from genie.flash_ipa.rigid import create_identity_rigid
        ts = create_identity_rigid(B, L)
        timesteps = torch.randint(0, 100, (B,))
        mask = torch.ones(B, L)

        # Forward pass
        try:
            with torch.no_grad():
                ts_out = model(ts, timesteps, mask)

            print(f"‚úÖ Forward pass successful")
            print(f"   Input shape: {ts.shape}")
            print(f"   Output shape: {ts_out.shape}")
            print(f"   Pair refinement: {model.pair_refinement is not None}")
            print(f"   Pair transform (Stage 2): {model.pair_transform is not None}")  # Stage 2

            # Memory usage
            if torch.cuda.is_available():
                mem_allocated = torch.cuda.max_memory_allocated() / (1024 ** 2)
                print(f"   Peak GPU memory: {mem_allocated:.1f} MB")
                torch.cuda.reset_peak_memory_stats()

        except Exception as e:
            print(f"‚ùå Forward pass failed: {e}")
            import traceback
            traceback.print_exc()

        print()

    print("=" * 80)
    print("üéâ Testing complete!")
    print("=" * 80)


if __name__ == "__main__":
    test_long_sequence_denoiser()
