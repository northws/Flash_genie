"""
é«˜çº§æ¢¯åº¦æ£€æŸ¥ç‚¹ (Stage 4)

æ­¤æ¨¡å—æä¾›è¶…è¶Š PyTorch æ ‡å‡†æ£€æŸ¥ç‚¹å‡½æ•°çš„é«˜çº§æ¢¯åº¦æ£€æŸ¥ç‚¹ç­–ç•¥ã€‚

æ ¸å¿ƒç‰¹æ€§:
1. é€‰æ‹©æ€§æ£€æŸ¥ç‚¹: ä»…æ£€æŸ¥ç‚¹æ˜‚è´µçš„æ“ä½œ
2. è‡ªé€‚åº”æ£€æŸ¥ç‚¹: åŸºäºå†…å­˜ä½¿ç”¨åŠ¨æ€è°ƒæ•´
3. å±‚-wise æ£€æŸ¥ç‚¹: å¯¹å±‚è¿›è¡Œç»†ç²’åº¦æ§åˆ¶

å†…å­˜ä¼˜åŠ¿:
- æ ‡å‡†: å­˜å‚¨æ‰€æœ‰æ¿€æ´» (O(L Ã— depth))
- æ£€æŸ¥ç‚¹: åªå­˜å‚¨ä¸€äº›æ¿€æ´»ï¼Œé‡æ–°è®¡ç®—å…¶ä»– (O(L Ã— âˆšdepth))
- å¯¹äº 8 å±‚æ¨¡å‹ï¼ŒL=1024: çº¦ 3x å†…å­˜å‡å°‘

åŸºäº:
- æ¢¯åº¦æ£€æŸ¥ç‚¹ (Chen et al. 2016)
- é€‰æ‹©æ€§æ¿€æ´»æ£€æŸ¥ç‚¹ (Jain et al. 2020)
- AlphaFold2 æ£€æŸ¥ç‚¹ç­–ç•¥

ä½œè€…: Stage 4 å®ç° (2026-01-13)
"""

import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint
from typing import Optional, Callable, List, Any, Tuple
import contextlib


class CheckpointConfig:
    """
    æ¢¯åº¦æ£€æŸ¥ç‚¹ç­–ç•¥çš„é…ç½®ã€‚
    """

    def __init__(
        self,
        enabled: bool = True,
        strategy: str = "selective",  # "none", "all", "selective", "adaptive"
        checkpoint_structure: bool = True,
        checkpoint_pairs: bool = False,  # é…å¯¹æ“ä½œæ˜‚è´µï¼Œè€ƒè™‘æ£€æŸ¥ç‚¹
        checkpoint_triangles: bool = True,  # ä¸‰è§’æ“ä½œéå¸¸æ˜‚è´µ
        min_memory_gb: float = 2.0,  # å¯ç”¨æ¿€è¿›æ£€æŸ¥ç‚¹å‰çš„æœ€å°ç©ºé—²å†…å­˜
    ):
        """
        åˆå§‹åŒ–æ£€æŸ¥ç‚¹é…ç½®ã€‚

        Args:
            enabled: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            strategy: æ£€æŸ¥ç‚¹ç­–ç•¥
            checkpoint_structure: æ£€æŸ¥ç‚¹ç»“æ„æ¨¡å—å±‚
            checkpoint_pairs: æ£€æŸ¥ç‚¹é…å¯¹æ“ä½œ
            checkpoint_triangles: æ£€æŸ¥ç‚¹ä¸‰è§’æ“ä½œ
            min_memory_gb: æœ€å°ç©ºé—²å†…å­˜é˜ˆå€¼
        """
        self.enabled = enabled
        self.strategy = strategy
        self.checkpoint_structure = checkpoint_structure
        self.checkpoint_pairs = checkpoint_pairs
        self.checkpoint_triangles = checkpoint_triangles
        self.min_memory_gb = min_memory_gb

    @staticmethod
    def get_adaptive_config(seq_len: int, available_memory_gb: float) -> "CheckpointConfig":
        """
        æ ¹æ®åºåˆ—é•¿åº¦å’Œå¯ç”¨å†…å­˜è·å–è‡ªé€‚åº”æ£€æŸ¥ç‚¹é…ç½®ã€‚

        Args:
            seq_len: åºåˆ—é•¿åº¦
            available_memory_gb: å¯ç”¨ GPU å†…å­˜ (GB)

        Returns:
            é’ˆå¯¹ç»™å®šæ¡ä»¶ä¼˜åŒ–çš„ CheckpointConfig
        """
        if seq_len < 256 and available_memory_gb > 10:
            # å†…å­˜å……è¶³çŸ­åºåˆ—: ä¸éœ€è¦æ£€æŸ¥ç‚¹
            return CheckpointConfig(enabled=False, strategy="none")

        elif seq_len < 512 and available_memory_gb > 8:
            # ä¸­ç­‰åºåˆ—: é€‰æ‹©æ€§æ£€æŸ¥ç‚¹
            return CheckpointConfig(
                enabled=True,
                strategy="selective",
                checkpoint_structure=False,
                checkpoint_pairs=False,
                checkpoint_triangles=True,
            )

        elif seq_len < 1024 and available_memory_gb > 6:
            # é•¿åºåˆ—: æ£€æŸ¥ç‚¹ä¸‰è§’å’Œé…å¯¹
            return CheckpointConfig(
                enabled=True,
                strategy="selective",
                checkpoint_structure=True,
                checkpoint_pairs=True,
                checkpoint_triangles=True,
            )

        else:
            # éå¸¸é•¿åºåˆ—æˆ–ä½å†…å­˜: æ£€æŸ¥ç‚¹ä¸€åˆ‡
            return CheckpointConfig(
                enabled=True,
                strategy="all",
                checkpoint_structure=True,
                checkpoint_pairs=True,
                checkpoint_triangles=True,
            )


class SelectiveCheckpoint:
    """
    é€‰æ‹©æ€§æ¢¯åº¦æ£€æŸ¥ç‚¹åŒ…è£…å™¨ã€‚

    ç”¨æ³•:
        with SelectiveCheckpoint(config):
            # æ ¹æ®é…ç½®å¯¹ä¸Šä¸‹æ–‡å†…çš„æ“ä½œè¿›è¡Œæ£€æŸ¥ç‚¹
            output = expensive_operation(input)
    """

    def __init__(self, config: CheckpointConfig):
        self.config = config
        self._original_checkpoint_setting = {}

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        pass

    def checkpoint_function(
        self,
        function: Callable,
        *args,
        use_reentrant: bool = True,
        **kwargs
    ) -> Any:
        """
        æ ¹æ®é…ç½®æœ‰æ¡ä»¶åœ°æ£€æŸ¥ç‚¹å‡½æ•°ã€‚

        Args:
            function: è¦æ£€æŸ¥ç‚¹çš„å‡½æ•°
            *args: å‡½æ•°çš„å‚æ•°
            use_reentrant: ä½¿ç”¨å¯é‡å…¥æ£€æŸ¥ç‚¹
            **kwargs: å‡½æ•°çš„å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°è¾“å‡º
        """
        if self.config.enabled:
            return checkpoint(function, *args, use_reentrant=use_reentrant, **kwargs)
        else:
            return function(*args, **kwargs)


class CheckpointedSequential(nn.Sequential):
    """
    æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹çš„é¡ºåºæ¨¡å—ã€‚

    æ¯ N å±‚æ£€æŸ¥ç‚¹ä¸€æ¬¡ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å±‚ã€‚
    """

    def __init__(self, *args, checkpoint_every: int = 2, config: Optional[CheckpointConfig] = None):
        """
        Args:
            *args: é¡ºåºä¸­åŒ…å«çš„æ¨¡å—
            checkpoint_every: æ¯ N å±‚æ£€æŸ¥ç‚¹ä¸€æ¬¡ (ä¾‹å¦‚: 2 = æ¯éš”ä¸€å±‚)
            config: æ£€æŸ¥ç‚¹é…ç½®
        """
        super().__init__(*args)
        self.checkpoint_every = checkpoint_every
        self.config = config or CheckpointConfig(enabled=True, strategy="selective")

    def forward(self, x):
        """å¸¦é€‰æ‹©æ€§æ£€æŸ¥ç‚¹çš„å‰å‘ä¼ æ’­ã€‚"""
        for i, module in enumerate(self):
            if self.config.enabled and (i % self.checkpoint_every == 0):
                # æ£€æŸ¥ç‚¹æ­¤å±‚
                x = checkpoint(module, x, use_reentrant=False)
            else:
                # å¸¸è§„å‰å‘ä¼ æ’­
                x = module(x)
        return x


class LayerWithCheckpoint(nn.Module):
    """
    ä»»ä½•å±‚çš„åŒ…è£…å™¨ï¼Œæ·»åŠ æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚

    ç”¨æ³•:
        layer = LayerWithCheckpoint(
            my_expensive_layer,
            checkpoint_enabled=True
        )
    """

    def __init__(
        self,
        layer: nn.Module,
        checkpoint_enabled: bool = True,
        use_reentrant: bool = False,
    ):
        super().__init__()
        self.layer = layer
        self.checkpoint_enabled = checkpoint_enabled
        self.use_reentrant = use_reentrant

    def forward(self, *args, **kwargs):
        """å¸¦å¯é€‰æ£€æŸ¥ç‚¹çš„å‰å‘ä¼ æ’­ã€‚"""
        if self.checkpoint_enabled and self.training:
            return checkpoint(
                self.layer,
                *args,
                use_reentrant=self.use_reentrant,
                **kwargs
            )
        else:
            return self.layer(*args, **kwargs)


def get_memory_stats() -> dict:
    """
    è·å–å½“å‰ GPU å†…å­˜ç»Ÿè®¡ä¿¡æ¯ã€‚

    Returns:
        åŒ…å«å†…å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    if not torch.cuda.is_available():
        return {
            "allocated_gb": 0.0,
            "reserved_gb": 0.0,
            "free_gb": 0.0,
        }

    allocated = torch.cuda.memory_allocated() / (1024 ** 3)
    reserved = torch.cuda.memory_reserved() / (1024 ** 3)

    # è·å–æ€»å†…å­˜
    total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
    free = total - allocated

    return {
        "allocated_gb": allocated,
        "reserved_gb": reserved,
        "free_gb": free,
        "total_gb": total,
    }


class AdaptiveCheckpointManager:
    """
    åŸºäºå†…å­˜ä½¿ç”¨è‡ªé€‚åº”ç®¡ç†æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚

    ç›‘æ§ GPU å†…å­˜å¹¶åŠ¨æ€å¯ç”¨/ç¦ç”¨æ£€æŸ¥ç‚¹ã€‚
    """

    def __init__(
        self,
        initial_config: Optional[CheckpointConfig] = None,
        memory_threshold_gb: float = 2.0,
    ):
        """
        Args:
            initial_config: åˆå§‹æ£€æŸ¥ç‚¹é…ç½®
            memory_threshold_gb: ç©ºé—²å†…å­˜é˜ˆå€¼ï¼Œè§¦å‘æ¿€è¿›æ£€æŸ¥ç‚¹
        """
        self.config = initial_config or CheckpointConfig()
        self.memory_threshold_gb = memory_threshold_gb
        self.stats = {"adaptations": 0, "memory_warnings": 0}

    def get_current_config(self, seq_len: int) -> CheckpointConfig:
        """
        æ ¹æ®å†…å­˜çŠ¶æ€è·å–å½“å‰æ£€æŸ¥ç‚¹é…ç½®ã€‚

        Args:
            seq_len: å½“å‰åºåˆ—é•¿åº¦

        Returns:
            é€‚å½“çš„ CheckpointConfig
        """
        mem_stats = get_memory_stats()
        free_memory = mem_stats["free_gb"]

        # å¦‚æœå†…å­˜ä½åˆ™æ›´æ–°é…ç½®
        if free_memory < self.memory_threshold_gb:
            self.stats["memory_warnings"] += 1
            # å¯ç”¨æ¿€è¿›æ£€æŸ¥ç‚¹
            return CheckpointConfig(
                enabled=True,
                strategy="all",
                checkpoint_structure=True,
                checkpoint_pairs=True,
                checkpoint_triangles=True,
            )
        else:
            # ä½¿ç”¨è‡ªé€‚åº”é…ç½®
            return CheckpointConfig.get_adaptive_config(seq_len, free_memory)

    def adapt_if_needed(self, seq_len: int) -> CheckpointConfig:
        """
        æ ¹æ®å½“å‰æ¡ä»¶éœ€è¦æ—¶è°ƒæ•´æ£€æŸ¥ç‚¹é…ç½®ã€‚

        Args:
            seq_len: å½“å‰åºåˆ—é•¿åº¦

        Returns:
            æ›´æ–°çš„ CheckpointConfig
        """
        new_config = self.get_current_config(seq_len)

        if new_config.strategy != self.config.strategy:
            self.stats["adaptations"] += 1
            self.config = new_config

        return self.config

    def get_stats(self) -> dict:
        """è·å–é€‚é…ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        mem_stats = get_memory_stats()
        return {
            **self.stats,
            **mem_stats,
            "current_strategy": self.config.strategy,
        }


def test_gradient_checkpointing():
    """æµ‹è¯•æ¢¯åº¦æ£€æŸ¥ç‚¹å·¥å…·ã€‚"""
    print("=" * 80)
    print("æµ‹è¯•æ¢¯åº¦æ£€æŸ¥ç‚¹ (Stage 4)")
    print("=" * 80)
    print()

    # æµ‹è¯• 1: åŸºæœ¬æ£€æŸ¥ç‚¹
    print("æµ‹è¯• 1: åŸºæœ¬æ£€æŸ¥ç‚¹é…ç½®")
    print("-" * 80)

    config = CheckpointConfig(enabled=True, strategy="selective")
    print(f"  é…ç½®: enabled={config.enabled}, strategy={config.strategy}")
    print(f"  âœ… åŸºæœ¬é…ç½®å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 2: è‡ªé€‚åº”é…ç½®
    print("æµ‹è¯• 2: è‡ªé€‚åº”é…ç½®")
    print("-" * 80)

    test_cases = [
        (256, 12.0),
        (512, 8.0),
        (1024, 6.0),
        (2048, 4.0),
    ]

    for seq_len, memory in test_cases:
        config = CheckpointConfig.get_adaptive_config(seq_len, memory)
        print(f"  L={seq_len:4d}, Memory={memory:.1f}GB: "
              f"strategy={config.strategy}, "
              f"checkpoint_triangles={config.checkpoint_triangles}")

    print(f"  âœ… è‡ªé€‚åº”é…ç½®å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 3: æ£€æŸ¥ç‚¹é¡ºåº
    print("æµ‹è¯• 3: æ£€æŸ¥ç‚¹é¡ºåº")
    print("-" * 80)

    layers = [
        nn.Linear(128, 128),
        nn.ReLU(),
        nn.Linear(128, 128),
        nn.ReLU(),
    ]

    seq = CheckpointedSequential(*layers, checkpoint_every=2)
    x = torch.randn(2, 128, requires_grad=True)

    y = seq(x)
    loss = y.sum()
    loss.backward()

    assert x.grad is not None, "æ¢¯åº¦æœªè®¡ç®—"
    print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"  è¾“å‡ºå½¢çŠ¶: {y.shape}")
    print(f"  æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
    print(f"  âœ… æ£€æŸ¥ç‚¹é¡ºåºå·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 4: å†…å­˜ç»Ÿè®¡
    print("æµ‹è¯• 4: å†…å­˜ç»Ÿè®¡")
    print("-" * 80)

    mem_stats = get_memory_stats()
    for key, value in mem_stats.items():
        print(f"  {key}: {value:.3f}")

    print(f"  âœ… å†…å­˜ç»Ÿè®¡å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 5: è‡ªé€‚åº”ç®¡ç†å™¨
    print("æµ‹è¯• 5: è‡ªé€‚åº”æ£€æŸ¥ç‚¹ç®¡ç†å™¨")
    print("-" * 80)

    manager = AdaptiveCheckpointManager(memory_threshold_gb=4.0)

    for seq_len in [256, 512, 1024]:
        config = manager.adapt_if_needed(seq_len)
        print(f"  L={seq_len:4d}: strategy={config.strategy}")

    stats = manager.get_stats()
    print(f"  é€‚é…æ¬¡æ•°: {stats['adaptations']}")
    print(f"  å†…å­˜è­¦å‘Š: {stats['memory_warnings']}")
    print(f"  âœ… è‡ªé€‚åº”ç®¡ç†å™¨å·¥ä½œæ­£å¸¸!")
    print()

    print("=" * 80)
    print("ğŸ‰ æ‰€æœ‰æ¢¯åº¦æ£€æŸ¥ç‚¹æµ‹è¯•é€šè¿‡!")
    print("=" * 80)


if __name__ == "__main__":
    test_gradient_checkpointing()
