"""
å› å­åŒ–é…å¯¹ç‰¹å¾ç½‘ç»œ

æ­¤æ¨¡å—å®ç°äº†é…å¯¹ç‰¹å¾ç½‘ç»œçš„å› å­åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ç”Ÿæˆä½ç§©å› å­åŒ–è¡¨ç¤ºï¼Œ
è€Œä¸æ˜¯å®ä¾‹åŒ–å®Œæ•´çš„ LÂ² Ã— C é…å¯¹å¼ é‡ã€‚

æ ¸å¿ƒåˆ›æ–°:
- æ ‡å‡†æ–¹å¼:    s[LÃ—C] â†’ p[LÂ²Ã—C] â†’ factors[LÃ—rankÃ—C]  (O(LÂ²) å†…å­˜)
- å› å­åŒ–æ–¹å¼:  s[LÃ—C] â†’ factors[LÃ—rankÃ—C]             (O(LÃ—rank) å†…å­˜)

å¯¹äº L=1024, rank=2, C=128:
- æ ‡å‡†æ–¹å¼: 1024Â² Ã— 128 Ã— 4 å­—èŠ‚ = 537 MB
- å› å­åŒ–: 1024 Ã— 2 Ã— 128 Ã— 4 å­—èŠ‚ = 1 MB
- å†…å­˜èŠ‚çœ: 537x

åŸºäº Flash-IPA è®ºæ–‡çš„å› å­åŒ–æŠ€æœ¯ã€‚

V2 æ”¹è¿› (2026-01):
- ä¿®å¤ FactorizedRelPos ä»¥æ­£ç¡®å¤„ç†åå¯¹ç§°ç›¸å¯¹ä½ç½®
- ä¿®å¤ FactorizedTemplate ä»¥ä¿ç•™å‡ ä½•ä¿¡æ¯
- æ·»åŠ  FactorizedPairRefinement ç”¨äºè½»é‡çº§é…å¯¹æ›´æ–°
"""

import torch
from torch import nn
import math


class FactorizedRelPos(nn.Module):
    """
    å› å­åŒ–ç›¸å¯¹ä½ç½®ç¼–ç  (V2 - å·²ä¿®å¤)ã€‚

    æ ¸å¿ƒæ´å¯Ÿ: ç›¸å¯¹ä½ç½®ç¼–ç  relpos[i,j] = f(i-j) æ˜¯åå¯¹ç§°çš„ã€‚
    æˆ‘ä»¬ä¸èƒ½ç®€å•åœ°å°†å…¶åˆ†è§£ä¸º left[i] + right[j]ã€‚

    è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨å¸¦åå¯¹ç§°ç»“æ„çš„å¯å­¦ä¹ ä½ç½®åµŒå…¥:
    - left[i] = pos_emb[i] + relpos_bias
    - right[j] = pos_emb[j] - relpos_bias  (åå¯¹ç§°)

    è¿™åœ¨å¯å› å­åŒ–çš„åŒæ—¶ä¿ç•™äº†ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚
    """

    def __init__(self, relpos_k, c_out, rank, max_seq_len=4096):
        """
        åˆå§‹åŒ–å› å­åŒ–ç›¸å¯¹ä½ç½®ç¼–ç å™¨ã€‚

        Args:
            relpos_k: ç›¸å¯¹ä½ç½®ç¼–ç çª—å£å¤§å°
            c_out: è¾“å‡ºç‰¹å¾ç»´åº¦
            rank: å› å­åŒ–ç§©
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        super().__init__()
        self.relpos_k = relpos_k
        self.n_bin = 2 * relpos_k + 1
        self.c_out = c_out
        self.rank = rank
        self.max_seq_len = max_seq_len

        # å¯å­¦ä¹ ç»å¯¹ä½ç½®åµŒå…¥
        self.pos_emb = nn.Embedding(max_seq_len, rank * c_out)

        # å¯å­¦ä¹ ç›¸å¯¹ä½ç½®åç½® (æ•æ‰åå¯¹ç§°éƒ¨åˆ†)
        # æ·»åŠ åˆ° leftï¼Œä» right å‡å»ä»¥åˆ›å»ºåå¯¹ç§°æ€§
        self.relpos_bias = nn.Parameter(torch.zeros(rank, c_out))

        # ç›¸å¯¹ä½ç½®ç®±åµŒå…¥ï¼Œç”¨äºé¢å¤–è¡¨è¾¾èƒ½åŠ›
        self.relpos_bin_emb = nn.Embedding(self.n_bin, rank * c_out)

        # ç”¨äºç»„åˆä½ç½®ä¿¡æ¯çš„æŠ•å½±å±‚
        self.proj_left = nn.Linear(rank * c_out * 2, rank * c_out)
        self.proj_right = nn.Linear(rank * c_out * 2, rank * c_out)

        # ä½¿ç”¨æ­£å¼¦æ¨¡å¼åˆå§‹åŒ–ä½ç½®åµŒå…¥ä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–
        self._init_pos_emb()

    def _init_pos_emb(self):
        """ä½¿ç”¨æ­£å¼¦æ¨¡å¼åˆå§‹åŒ–ä½ç½®åµŒå…¥ä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–ã€‚"""
        d_model = self.rank * self.c_out
        pe = torch.zeros(self.max_seq_len, d_model)
        position = torch.arange(0, self.max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term[:d_model//2] if d_model % 2 == 1 else div_term)
        self.pos_emb.weight.data.copy_(pe)

    def forward(self, L, device):
        """
        ç”Ÿæˆå› å­åŒ–ç›¸å¯¹ä½ç½®ç¼–ç ã€‚

        Args:
            L: åºåˆ—é•¿åº¦
            device: torch è®¾å¤‡

        Returns:
            relpos_left: [L, rank, C]
            relpos_right: [L, rank, C]
        """
        # è·å–ä½ç½®ç´¢å¼•
        pos = torch.arange(L, device=device)

        # ç»å¯¹ä½ç½®åµŒå…¥ [L, rank*C]
        abs_pos = self.pos_emb(pos)

        # è®¡ç®—åˆ°ä¸­å¿ƒä½ç½®çš„ç›¸å¯¹ç®±ç´¢å¼•
        # å¯¹äºä½ç½® iï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ä¸­å¿ƒ (L//2) è·ç¦»å¯¹åº”çš„ç®±
        center = L // 2
        rel_to_center = pos - center
        rel_to_center_clipped = torch.clamp(rel_to_center, -self.relpos_k, self.relpos_k)
        bin_idx = (rel_to_center_clipped + self.relpos_k).long()

        # è·å–ç›¸å¯¹ä½ç½®ç®±åµŒå…¥ [L, rank*C]
        rel_bin_emb = self.relpos_bin_emb(bin_idx)

        # ç»„åˆç»å¯¹å’Œç›¸å¯¹ä¿¡æ¯
        combined = torch.cat([abs_pos, rel_bin_emb], dim=-1)  # [L, 2*rank*C]

        # æŠ•å½±åˆ°å¸¦åå¯¹ç§°åç½®çš„ left/right å› å­
        left_base = self.proj_left(combined).view(L, self.rank, self.c_out)
        right_base = self.proj_right(combined).view(L, self.rank, self.c_out)

        # æ·»åŠ åå¯¹ç§°åç½®: left å¾—åˆ° +biasï¼Œright å¾—åˆ° -bias
        # è¿™ç¡®ä¿é‡å»ºæ—¶ relpos[i,j] â‰  relpos[j,i]
        relpos_left = left_base + self.relpos_bias.unsqueeze(0)
        relpos_right = right_base - self.relpos_bias.unsqueeze(0)

        return relpos_left, relpos_right


class FactorizedTemplate(nn.Module):
    """
    å› å­åŒ–æ¨¡æ¿ç‰¹å¾ç¼–ç  (V2 - å·²ä¿®å¤)ã€‚

    æ ¸å¿ƒæ´å¯Ÿ: æ¨¡æ¿ç‰¹å¾åŒ…å«æ®‹åŸºå¯¹ä¹‹é—´çš„å‡ ä½•ä¿¡æ¯ (è·ç¦»ã€è§’åº¦)ã€‚
    ç®€å•å¹³å‡ä¼šä¸¢å¤±è¿™äº›å…³é”®ä¿¡æ¯ã€‚

    è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨ SVD é£æ ¼åˆ†è§£:
    1. å¯¹è§’çº¿ç‰¹å¾ (è‡ªä¿¡æ¯)
    2. å¸¦æ³¨æ„åŠ›åŠ æƒæ± åŒ–çš„è¡Œ/åˆ—èšåˆ
    3. å¯å­¦ä¹ çš„å¥‡å¼‚å€¼ç¼©æ”¾

    è¿™åœ¨å¯å› å­åŒ–çš„åŒæ—¶ä¿ç•™äº†æ›´å¤šå‡ ä½•ä¿¡æ¯ã€‚
    """

    def __init__(self, template_fn, c_template, c_out, rank):
        """
        åˆå§‹åŒ–å› å­åŒ–æ¨¡æ¿ç¼–ç å™¨ã€‚

        Args:
            template_fn: æ¨¡æ¿ç‰¹å¾æå–å‡½æ•°
            c_template: æ¨¡æ¿ç‰¹å¾ç»´åº¦
            c_out: è¾“å‡ºç‰¹å¾ç»´åº¦
            rank: å› å­åŒ–ç§©
        """
        super().__init__()
        self.template_fn = template_fn
        self.rank = rank
        self.c_out = c_out
        self.c_template = c_template

        # SVD é£æ ¼å› å­åŒ–: U @ Sigma @ V^T
        # U æŠ•å½± (å·¦å¥‡å¼‚å‘é‡)
        self.linear_U = nn.Linear(c_template * 3, rank * c_out)
        # V æŠ•å½± (å³å¥‡å¼‚å‘é‡)
        self.linear_V = nn.Linear(c_template * 3, rank * c_out)
        # å¯å­¦ä¹ çš„å¥‡å¼‚å€¼ (æ¯ç§©çš„é‡è¦æ€§åŠ æƒ)
        self.sigma = nn.Parameter(torch.ones(rank))

        # ç”¨äºåŠ æƒèšåˆçš„æ³¨æ„åŠ› (ä¼˜äºç®€å•å¹³å‡)
        self.attn_query = nn.Linear(c_template, 1)

        # ç”¨äºç¨³å®šæ€§çš„å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(c_template * 3)

    def _attention_pool(self, feat, dim):
        """
        æ²¿æŒ‡å®šç»´åº¦è¿›è¡Œæ³¨æ„åŠ›åŠ æƒæ± åŒ–ã€‚

        Args:
            feat: [B, L, L, C] æ¨¡æ¿ç‰¹å¾
            dim: æ± åŒ–ç»´åº¦ (1 æˆ– 2)

        Returns:
            pooled: [B, L, C] æ³¨æ„åŠ›åŠ æƒç‰¹å¾
        """
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_logits = self.attn_query(feat).squeeze(-1)  # [B, L, L]

        if dim == 2:
            attn_weights = torch.softmax(attn_logits, dim=2)  # [B, L, L]
            pooled = torch.einsum('bij,bijc->bic', attn_weights, feat)
        else:  # dim == 1
            attn_weights = torch.softmax(attn_logits, dim=1)  # [B, L, L]
            pooled = torch.einsum('bij,bijc->bjc', attn_weights, feat)

        return pooled

    def forward(self, t):
        """
        ç”Ÿæˆå› å­åŒ–æ¨¡æ¿ç‰¹å¾ã€‚

        Args:
            t: è¾“å…¥å˜æ¢ (Rigid å¯¹è±¡)

        Returns:
            template_left: [B, L, rank, C]
            template_right: [B, L, rank, C]
        """
        # è·å–æ¨¡æ¿ç‰¹å¾ [B, L, L, c_template]
        template_feat = self.template_fn(t)

        B, L, _, C_t = template_feat.shape

        # æå–å¯¹è§’çº¿ç‰¹å¾ (è‡ªä¿¡æ¯) [B, L, C_t]
        diag_idx = torch.arange(L, device=template_feat.device)
        diag_feat = template_feat[:, diag_idx, diag_idx, :]  # [B, L, C_t]

        # æ³¨æ„åŠ›åŠ æƒè¡Œèšåˆ [B, L, C_t]
        row_feat = self._attention_pool(template_feat, dim=2)

        # æ³¨æ„åŠ›åŠ æƒåˆ—èšåˆ [B, L, C_t]
        col_feat = self._attention_pool(template_feat, dim=1)

        # ç»„åˆ left å› å­çš„ç‰¹å¾: [B, L, 3*C_t]
        left_combined = torch.cat([diag_feat, row_feat, col_feat], dim=-1)
        left_combined = self.layer_norm(left_combined)

        # ç»„åˆ right å› å­çš„ç‰¹å¾ (ä¸åŒç»„åˆä»¥ä¿æŒä¸å¯¹ç§°æ€§)
        right_combined = torch.cat([diag_feat, col_feat, row_feat], dim=-1)
        right_combined = self.layer_norm(right_combined)

        # æŠ•å½±åˆ°å› å­åŒ–å½¢å¼ [B, L, rankÃ—C] â†’ [B, L, rank, C]
        template_left = self.linear_U(left_combined).view(B, L, self.rank, self.c_out)
        template_right = self.linear_V(right_combined).view(B, L, self.rank, self.c_out)

        # åº”ç”¨å¥‡å¼‚å€¼ç¼©æ”¾ (é‡è¦æ€§åŠ æƒ)
        # åªåº”ç”¨äº left å› å­ (ç±»ä¼¼ SVD: U @ Sigma)
        template_left = template_left * self.sigma.view(1, 1, -1, 1)

        return template_left, template_right


class FactorizedPairFeatureNet(nn.Module):
    """
    å†…å­˜é«˜æ•ˆçš„å› å­åŒ–é…å¯¹ç‰¹å¾ç½‘ç»œã€‚

    æ­¤æ¨¡å—ç›´æ¥ç”Ÿæˆå› å­åŒ–é…å¯¹è¡¨ç¤ºï¼Œè€Œæ— éœ€å®ä¾‹åŒ–å®Œæ•´çš„ LÂ² é…å¯¹å¼ é‡ã€‚

    æ ¸å¿ƒä¼˜åŠ¿:
    1. å†…å­˜: O(LÂ²) â†’ O(LÃ—rank) (é€šå¸¸ 256-512x èŠ‚çœ)
    2. é€Ÿåº¦: æ›´å¿«çš„å‰å‘ä¼ æ’­ (æ— éœ€å®ä¾‹åŒ–å¼€é”€)
    3. å…¼å®¹æ€§: è¾“å‡ºæ ¼å¼ä¸ LinearFactorizer åŒ¹é…

    ä½¿ç”¨æ–¹æ³•:
        # Flash-IPA æ¨¡å¼
        factor_1, factor_2 = factorized_pair_net(s, t, mask)
        s = flash_ipa(s, None, factor_1, factor_2, t, mask)

        # å¦‚æœéœ€è¦ï¼Œé‡å»ºå®Œæ•´é…å¯¹ (ç”¨äºè°ƒè¯•)
        p_reconstructed = reconstruct_pair(factor_1, factor_2)
    """

    def __init__(self, c_s, c_p, rank, relpos_k, template_type):
        """
        åˆå§‹åŒ–å› å­åŒ–é…å¯¹ç‰¹å¾ç½‘ç»œã€‚

        Args:
            c_s: å•ç‰¹å¾ç»´åº¦
            c_p: é…å¯¹ç‰¹å¾ç»´åº¦ (è¾“å‡º)
            rank: å› å­åŒ–ç§© (é€šå¸¸ 2-4)
            relpos_k: ç›¸å¯¹ä½ç½®ç¼–ç çª—å£
            template_type: æ¨¡æ¿ç‰¹å¾ç±»å‹
        """
        super().__init__()

        self.c_s = c_s
        self.c_p = c_p
        self.rank = rank

        # å› å­åŒ– single â†’ pair æŠ•å½±
        # ç”Ÿæˆç§©å› å­åŒ–è¡¨ç¤º
        self.linear_left = nn.Linear(c_s, rank * c_p)
        self.linear_right = nn.Linear(c_s, rank * c_p)

        # å› å­åŒ– relpos
        self.relpos_encoder = FactorizedRelPos(relpos_k, c_p, rank)

        # å› å­åŒ– template
        from genie.model.template import get_template_fn
        template_fn, c_template = get_template_fn(template_type)
        self.template_encoder = FactorizedTemplate(template_fn, c_template, c_p, rank)

    def forward(self, s, t, mask):
        """
        ç”Ÿæˆå› å­åŒ–é…å¯¹è¡¨ç¤ºã€‚

        Args:
            s: å•è¡¨ç¤º [B, L, C_s]
            t: åˆšæ€§å˜æ¢
            mask: åºåˆ—æ©ç  [B, L]

        Returns:
            factor_1: [B, L, rank, C_p] - å·¦å› å­
            factor_2: [B, L, rank, C_p] - å³å› å­

        å› å­åŒ–è¡¨ç¤ºè¿‘ä¼¼å®Œæ•´é…å¯¹å¼ é‡ä¸º:
            p[i, j] â‰ˆ sum_r (factor_1[i, r] * factor_2[j, r])

        è¿™æ˜¯å®Œæ•´é…å¯¹å¼ é‡çš„ä½ç§©è¿‘ä¼¼:
            p[i, j] = s_i + s_j + relpos[i,j] + template[i,j]
        """
        B, L, _ = s.shape

        # æŠ•å½±å•ç‰¹å¾åˆ°å› å­åŒ–å½¢å¼ [B, L, rankÃ—C] â†’ [B, L, rank, C]
        left = self.linear_left(s).view(B, L, self.rank, self.c_p)
        right = self.linear_right(s).view(B, L, self.rank, self.c_p)

        # æ·»åŠ å› å­åŒ–ç›¸å¯¹ä½ç½®ç¼–ç  [L, rank, C]
        relpos_left, relpos_right = self.relpos_encoder(L, s.device)
        left = left + relpos_left.unsqueeze(0)  # [B, L, rank, C]
        right = right + relpos_right.unsqueeze(0)  # [B, L, rank, C]

        # æ·»åŠ å› å­åŒ–æ¨¡æ¿ç‰¹å¾ [B, L, rank, C]
        template_left, template_right = self.template_encoder(t)
        left = left + template_left
        right = right + template_right

        # åº”ç”¨æ©ç  [B, L] â†’ [B, L, 1, 1]
        mask_expanded = mask.unsqueeze(-1).unsqueeze(-1)
        left = left * mask_expanded
        right = right * mask_expanded

        return left, right

    @staticmethod
    def reconstruct_pair(factor_1, factor_2):
        """
        ä»å› å­é‡å»ºå®Œæ•´é…å¯¹å¼ é‡ (ç”¨äºè°ƒè¯•/éªŒè¯)ã€‚

        Args:
            factor_1: [B, L, rank, C]
            factor_2: [B, L, rank, C]

        Returns:
            p: [B, L, L, C] - é‡å»ºçš„é…å¯¹å¼ é‡
        """
        # p[i, j] = sum_r (factor_1[i, r] * factor_2[j, r])
        # ä½¿ç”¨ einsum: 'birc,bjrc->bijc'
        B, L, rank, C = factor_1.shape
        p = torch.einsum('birc,bjrc->bijc', factor_1, factor_2)
        return p


class AdaptiveFactorizationRank(nn.Module):
    """
    æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´å› å­åŒ–ç§©ã€‚

    è¾ƒçŸ­çš„åºåˆ—å¯ä»¥æ‰¿å—æ›´é«˜çš„ç§© (æ›´å¤šè¡¨è¾¾èƒ½åŠ›)ï¼Œ
    è¾ƒé•¿çš„åºåˆ—éœ€è¦æ›´ä½çš„ç§© (æ›´å°‘å†…å­˜)ã€‚
    """

    @staticmethod
    def compute_rank(seq_len, base_rank=2, max_rank=8):
        """
        æ ¹æ®åºåˆ—é•¿åº¦è®¡ç®—å› å­åŒ–ç§©ã€‚

        ç­–ç•¥:
            L < 256:  rank = max_rank (ä¾‹å¦‚ 8)
            256-512:  rank = 4
            512-1024: rank = 2
            > 1024:   rank = 2 (æœ€å°å€¼)

        Args:
            seq_len: åºåˆ—é•¿åº¦
            base_rank: æœ€å°ç§©
            max_rank: æœ€å¤§ç§©

        Returns:
            rank: å› å­åŒ–ç§©
        """
        if seq_len < 256:
            return max_rank
        elif seq_len < 512:
            return max(base_rank * 2, base_rank)
        else:
            return base_rank


class FactorizedPairRefinement(nn.Module):
    """
    è½»é‡çº§å› å­åŒ–é…å¯¹ç‰¹å¾ç²¾ç‚¼ã€‚

    æ­¤æ¨¡å—åœ¨ä¸å®ä¾‹åŒ–å®Œæ•´ LÂ² å¼ é‡çš„æƒ…å†µä¸‹æä¾›é…å¯¹ç‰¹å¾æ›´æ–°ã€‚
    å®ƒä»¥å› å­åŒ–æ–¹å¼æ¨¡æ‹Ÿä¸‰è§’æ›´æ–°çš„æ•ˆæœã€‚

    å¤æ‚åº¦: O(L Ã— rankÂ² Ã— C) è€Œé O(LÂ³ Ã— C)

    æ ¸å¿ƒæ€æƒ³: ä¸åœ¨å®Œæ•´é…å¯¹å¼ é‡ä¸Šæ‰§è¡Œä¸‰è§’ä¹˜æ³•æ›´æ–°ï¼Œ
    è€Œæ˜¯æ‰§è¡Œå› å­åˆ°å› å­çš„äº¤äº’ï¼Œæ•æ‰ç±»ä¼¼çš„ä¿¡æ¯æµã€‚
    """

    def __init__(self, c_p, rank, n_layers=2, dropout=0.1):
        """
        åˆå§‹åŒ–å› å­åŒ–é…å¯¹ç²¾ç‚¼æ¨¡å—ã€‚

        Args:
            c_p: é…å¯¹ç‰¹å¾ç»´åº¦
            rank: å› å­åŒ–ç§©
            n_layers: ç²¾ç‚¼å±‚æ•°
            dropout: Dropout æ¯”ç‡
        """
        super().__init__()
        self.c_p = c_p
        self.rank = rank
        self.n_layers = n_layers

        self.layers = nn.ModuleList([
            FactorizedPairRefinementLayer(c_p, rank, dropout)
            for _ in range(n_layers)
        ])

    def forward(self, factor_1, factor_2, mask):
        """
        ç²¾ç‚¼å› å­åŒ–é…å¯¹ç‰¹å¾ã€‚

        Args:
            factor_1: [B, L, rank, C] å·¦å› å­
            factor_2: [B, L, rank, C] å³å› å­
            mask: [B, L] åºåˆ—æ©ç 

        Returns:
            factor_1: [B, L, rank, C] ç²¾ç‚¼åçš„å·¦å› å­
            factor_2: [B, L, rank, C] ç²¾ç‚¼åçš„å³å› å­
        """
        for layer in self.layers:
            factor_1, factor_2 = layer(factor_1, factor_2, mask)
        return factor_1, factor_2


class FactorizedPairRefinementLayer(nn.Module):
    """
    å•å±‚å› å­åŒ–é…å¯¹ç²¾ç‚¼ã€‚

    é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¨¡æ‹Ÿç±»ä¼¼ä¸‰è§’çš„æ›´æ–°:
    1. è·¨å› å­æ³¨æ„åŠ› (factor_1 å…³æ³¨ factor_2ï¼Œåä¹‹äº¦ç„¶)
    2. è‡ªå› å­ç²¾ç‚¼
    3. é—¨æ§æ®‹å·®è¿æ¥
    """

    def __init__(self, c_p, rank, dropout=0.1):
        super().__init__()
        self.c_p = c_p
        self.rank = rank

        # è·¨å› å­äº¤äº’ (æ¨¡æ‹Ÿä¸‰è§’ä¹˜æ³•æ›´æ–°)
        # factor_1[i] ä¸èšåˆçš„ factor_2 ä¿¡æ¯äº¤äº’
        self.cross_attn_1 = nn.MultiheadAttention(
            embed_dim=c_p,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        self.cross_attn_2 = nn.MultiheadAttention(
            embed_dim=c_p,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )

        # è‡ªç²¾ç‚¼ FFN
        self.ffn_1 = nn.Sequential(
            nn.Linear(c_p, c_p * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(c_p * 4, c_p),
            nn.Dropout(dropout)
        )
        self.ffn_2 = nn.Sequential(
            nn.Linear(c_p, c_p * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(c_p * 4, c_p),
            nn.Dropout(dropout)
        )

        # ç”¨äºæ®‹å·®è¿æ¥çš„é—¨
        self.gate_1 = nn.Sequential(
            nn.Linear(c_p * 2, c_p),
            nn.Sigmoid()
        )
        self.gate_2 = nn.Sequential(
            nn.Linear(c_p * 2, c_p),
            nn.Sigmoid()
        )

        # å±‚å½’ä¸€åŒ–
        self.ln1_1 = nn.LayerNorm(c_p)
        self.ln1_2 = nn.LayerNorm(c_p)
        self.ln2_1 = nn.LayerNorm(c_p)
        self.ln2_2 = nn.LayerNorm(c_p)

        # ç§©æ··åˆ (å…è®¸ç§©ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨)
        self.rank_mix_1 = nn.Linear(rank, rank)
        self.rank_mix_2 = nn.Linear(rank, rank)

    def forward(self, factor_1, factor_2, mask):
        """
        å•ç²¾ç‚¼å±‚çš„å‰å‘ä¼ æ’­ã€‚

        Args:
            factor_1: [B, L, rank, C]
            factor_2: [B, L, rank, C]
            mask: [B, L]

        Returns:
            factor_1_out: [B, L, rank, C]
            factor_2_out: [B, L, rank, C]
        """
        B, L, R, C = factor_1.shape

        # ä»åºåˆ—æ©ç åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        # [B, L] -> [B, L, L] key_padding_mask æ ¼å¼
        attn_mask = ~mask.bool() if mask is not None else None

        # ä¸ºè·¨æ³¨æ„åŠ›åˆ†åˆ«å¤„ç†æ¯ä¸ªç§©
        # è¿™åœ¨å…è®¸å› å­äº¤äº’çš„åŒæ—¶ä¿æŒä½å†…å­˜ä½¿ç”¨
        f1_updates = []
        f2_updates = []

        for r in range(R):
            # æå–ç§©åˆ‡ç‰‡ [B, L, C]
            f1_r = factor_1[:, :, r, :]
            f2_r = factor_2[:, :, r, :]

            # è·¨æ³¨æ„åŠ›: f1 å…³æ³¨ f2
            f1_r_norm = self.ln1_1(f1_r)
            f2_r_norm = self.ln1_2(f2_r)

            f1_cross, _ = self.cross_attn_1(
                f1_r_norm, f2_r_norm, f2_r_norm,
                key_padding_mask=attn_mask
            )

            f2_cross, _ = self.cross_attn_2(
                f2_r_norm, f1_r_norm, f1_r_norm,
                key_padding_mask=attn_mask
            )

            # é—¨æ§æ®‹å·®
            gate_1 = self.gate_1(torch.cat([f1_r, f1_cross], dim=-1))
            gate_2 = self.gate_2(torch.cat([f2_r, f2_cross], dim=-1))

            f1_r = f1_r + gate_1 * f1_cross
            f2_r = f2_r + gate_2 * f2_cross

            # FFN ç²¾ç‚¼
            f1_r = f1_r + self.ffn_1(self.ln2_1(f1_r))
            f2_r = f2_r + self.ffn_2(self.ln2_2(f2_r))

            f1_updates.append(f1_r)
            f2_updates.append(f2_r)

        # å †å å› [B, L, R, C]
        factor_1_out = torch.stack(f1_updates, dim=2)
        factor_2_out = torch.stack(f2_updates, dim=2)

        # ç§©æ··åˆ: å…è®¸ç§©ä¹‹é—´ä¿¡æ¯æµåŠ¨
        # [B, L, R, C] -> [B, L, C, R] -> æ··åˆ -> [B, L, C, R] -> [B, L, R, C]
        factor_1_out = self.rank_mix_1(factor_1_out.permute(0, 1, 3, 2)).permute(0, 1, 3, 2)
        factor_2_out = self.rank_mix_2(factor_2_out.permute(0, 1, 3, 2)).permute(0, 1, 3, 2)

        # åº”ç”¨æ©ç 
        if mask is not None:
            mask_expanded = mask.unsqueeze(-1).unsqueeze(-1)
            factor_1_out = factor_1_out * mask_expanded
            factor_2_out = factor_2_out * mask_expanded

        return factor_1_out, factor_2_out


def test_factorized_pair_features():
    """
    æµ‹è¯•å› å­åŒ–é…å¯¹ç‰¹å¾ä¸æ ‡å‡†å®ç°çš„å¯¹æ¯”ã€‚
    """
    print("=" * 60)
    print("æµ‹è¯•å› å­åŒ–é…å¯¹ç‰¹å¾")
    print("=" * 60)

    # å‚æ•°
    B, L, C_s, C_p = 2, 128, 128, 128
    rank = 2
    relpos_k = 32

    # åˆ›å»ºå› å­åŒ–æ¨¡å‹
    from genie.model.template import get_template_fn
    factorized_net = FactorizedPairFeatureNet(
        c_s=C_s,
        c_p=C_p,
        rank=rank,
        relpos_k=relpos_k,
        template_type='v1'
    )

    # æµ‹è¯•è¾“å…¥
    s = torch.randn(B, L, C_s)
    from genie.flash_ipa.rigid import create_identity_rigid
    t = create_identity_rigid(B, L)
    mask = torch.ones(B, L)

    # å‰å‘ä¼ æ’­
    factor_1, factor_2 = factorized_net(s, t, mask)

    # æ£€æŸ¥å½¢çŠ¶
    assert factor_1.shape == (B, L, rank, C_p), f"æœŸæœ› {(B, L, rank, C_p)}ï¼Œå¾—åˆ° {factor_1.shape}"
    assert factor_2.shape == (B, L, rank, C_p), f"æœŸæœ› {(B, L, rank, C_p)}ï¼Œå¾—åˆ° {factor_2.shape}"

    # é‡å»ºé…å¯¹å¼ é‡
    p_reconstructed = FactorizedPairFeatureNet.reconstruct_pair(factor_1, factor_2)
    assert p_reconstructed.shape == (B, L, L, C_p)

    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    factor_memory = factor_1.numel() * 4 + factor_2.numel() * 4  # å­—èŠ‚
    full_memory = L * L * C_p * 4  # å‡è®¾å®Œæ•´é…å¯¹å¼ é‡

    print(f"âœ… å½¢çŠ¶æµ‹è¯•é€šè¿‡")
    print(f"âœ… å› å­ 1: {factor_1.shape}")
    print(f"âœ… å› å­ 2: {factor_2.shape}")
    print(f"âœ… é‡å»º: {p_reconstructed.shape}")
    print(f"")
    print(f"å†…å­˜å¯¹æ¯”:")
    print(f"  å› å­åŒ–: {factor_memory / 1024 / 1024:.2f} MB")
    print(f"  å®Œæ•´é…å¯¹: {full_memory / 1024 / 1024:.2f} MB")
    print(f"  èŠ‚çœ: {full_memory / factor_memory:.1f}x")
    print(f"")
    print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    test_factorized_pair_features()
