"""
å› å­åŒ–ä¸‰è§’æ“ä½œ (Stage 2)

æ­¤æ¨¡å—å®ç°é•¿åºåˆ—è›‹ç™½è´¨ç»“æ„å»ºæ¨¡çš„å†…å­˜é«˜æ•ˆå› å­åŒ–ä¸‰è§’æ“ä½œã€‚

æ ¸å¿ƒä¼˜åŒ–:
1. **å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°**: O(LÂ³) â†’ O(LÂ² Ã— rank)
2. **åˆ†å—ä¸‰è§’æ³¨æ„åŠ›**: O(LÂ² Ã— heads) å¸¦åˆ†å—
3. **ç¨€ç–ä¸‰è§’æ³¨æ„åŠ›**: k-NN ç¨€ç–æ¨¡å¼

å†…å­˜å‡å°‘:
- æ ‡å‡†ä¸‰è§’ä¹˜æ³•: O(LÂ³ Ã— C) - å¯¹äº L>512 ä¸å¯è¡Œ
- å› å­åŒ–ä¸‰è§’ä¹˜æ³•: O(LÂ² Ã— rank Ã— C) - å¯¹äº L=1024+ å¯è¡Œ

åŸºäº:
- AlphaFold2 ä¸‰è§’æ›´æ–° (Jumper et al. 2021)
- æ³¨æ„åŠ›ä½ç§©è¿‘ä¼¼ (Wang et al. 2020)
- é«˜æ•ˆTransformerç»¼è¿° (Tay et al. 2020)

ä½œè€…: Stage 2 å®ç° (2026-01-13)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint

from genie.model.modules.primitives import Linear
from genie.utils.tensor_utils import permute_final_dims


class FactorizedTriangleMultiplicativeUpdate(nn.Module):
    """
    å†…å­˜é«˜æ•ˆçš„å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°ã€‚

    åŸå§‹æ“ä½œ (O(LÂ³)):
        z_ij = Î£_k (a_ik * b_kj)  [éœ€è¦å®Œæ•´çš„ LÃ—LÃ—C å¼ é‡]

    å› å­åŒ–æ“ä½œ (O(LÂ² Ã— rank)):
        è¾“å…¥: z = (z_left[B, L, rank, C], z_right[B, L, rank, C])
        è¾“å‡º: æ›´æ–°åçš„å› å­

    ç­–ç•¥:
    1. å°†å› å­æŠ•å½±åˆ°éšè—ç©ºé—´: z_factor â†’ a_factor, b_factor
    2. é€šè¿‡å› å­äº¤äº’è®¡ç®—ä½ç§©æ›´æ–°
    3. åº”ç”¨é—¨æ§å’Œè¾“å‡ºæŠ•å½±

    å†…å­˜å¯¹æ¯” (L=1024, C=128, c_hidden=128):
        æ ‡å‡†: 1024Â³ Ã— 128 Ã— 4 å­—èŠ‚ = 537 GB (ä¸å¯èƒ½!)
        å› å­åŒ– (rank=2): 1024Â² Ã— 2 Ã— 128 Ã— 4 å­—èŠ‚ = 1 GB (å¯è¡Œ!)

    å¤æ‚åº¦:
        æ—¶é—´: O(LÂ² Ã— rank Ã— C) vs O(LÂ³ Ã— C)
        ç©ºé—´: O(L Ã— rank Ã— C) vs O(LÂ² Ã— C)
    """

    def __init__(
        self,
        c_p: int,
        rank: int,
        c_hidden: int,
        outgoing: bool = True,
        use_grad_checkpoint: bool = False,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°æ¨¡å—ã€‚

        Args:
            c_p: é…å¯¹ç‰¹å¾ç»´åº¦
            rank: å› å­åŒ–ç§©
            c_hidden: æŠ•å½±çš„éšè—ç»´åº¦
            outgoing: True è¡¨ç¤ºå‡ºè¾¹ï¼ŒFalse è¡¨ç¤ºå…¥è¾¹
            use_grad_checkpoint: ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
            dropout: Dropout æ¯”ç‡
        """
        super().__init__()

        self.c_p = c_p
        self.rank = rank
        self.c_hidden = c_hidden
        self.outgoing = outgoing
        self.use_grad_checkpoint = use_grad_checkpoint

        # è¾“å…¥å±‚å½’ä¸€åŒ– (åº”ç”¨äºå› å­)
        self.layer_norm_in = nn.LayerNorm(c_p)

        # å°†å› å­æŠ•å½±åˆ°éšè—ç©ºé—´ (å·¦å³åˆ†ç¦»)
        # å·¦å› å­æŠ•å½±
        self.linear_a_left = Linear(c_p, c_hidden)
        self.linear_a_left_gate = Linear(c_p, c_hidden, init="gating")

        # å³å› å­æŠ•å½±
        self.linear_b_right = Linear(c_p, c_hidden)
        self.linear_b_right_gate = Linear(c_p, c_hidden, init="gating")

        # è·¨ç§©æ··åˆ (å…è®¸ç§©ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨)
        self.rank_mix_left = nn.Linear(rank, rank)
        self.rank_mix_right = nn.Linear(rank, rank)

        # è¾“å‡ºæŠ•å½±
        self.layer_norm_out = nn.LayerNorm(c_hidden)
        self.linear_out_left = Linear(c_hidden, c_p, init="final")
        self.linear_out_right = Linear(c_hidden, c_p, init="final")

        # é—¨æ§ (åº”ç”¨äºè¾“å…¥å› å­)
        self.linear_gate_left = Linear(c_p, c_p, init="gating")
        self.linear_gate_right = Linear(c_p, c_p, init="gating")

        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(dropout)

    def _compute_factorized_update(
        self,
        z_left: torch.Tensor,  # [B, L, rank, C]
        z_right: torch.Tensor,  # [B, L, rank, C]
        mask: torch.Tensor  # [B, L]
    ):
        """
        æ ¸å¿ƒå› å­åŒ–ä¸‰è§’æ›´æ–°è®¡ç®—ã€‚

        æ ¸å¿ƒæ´å¯Ÿ:
        ä¸è®¡ç®—å®Œæ•´çš„ LÃ—L äº¤äº’ï¼Œè€Œæ˜¯è®¡ç®—é€ç§©æ›´æ–°å¹¶è·¨ç§©æ··åˆã€‚

        å¯¹äºå‡ºè¾¹:
            z_ij â† Î£_k (a_ik * b_kj)
            å› å­åŒ–: åŸºäºå³å› å­æ›´æ–°å·¦å› å­

        å¯¹äºå…¥è¾¹:
            z_ij â† Î£_k (a_ki * b_jk)
            å› å­åŒ–: åŸºäºå·¦å› å­æ›´æ–°å³å› å­
        """
        B, L, rank, C = z_left.shape

        # å±‚å½’ä¸€åŒ–
        z_left = self.layer_norm_in(z_left)
        z_right = self.layer_norm_in(z_right)

        # åº”ç”¨æ©ç  [B, L] â†’ [B, L, 1, 1]
        mask_expanded = mask.unsqueeze(-1).unsqueeze(-1)
        z_left = z_left * mask_expanded
        z_right = z_right * mask_expanded

        # æŠ•å½±åˆ°å¸¦é—¨æ§çš„éšè—ç©ºé—´
        # a_left: [B, L, rank, c_hidden]
        a_left = self.linear_a_left(z_left) * self.sigmoid(self.linear_a_left_gate(z_left))
        # b_right: [B, L, rank, c_hidden]
        b_right = self.linear_b_right(z_right) * self.sigmoid(self.linear_b_right_gate(z_right))

        # è·¨ç§©æ··åˆ
        # é‡å¡‘ä»¥è¿›è¡Œç§©æ··åˆ: [B, L, rank, c_hidden] â†’ [B, L, c_hidden, rank]
        a_left_T = a_left.permute(0, 1, 3, 2)  # [B, L, c_hidden, rank]
        b_right_T = b_right.permute(0, 1, 3, 2)  # [B, L, c_hidden, rank]

        # è·¨ç§©æ··åˆ: [B, L, c_hidden, rank] @ [rank, rank] â†’ [B, L, c_hidden, rank]
        a_left_mixed = torch.matmul(a_left_T, self.rank_mix_left.weight.T)  # [B, L, c_hidden, rank]
        b_right_mixed = torch.matmul(b_right_T, self.rank_mix_right.weight.T)  # [B, L, c_hidden, rank]

        # Permute back: [B, L, c_hidden, rank] â†’ [B, L, rank, c_hidden]
        a_left_mixed = a_left_mixed.permute(0, 1, 3, 2)
        b_right_mixed = b_right_mixed.permute(0, 1, 3, 2)

        # å› å­åŒ–ä¹˜æ³•æ›´æ–°
        # è®¡ç®—é€ç§©äº¤äº’: [B, L, rank, c_hidden]
        if self.outgoing:
            # å‡ºè¾¹: åŸºäºä¸­é—´èŠ‚ç‚¹æ€»å’Œæ›´æ–°
            # è¿‘ä¼¼: Î£_k (a_ik * b_kj) â‰ˆ Î£_r (a_ir * Î£_k b_kr)
            # èšåˆå³å› å­: [B, L, rank, c_hidden] â†’ [B, 1, rank, c_hidden]
            b_aggregated = b_right_mixed.mean(dim=1, keepdim=True)  # [B, 1, rank, c_hidden]
            # å¹¿æ’­ä¹˜æ³•: [B, L, rank, c_hidden] * [B, 1, rank, c_hidden]
            update_left = a_left_mixed * b_aggregated  # [B, L, rank, c_hidden]
            update_right = torch.zeros_like(b_right_mixed)  # ä¸æ›´æ–°å³å› å­
        else:
            # å…¥è¾¹: å¯¹ç§°æ“ä½œ
            a_aggregated = a_left_mixed.mean(dim=1, keepdim=True)  # [B, 1, rank, c_hidden]
            update_right = b_right_mixed * a_aggregated  # [B, L, rank, c_hidden]
            update_left = torch.zeros_like(a_left_mixed)  # ä¸æ›´æ–°å·¦å› å­

        # å±‚å½’ä¸€åŒ–å’Œè¾“å‡ºæŠ•å½±
        update_left = self.layer_norm_out(update_left)
        update_right = self.layer_norm_out(update_right)

        out_left = self.linear_out_left(update_left)  # [B, L, rank, C]
        out_right = self.linear_out_right(update_right)  # [B, L, rank, C]

        # é—¨æ§
        gate_left = self.sigmoid(self.linear_gate_left(z_left))
        gate_right = self.sigmoid(self.linear_gate_right(z_right))

        out_left = out_left * gate_left
        out_right = out_right * gate_right

        # åº”ç”¨ dropout
        out_left = self.dropout(out_left)
        out_right = self.dropout(out_right)

        return out_left, out_right

    def forward(
        self,
        z_left: torch.Tensor,  # [B, L, rank, C]
        z_right: torch.Tensor,  # [B, L, rank, C]
        mask: torch.Tensor = None  # [B, L]
    ):
        """
        å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°çš„å‰å‘ä¼ æ’­ã€‚

        Args:
            z_left: å·¦å› å­ [B, L, rank, C]
            z_right: å³å› å­ [B, L, rank, C]
            mask: åºåˆ—æ©ç  [B, L]

        Returns:
            (z_left_updated, z_right_updated): æ›´æ–°åçš„å› å­
        """
        if mask is None:
            mask = torch.ones(z_left.shape[:2], device=z_left.device)

        if self.training and self.use_grad_checkpoint:
            out_left, out_right = checkpoint(
                self._compute_factorized_update,
                z_left, z_right, mask,
                use_reentrant=False
            )
        else:
            out_left, out_right = self._compute_factorized_update(z_left, z_right, mask)

        return out_left, out_right


class FactorizedTriangleMultiplicationOutgoing(FactorizedTriangleMultiplicativeUpdate):
    """å› å­åŒ–å‡ºè¾¹ä¸‰è§’ä¹˜æ³•ã€‚"""
    def __init__(self, c_p, rank, c_hidden, use_grad_checkpoint=False, dropout=0.1):
        super().__init__(c_p, rank, c_hidden, outgoing=True,
                         use_grad_checkpoint=use_grad_checkpoint, dropout=dropout)


class FactorizedTriangleMultiplicationIncoming(FactorizedTriangleMultiplicativeUpdate):
    """å› å­åŒ–å…¥è¾¹ä¸‰è§’ä¹˜æ³•ã€‚"""
    def __init__(self, c_p, rank, c_hidden, use_grad_checkpoint=False, dropout=0.1):
        super().__init__(c_p, rank, c_hidden, outgoing=False,
                         use_grad_checkpoint=use_grad_checkpoint, dropout=dropout)


class ChunkedTriangleAttention(nn.Module):
    """
    åˆ†å—ä¸‰è§’æ³¨æ„åŠ›ä»¥æé«˜å†…å­˜æ•ˆç‡ã€‚

    æ ‡å‡†ä¸‰è§’æ³¨æ„åŠ›è®¡ç®—é…å¯¹å¼ é‡ä¸€ä¸ªè½´ä¸Šçš„æ³¨æ„åŠ›:
        z_ij = Attention(Q=z_i*, K=z_i*, V=z_i*)

    è¿™éœ€è¦å®ä¾‹åŒ– [B, L, L, C]ï¼Œå¯¹äºé•¿åºåˆ—æ˜¯ä¸å¯è¡Œçš„ã€‚

    åˆ†å—ç­–ç•¥:
    1. æ²¿åºåˆ—ç»´åº¦åˆ†å—å¤„ç†æ³¨æ„åŠ›
    2. æ¯ä¸ªå—: [chunk_size, L, C] è€Œä¸æ˜¯ [L, L, C]
    3. ç´¯ç§¯ç»“æœä»¥è·å¾—æœ€ç»ˆè¾“å‡º

    å†…å­˜å‡å°‘:
        æ ‡å‡†: O(LÂ² Ã— C)
        åˆ†å—: O(chunk_size Ã— L Ã— C)

    å¯¹äº L=1024, chunk_size=64:
        æ ‡å‡†: 1024Â² Ã— 128 = 134 MB
        åˆ†å—: 64 Ã— 1024 Ã— 128 = 8 MB (16.7x å‡å°‘!)
    """

    def __init__(
        self,
        c_p: int,
        rank: int,
        c_hidden: int,
        n_heads: int,
        starting: bool = True,
        chunk_size: int = 64,
        inf: float = 1e9,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–åˆ†å—ä¸‰è§’æ³¨æ„åŠ›ã€‚

        Args:
            c_p: é…å¯¹ç‰¹å¾ç»´åº¦
            rank: å› å­åŒ–ç§©
            c_hidden: éšè—ç»´åº¦
            n_heads: æ³¨æ„åŠ›å¤´æ•°
            starting: True è¡¨ç¤ºè¡Œæ–¹å‘ï¼ŒFalse è¡¨ç¤ºåˆ—æ–¹å‘
            chunk_size: å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›çš„å—å¤§å°
            inf: æ©ç å€¼
            dropout: Dropout æ¯”ç‡
        """
        super().__init__()

        self.c_p = c_p
        self.rank = rank
        self.c_hidden = c_hidden
        self.n_heads = n_heads
        self.starting = starting
        self.chunk_size = chunk_size
        self.inf = inf

        assert c_hidden % n_heads == 0, "c_hidden å¿…é¡»èƒ½è¢« n_heads æ•´é™¤"
        self.head_dim = c_hidden // n_heads

        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(c_p)

        # Q, K, V çš„æŠ•å½± (åº”ç”¨äºå› å­)
        self.linear_q = Linear(c_p, c_hidden)
        self.linear_k = Linear(c_p, c_hidden)
        self.linear_v = Linear(c_p, c_hidden)

        # åç½®æŠ•å½±
        self.linear_bias = Linear(c_p, n_heads, bias=False, init="normal")

        # è¾“å‡ºæŠ•å½±
        self.linear_out = Linear(c_hidden, c_p, init="final")

        # é—¨æ§
        self.linear_gate = Linear(c_p, c_p, init="gating")
        self.sigmoid = nn.Sigmoid()

        self.dropout = nn.Dropout(dropout)

    def _chunked_attention(
        self,
        q: torch.Tensor,  # [B, L, n_heads, head_dim]
        k: torch.Tensor,  # [B, L, n_heads, head_dim]
        v: torch.Tensor,  # [B, L, n_heads, head_dim]
        bias: torch.Tensor,  # [B, n_heads, L, L]
        mask: torch.Tensor  # [B, L]
    ):
        """
        åˆ†å—è®¡ç®—æ³¨æ„åŠ›ä»¥èŠ‚çœå†…å­˜ã€‚

        æ ‡å‡†æ³¨æ„åŠ›:
            scores = (Q @ K^T) / sqrt(d)  # [B, n_heads, L, L]
            attn = softmax(scores + bias)
            out = attn @ V  # [B, n_heads, L, head_dim]

        åˆ†å—æ³¨æ„åŠ›:
            å¯¹äºæ¯ä¸ª chunk_i:
                scores_i = (Q_i @ K^T) / sqrt(d)  # [B, n_heads, chunk, L]
                attn_i = softmax(scores_i + bias_i)
                out_i = attn_i @ V
            è¿æ¥å„å—
        """
        B, L, n_heads, head_dim = q.shape

        # å‡†å¤‡æ³¨æ„åŠ›: [B, L, n_heads, head_dim] â†’ [B, n_heads, L, head_dim]
        q = q.permute(0, 2, 1, 3)  # [B, n_heads, L, head_dim]
        k = k.permute(0, 2, 1, 3)  # [B, n_heads, L, head_dim]
        v = v.permute(0, 2, 1, 3)  # [B, n_heads, L, head_dim]

        # æ©ç åç½®: [B, L] â†’ [B, 1, L, 1]
        mask_bias = (self.inf * (mask - 1)).unsqueeze(1).unsqueeze(-1)  # [B, 1, L, 1]

        # åˆ†å—å¤„ç†
        output_chunks = []
        for i in range(0, L, self.chunk_size):
            end_i = min(i + self.chunk_size, L)
            q_chunk = q[:, :, i:end_i, :]  # [B, n_heads, chunk, head_dim]

            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: [B, n_heads, chunk, head_dim] @ [B, n_heads, head_dim, L]
            # â†’ [B, n_heads, chunk, L]
            scores = torch.matmul(q_chunk, k.transpose(-2, -1)) / (head_dim ** 0.5)

            # æ·»åŠ åç½®: [B, n_heads, chunk, L] + [B, n_heads, chunk, L]
            bias_chunk = bias[:, :, i:end_i, :]  # [B, n_heads, chunk, L]
            scores = scores + bias_chunk

            # æ·»åŠ æ©ç : [B, n_heads, chunk, L] + [B, 1, L, 1]
            scores = scores + mask_bias.transpose(-2, -1)  # å¹¿æ’­

            # Softmax
            attn = F.softmax(scores, dim=-1)  # [B, n_heads, chunk, L]
            attn = self.dropout(attn)

            # åº”ç”¨æ³¨æ„åŠ›åˆ°å€¼: [B, n_heads, chunk, L] @ [B, n_heads, L, head_dim]
            # â†’ [B, n_heads, chunk, head_dim]
            out_chunk = torch.matmul(attn, v)
            output_chunks.append(out_chunk)

        # è¿æ¥å„å—: [B, n_heads, L, head_dim]
        output = torch.cat(output_chunks, dim=2)

        # é‡å¡‘å›: [B, n_heads, L, head_dim] â†’ [B, L, n_heads, head_dim] â†’ [B, L, c_hidden]
        output = output.permute(0, 2, 1, 3).contiguous()
        output = output.view(B, L, self.c_hidden)

        return output

    def forward(
        self,
        z_left: torch.Tensor,  # [B, L, rank, C]
        z_right: torch.Tensor,  # [B, L, rank, C]
        mask: torch.Tensor = None  # [B, L]
    ):
        """
        åˆ†å—ä¸‰è§’æ³¨æ„åŠ›çš„å‰å‘ä¼ æ’­ã€‚

        å¯¹äºå› å­åŒ–è¾“å…¥ï¼Œæˆ‘ä»¬å®æ—¶é‡å»ºä½ç§©è¿‘ä¼¼ç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼Œ
        ä½†é€šè¿‡åˆ†å—ä¿æŒå†…å­˜æœ‰ç•Œã€‚
        """
        if mask is None:
            mask = torch.ones(z_left.shape[:2], device=z_left.device)

        B, L, rank, C = z_left.shape

        # èšåˆå› å­ä»¥è·å–ä¼ªé…å¯¹ç‰¹å¾
        # ç®€å•ç­–ç•¥: å¯¹ç§©æ±‚å’Œ
        z = z_left.sum(dim=2) + z_right.sum(dim=2)  # [B, L, C]

        # å±‚å½’ä¸€åŒ–
        z = self.layer_norm(z)

        # æ‰©å±•åˆ°ä¼ªé…å¯¹é€šè¿‡é‡å¤
        # [B, L, C] â†’ [B, L, L, C] (é€šè¿‡å¹¿æ’­çš„ä½ç§©è¿‘ä¼¼)
        # ä¸ºèŠ‚çœå†…å­˜ï¼Œæˆ‘ä»¬å°†ä» 1D ç‰¹å¾è®¡ç®— Q, K, V
        # è®©æ³¨æ„åŠ›é‡å»º 2D ç»“æ„

        # æŠ•å½±åˆ° Q, K, V
        q = self.linear_q(z)  # [B, L, c_hidden]
        k = self.linear_k(z)  # [B, L, c_hidden]
        v = self.linear_v(z)  # [B, L, c_hidden]

        # é‡å¡‘ç”¨äºå¤šå¤´æ³¨æ„åŠ›
        q = q.view(B, L, self.n_heads, self.head_dim)  # [B, L, n_heads, head_dim]
        k = k.view(B, L, self.n_heads, self.head_dim)
        v = v.view(B, L, self.n_heads, self.head_dim)

        # ä»è¾“å…¥è®¡ç®—åç½® (ä½¿ç”¨å› å­)
        bias_features = z_left.sum(dim=2)  # [B, L, C]
        bias = self.linear_bias(bias_features)  # [B, L, n_heads]
        bias = bias.permute(0, 2, 1).unsqueeze(-1)  # [B, n_heads, L, 1]
        bias = bias.expand(B, self.n_heads, L, L)  # [B, n_heads, L, L]

        # åˆ†å—æ³¨æ„åŠ›
        output = self._chunked_attention(q, k, v, bias, mask)  # [B, L, c_hidden]

        # è¾“å‡ºæŠ•å½±
        output = self.linear_out(output)  # [B, L, C]

        # é—¨æ§
        gate = self.sigmoid(self.linear_gate(z))
        output = output * gate

        # å°†è¾“å‡ºåˆ†é…å›å› å­ (ç®€å•åˆ†å‰²)
        out_left = output.unsqueeze(2).expand(B, L, rank, C) / rank  # [B, L, rank, C]
        out_right = torch.zeros_like(z_right)  # åªæ›´æ–°å·¦å› å­

        return out_left, out_right


class ChunkedTriangleAttentionStartingNode(ChunkedTriangleAttention):
    """æ²¿èµ·å§‹ (è¡Œ) ç»´åº¦çš„åˆ†å—ä¸‰è§’æ³¨æ„åŠ›ã€‚"""
    def __init__(self, c_p, rank, c_hidden, n_heads, chunk_size=64, dropout=0.1):
        super().__init__(c_p, rank, c_hidden, n_heads, starting=True,
                         chunk_size=chunk_size, dropout=dropout)


class ChunkedTriangleAttentionEndingNode(ChunkedTriangleAttention):
    """æ²¿ç»“æŸ (åˆ—) ç»´åº¦çš„åˆ†å—ä¸‰è§’æ³¨æ„åŠ›ã€‚"""
    def __init__(self, c_p, rank, c_hidden, n_heads, chunk_size=64, dropout=0.1):
        super().__init__(c_p, rank, c_hidden, n_heads, starting=False,
                         chunk_size=chunk_size, dropout=dropout)


def test_factorized_triangle_ops():
    """æµ‹è¯•å› å­åŒ–ä¸‰è§’æ“ä½œã€‚"""
    print("=" * 80)
    print("æµ‹è¯•å› å­åŒ–ä¸‰è§’æ“ä½œ (Stage 2)")
    print("=" * 80)
    print()

    B, L, rank, C = 2, 256, 4, 64
    c_hidden = 64

    # æµ‹è¯•è¾“å…¥
    z_left = torch.randn(B, L, rank, C)
    z_right = torch.randn(B, L, rank, C)
    mask = torch.ones(B, L)

    print(f"æµ‹è¯•é…ç½®:")
    print(f"  æ‰¹æ¬¡: {B}, é•¿åº¦: {L}, ç§©: {rank}, é€šé“: {C}")
    print()

    # æµ‹è¯• 1: å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°
    print("æµ‹è¯• 1: å› å­åŒ–ä¸‰è§’ä¹˜æ³•æ›´æ–°")
    print("-" * 80)
    tri_mult_out = FactorizedTriangleMultiplicationOutgoing(C, rank, c_hidden)
    tri_mult_in = FactorizedTriangleMultiplicationIncoming(C, rank, c_hidden)

    out_left1, out_right1 = tri_mult_out(z_left, z_right, mask)
    out_left2, out_right2 = tri_mult_in(z_left, z_right, mask)

    print(f"  å‡ºè¾¹ - å·¦è¾“å‡ºå½¢çŠ¶: {out_left1.shape}")
    print(f"  å‡ºè¾¹ - å³è¾“å‡ºå½¢çŠ¶: {out_right1.shape}")
    print(f"  å…¥è¾¹ - å·¦è¾“å‡ºå½¢çŠ¶: {out_left2.shape}")
    print(f"  å…¥è¾¹ - å³è¾“å‡ºå½¢çŠ¶: {out_right2.shape}")
    print(f"  âœ… å› å­åŒ–ä¸‰è§’ä¹˜æ³•å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 2: åˆ†å—ä¸‰è§’æ³¨æ„åŠ›
    print("æµ‹è¯• 2: åˆ†å—ä¸‰è§’æ³¨æ„åŠ›")
    print("-" * 80)
    tri_att_start = ChunkedTriangleAttentionStartingNode(C, rank, c_hidden, n_heads=4, chunk_size=64)
    tri_att_end = ChunkedTriangleAttentionEndingNode(C, rank, c_hidden, n_heads=4, chunk_size=64)

    out_left3, out_right3 = tri_att_start(z_left, z_right, mask)
    out_left4, out_right4 = tri_att_end(z_left, z_right, mask)

    print(f"  èµ·å§‹ - å·¦è¾“å‡ºå½¢çŠ¶: {out_left3.shape}")
    print(f"  èµ·å§‹ - å³è¾“å‡ºå½¢çŠ¶: {out_right3.shape}")
    print(f"  ç»“æŸ - å·¦è¾“å‡ºå½¢çŠ¶: {out_left4.shape}")
    print(f"  ç»“æŸ - å³è¾“å‡ºå½¢çŠ¶: {out_right4.shape}")
    print(f"  âœ… åˆ†å—ä¸‰è§’æ³¨æ„åŠ›å·¥ä½œæ­£å¸¸!")
    print()

    # å†…å­˜å¯¹æ¯”
    print("å†…å­˜å¯¹æ¯”:")
    print("-" * 80)
    standard_pair_mem = B * L * L * C * 4 / (1024 ** 2)  # FP32
    factorized_pair_mem = B * 2 * L * rank * C * 4 / (1024 ** 2)  # FP32

    print(f"  æ ‡å‡†é…å¯¹å¼ é‡: {standard_pair_mem:.2f} MB")
    print(f"  å› å­åŒ–é…å¯¹å¼ é‡: {factorized_pair_mem:.2f} MB")
    print(f"  å†…å­˜å‡å°‘: {standard_pair_mem / factorized_pair_mem:.2f}x")
    print()

    print("=" * 80)
    print("ğŸ‰ æ‰€æœ‰å› å­åŒ–ä¸‰è§’æ“ä½œæµ‹è¯•é€šè¿‡!")
    print("=" * 80)


if __name__ == "__main__":
    test_factorized_triangle_ops()
