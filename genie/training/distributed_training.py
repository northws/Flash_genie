"""
åˆ†å¸ƒå¼è®­ç»ƒå·¥å…· (Stage 5)

æ­¤æ¨¡å—æä¾›ç”¨äºæ‰©å±•åˆ°å¤šä¸ª GPU çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·:
1. æ•°æ®å¹¶è¡Œè®­ç»ƒ (DDP)
2. é•¿åºåˆ—çš„å¼ é‡å¹¶è¡Œ
3. ç®¡é“å¹¶è¡Œ (å¯é€‰)
4. æ¢¯åº¦ç´¯ç§¯è¾…åŠ©å·¥å…·

æ ¸å¿ƒä¼˜åŠ¿:
- å¤š GPU 4-8x ååé‡
- é€šè¿‡å¼ é‡å¹¶è¡Œæ”¯æŒè¶…é•¿åºåˆ—
- é«˜æ•ˆæ¢¯åº¦åŒæ­¥

åŸºäº:
- PyTorch åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
- Megatron-LM (å¼ é‡å¹¶è¡Œ)
- AlphaFold2 åˆ†å¸ƒå¼è®­ç»ƒ

ä½œè€…: Stage 5 å®ç° (2026-01-13)
"""

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from typing import Optional, Dict, Any, Tuple
import os


class DistributedConfig:
    """
    åˆ†å¸ƒå¼è®­ç»ƒé…ç½®ã€‚
    """

    def __init__(
        self,
        # åŸºæœ¬è®¾ç½®
        world_size: int = 1,
        rank: int = 0,
        local_rank: int = 0,
        backend: str = "nccl",  # GPU ç”¨ "nccl"ï¼ŒCPU ç”¨ "gloo"
        # ç­–ç•¥
        strategy: str = "ddp",  # "ddp", "tensor_parallel", "hybrid"
        # DDP è®¾ç½®
        find_unused_parameters: bool = False,
        gradient_as_bucket_view: bool = True,
        # å¼ é‡å¹¶è¡Œè®¾ç½®
        tensor_parallel_size: int = 1,
        sequence_parallel: bool = False,  # åˆ†å‰²åºåˆ—ç»´åº¦
        # é€šä¿¡
        all_reduce_bucket_size_mb: int = 25,
    ):
        """
        Args:
            world_size: è¿›ç¨‹æ€»æ•°
            rank: æ­¤è¿›ç¨‹çš„å…¨å±€æ’å
            local_rank: æ­¤èŠ‚ç‚¹ä¸Šçš„æœ¬åœ°æ’å
            backend: é€šä¿¡åç«¯
            strategy: å¹¶è¡Œç­–ç•¥
            find_unused_parameters: åœ¨ DDP ä¸­æŸ¥æ‰¾æœªä½¿ç”¨çš„å‚æ•°
            gradient_as_bucket_view: ä¸ºæ¢¯åº¦ä½¿ç”¨æ¡¶è§†å›¾
            tensor_parallel_size: å¼ é‡å¹¶è¡Œç»„çš„å¤§å°
            sequence_parallel: å¯ç”¨åºåˆ—å¹¶è¡Œ
            all_reduce_bucket_size_mb: all-reduce çš„æ¡¶å¤§å°
        """
        self.world_size = world_size
        self.rank = rank
        self.local_rank = local_rank
        self.backend = backend
        self.strategy = strategy
        self.find_unused_parameters = find_unused_parameters
        self.gradient_as_bucket_view = gradient_as_bucket_view
        self.tensor_parallel_size = tensor_parallel_size
        self.sequence_parallel = sequence_parallel
        self.all_reduce_bucket_size_mb = all_reduce_bucket_size_mb

    @staticmethod
    def from_env() -> "DistributedConfig":
        """
        ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®ã€‚

        æœŸæœ›:
            WORLD_SIZE: è¿›ç¨‹æ€»æ•°
            RANK: å…¨å±€æ’å
            LOCAL_RANK: èŠ‚ç‚¹ä¸Šçš„æœ¬åœ°æ’å
        """
        return DistributedConfig(
            world_size=int(os.environ.get("WORLD_SIZE", 1)),
            rank=int(os.environ.get("RANK", 0)),
            local_rank=int(os.environ.get("LOCAL_RANK", 0)),
        )

    def is_distributed(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚"""
        return self.world_size > 1

    def is_main_process(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ (rank 0)ã€‚"""
        return self.rank == 0


def setup_distributed(config: DistributedConfig) -> bool:
    """
    åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒã€‚

    Args:
        config: åˆ†å¸ƒå¼é…ç½®

    Returns:
        å¦‚æœåˆ†å¸ƒå¼è®­ç»ƒå·²åˆå§‹åŒ–åˆ™è¿”å› True
    """
    if not config.is_distributed():
        return False

    if not dist.is_initialized():
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group(
            backend=config.backend,
            world_size=config.world_size,
            rank=config.rank,
        )

        # è®¾ç½®è®¾å¤‡
        if torch.cuda.is_available():
            torch.cuda.set_device(config.local_rank)

    return True


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒã€‚"""
    if dist.is_initialized():
        dist.destroy_process_group()


class DistributedModelWrapper:
    """
    åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒçš„åŒ…è£…å™¨ã€‚

    å¤„ç†:
    - æ¨¡å‹åˆ†å‘ (DDP)
    - æ¢¯åº¦åŒæ­¥
    - åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹
    """

    def __init__(
        self,
        model: nn.Module,
        config: DistributedConfig,
        sync_batch_norm: bool = False,
    ):
        """
        Args:
            model: è¦åŒ…è£…çš„æ¨¡å‹
            config: åˆ†å¸ƒå¼é…ç½®
            sync_batch_norm: ä½¿ç”¨åŒæ­¥æ‰¹å½’ä¸€åŒ–
        """
        self.config = config
        self.is_distributed = config.is_distributed()

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        if torch.cuda.is_available():
            self.device = torch.device(f"cuda:{config.local_rank}")
            model = model.to(self.device)
        else:
            self.device = torch.device("cpu")

        # å¦‚æœæ˜¯åˆ†å¸ƒå¼åˆ™ç”¨ DDP åŒ…è£…
        if self.is_distributed:
            if sync_batch_norm:
                model = nn.SyncBatchNorm.convert_sync_batchnorm(model)

            self.model = DDP(
                model,
                device_ids=[config.local_rank] if torch.cuda.is_available() else None,
                output_device=config.local_rank if torch.cuda.is_available() else None,
                find_unused_parameters=config.find_unused_parameters,
                gradient_as_bucket_view=config.gradient_as_bucket_view,
            )
        else:
            self.model = model

    def get_model(self) -> nn.Module:
        """è·å–åº•å±‚æ¨¡å‹ (å¦‚éœ€è¦åˆ™è§£åŒ… DDP)ã€‚"""
        if isinstance(self.model, DDP):
            return self.model.module
        return self.model

    def save_checkpoint(self, path: str, **kwargs):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹ (ä»…åœ¨ä¸»è¿›ç¨‹)ã€‚

        Args:
            path: ä¿å­˜æ£€æŸ¥ç‚¹çš„è·¯å¾„
            **kwargs: è¦ä¿å­˜çš„é™„åŠ é¡¹
        """
        if not self.config.is_main_process():
            return

        checkpoint = {
            "model": self.get_model().state_dict(),
            **kwargs,
        }
        torch.save(checkpoint, path)

    def load_checkpoint(self, path: str) -> Dict[str, Any]:
        """
        åŠ è½½æ£€æŸ¥ç‚¹ã€‚

        Args:
            path: æ£€æŸ¥ç‚¹è·¯å¾„

        Returns:
            æ£€æŸ¥ç‚¹å­—å…¸
        """
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(path, map_location=self.device)

        # åŠ è½½æ¨¡å‹çŠ¶æ€
        self.get_model().load_state_dict(checkpoint["model"])

        return checkpoint


class GradientAccumulator:
    """
    å¤§æ‰¹é‡çš„æ¢¯åº¦ç´¯ç§¯ç®¡ç†å™¨ã€‚

    åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰ç´¯ç§¯å¤šä¸ªå°æ‰¹é‡çš„æ¢¯åº¦ã€‚
    """

    def __init__(
        self,
        accumulation_steps: int = 1,
        distributed: bool = False,
    ):
        """
        Args:
            accumulation_steps: ç´¯ç§¯æ¢¯åº¦çš„æ­¥æ•°
            distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
        """
        self.accumulation_steps = accumulation_steps
        self.distributed = distributed
        self.current_step = 0

    def should_step(self) -> bool:
        """æ£€æŸ¥ä¼˜åŒ–å™¨æ˜¯å¦åº”è¯¥æ­¥éª¤ã€‚"""
        return (self.current_step + 1) % self.accumulation_steps == 0

    def scale_loss(self, loss: torch.Tensor) -> torch.Tensor:
        """
        æŒ‰ç´¯ç§¯æ­¥æ•°ç¼©æ”¾æŸå¤±ã€‚

        Args:
            loss: æœªç¼©æ”¾çš„æŸå¤±

        Returns:
            ç¼©æ”¾åçš„æŸå¤±
        """
        return loss / self.accumulation_steps

    def step(self):
        """é€’å¢æ­¥è®¡æ•°å™¨ã€‚"""
        self.current_step += 1

    def reset(self):
        """é‡ç½®æ­¥è®¡æ•°å™¨ã€‚"""
        self.current_step = 0

    def synchronize_gradients(self):
        """åŒæ­¥è·¨è¿›ç¨‹çš„æ¢¯åº¦ (å¦‚æœåˆ†å¸ƒå¼)ã€‚"""
        if self.distributed and dist.is_initialized():
            # DDP è‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥
            pass


class SequenceTensorParallel:
    """
    åºåˆ—ç»´åº¦çš„å¼ é‡å¹¶è¡Œã€‚

    å°†é•¿åºåˆ—åˆ†å‰²åˆ°å¤šä¸ª GPU:
        GPU 0: æ®‹åŸº 0 åˆ° L/N
        GPU 1: æ®‹åŸº L/N åˆ° 2L/N
        ...
        GPU N-1: æ®‹åŸº (N-1)L/N åˆ° L

    é€‚ç”¨äºæ— æ³•æ”¾åœ¨å•ä¸ª GPU ä¸Šçš„è¶…é•¿åºåˆ—ã€‚
    """

    def __init__(
        self,
        world_size: int,
        rank: int,
    ):
        """
        Args:
            world_size: ç”¨äºå¼ é‡å¹¶è¡Œçš„ GPU æ•°é‡
            rank: æ­¤ GPU çš„æ’å
        """
        self.world_size = world_size
        self.rank = rank

    def split_sequence(
        self,
        x: torch.Tensor,  # [B, L, ...]
        dim: int = 1,
    ) -> torch.Tensor:
        """
        è·¨ GPU åˆ†å‰²åºåˆ—ã€‚

        Args:
            x: è¾“å…¥å¼ é‡ [B, L, ...]
            dim: åˆ†å‰²çš„ç»´åº¦ (é€šå¸¸æ˜¯åºåˆ—ç»´åº¦)

        Returns:
            æœ¬åœ°å— [B, L/N, ...]
        """
        if self.world_size == 1:
            return x

        # å‡åŒ€åˆ†å‰²
        chunks = torch.chunk(x, self.world_size, dim=dim)
        return chunks[self.rank]

    def gather_sequence(
        self,
        x: torch.Tensor,  # [B, L/N, ...]
        dim: int = 1,
    ) -> torch.Tensor:
        """
        ä»æ‰€æœ‰ GPU æ”¶é›†åºåˆ—ã€‚

        Args:
            x: æœ¬åœ°å— [B, L/N, ...]
            dim: æ”¶é›†çš„ç»´åº¦

        Returns:
            å®Œæ•´åºåˆ— [B, L, ...]
        """
        if self.world_size == 1:
            return x

        if not dist.is_initialized():
            return x

        # All-gather
        chunks = [torch.zeros_like(x) for _ in range(self.world_size)]
        dist.all_gather(chunks, x)

        # è¿æ¥
        return torch.cat(chunks, dim=dim)

    def all_reduce_mean(self, x: torch.Tensor) -> torch.Tensor:
        """
        å¸¦å‡å€¼èšåˆçš„ all-reduceã€‚

        Args:
            x: è¦è§„çº¦çš„å¼ é‡

        Returns:
            è§„çº¦åçš„å¼ é‡
        """
        if self.world_size == 1:
            return x

        if dist.is_initialized():
            dist.all_reduce(x, op=dist.ReduceOp.SUM)
            x = x / self.world_size

        return x


def get_distributed_sampler(
    dataset,
    config: DistributedConfig,
    shuffle: bool = True,
) -> Optional[torch.utils.data.Sampler]:
    """
    è·å–æ•°æ®é›†çš„åˆ†å¸ƒå¼é‡‡æ ·å™¨ã€‚

    Args:
        dataset: è¦é‡‡æ ·
        config: åˆ†å¸ƒå¼é…ç½®
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®

    Returns:
        å¦‚æœä¸æ˜¯åˆ†å¸ƒå¼åˆ™è¿”å› Noneï¼Œå¦åˆ™è¿”å›åˆ†å¸ƒå¼é‡‡æ ·å™¨
    """
    if not config.is_distributed():
        return None

    from torch.utils.data.distributed import DistributedSampler

    return DistributedSampler(
        dataset,
        num_replicas=config.world_size,
        rank=config.rank,
        shuffle=shuffle,
    )


def test_distributed_utilities():
    """æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ã€‚"""
    print("=" * 80)
    print("æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒå·¥å…· (Stage 5)")
    print("=" * 80)
    print()

    # æµ‹è¯• 1: åˆ†å¸ƒå¼é…ç½®
    print("æµ‹è¯• 1: åˆ†å¸ƒå¼é…ç½®")
    print("-" * 80)

    config = DistributedConfig(world_size=4, rank=0, local_rank=0)
    print(f"  ä¸–ç•Œå¤§å°: {config.world_size}")
    print(f"  æ’å: {config.rank}")
    print(f"  æ˜¯åˆ†å¸ƒå¼: {config.is_distributed()}")
    print(f"  æ˜¯ä¸»è¿›ç¨‹: {config.is_main_process()}")
    print(f"  âœ… é…ç½®å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 2: æ¢¯åº¦ç´¯ç§¯
    print("æµ‹è¯• 2: æ¢¯åº¦ç´¯ç§¯")
    print("-" * 80)

    accumulator = GradientAccumulator(accumulation_steps=4)

    for step in range(8):
        should_step = accumulator.should_step()
        print(f"  æ­¥ {step}: should_step={should_step}")
        accumulator.step()

    print(f"  âœ… æ¢¯åº¦ç´¯ç§¯å·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 3: åºåˆ—å¼ é‡å¹¶è¡Œ (æ¨¡æ‹Ÿ)
    print("æµ‹è¯• 3: åºåˆ—å¼ é‡å¹¶è¡Œ (æ¨¡æ‹Ÿ)")
    print("-" * 80)

    world_size = 4
    B, L, C = 2, 1024, 128

    x = torch.randn(B, L, C)

    for rank in range(world_size):
        tp = SequenceTensorParallel(world_size=world_size, rank=rank)
        local_chunk = tp.split_sequence(x, dim=1)
        expected_len = L // world_size
        print(f"  æ’å {rank}: æœ¬åœ°å—å½¢çŠ¶ {local_chunk.shape} "
              f"(æœŸæœ› L={expected_len})")
        assert local_chunk.shape[1] == expected_len, "åˆ†å‰²ä¸æ­£ç¡®"

    print(f"  âœ… åºåˆ—å¹¶è¡Œæ¨¡æ‹Ÿå·¥ä½œæ­£å¸¸!")
    print()

    # æµ‹è¯• 4: æ¨¡å‹åŒ…è£…å™¨ (å• GPU)
    print("æµ‹è¯• 4: åˆ†å¸ƒå¼æ¨¡å‹åŒ…è£…å™¨ (å• GPU)")
    print("-" * 80)

    model = nn.Linear(128, 128)
    config_single = DistributedConfig(world_size=1, rank=0, local_rank=0)

    wrapper = DistributedModelWrapper(model, config_single)
    print(f"  æ˜¯åˆ†å¸ƒå¼: {wrapper.is_distributed}")
    print(f"  è®¾å¤‡: {wrapper.device}")
    print(f"  âœ… æ¨¡å‹åŒ…è£…å™¨å·¥ä½œæ­£å¸¸!")
    print()

    print("=" * 80)
    print("ğŸ‰ æ‰€æœ‰åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·æµ‹è¯•é€šè¿‡!")
    print("=" * 80)
    print()
    print("æ³¨æ„: å®Œå…¨å¤š GPU æµ‹è¯•éœ€è¦å®é™…çš„åˆ†å¸ƒå¼ç¯å¢ƒã€‚")
    print("ä½¿ç”¨ torchrun æµ‹è¯•å¤šä¸ª GPU:")
    print("  torchrun --nproc_per_node=4 test_distributed.py")


if __name__ == "__main__":
    test_distributed_utilities()
