# =============================================================================
# Stage 4 Configuration - Advanced Optimizations
# =============================================================================
# Stage: 1 + 2 + 3 + V2 + 4
# Max Length: L=8192-12288
# Memory: ~20GB for L=8192
# Memory Saving: 2-3x additional (checkpointing)
# Speed Improvement: 12x (axial attention)
# Use Case: Very long sequences with model compression
# =============================================================================

# Basic Training Settings
name stage4_advanced_training
seed 42
batchSize 1
gradientAccumulation 8
maximumNumResidues 8192

# Model Architecture
singleFeatureDimension 128
pairFeatureDimension 96         # Further reduced
structureEncoderDepth 3
structureEncoderHeads 4

# *** STAGE 1: Factorized Pairs ***
useFactorizedPairs True
zFactorRank 2
pairFactorDim 64

# *** STAGE 2: Factorized Triangle Ops ***
useFactorizedTriangleOps True
triangleUpdateRank 4
triangleAttentionChunkSize 256
numTriangleLayers 4

# *** STAGE 3: Training Optimizations ***
useProgressiveTraining True
progressiveStartLength 1024
progressiveEndLength 8192
progressiveNumStages 4
progressiveEpochsPerStage 50

useChunkedLoss True
lossChunkSize 512

useMixedPrecision True
precision 16

# *** STAGE 3 V2: Sparse k-NN ***
useSparseKNN True
kNeighbors 32
knnStrategy hybrid
knnUpdateFrequency 10

# *** STAGE 4 KEY FEATURES: Advanced Optimizations ***

# Axial Attention: Decompose L² attention into 2×L
useAxialAttention True
axialRowChunkSize 128          # Process 128 rows at a time
axialColChunkSize 128          # Process 128 columns at a time

# Gradient Checkpointing: Trade compute for memory
useGradientCheckpointing True
checkpointingStrategy adaptive  # Options: uniform, adaptive, selective
checkpointEveryNLayers 1       # Checkpoint every layer

# Model Compression: Parameter sharing
useModelCompression True
compressionStrategy universal   # Options: universal, adjacent, none
numSharedLayers 2              # Share parameters across 2 layers
compressionRatio 4             # 4x parameter reduction

# Learning Rate & Optimization
learningRate 2e-4
warmupEpochs 100
numEpochs 500
gradientClipVal 1.0
weightDecay 0.01

# Loss Settings
lossWeights {
    "translation": 1.0,
    "rotation": 1.0,
    "backbone": 1.0,
    "chi": 0.5,
    "mhc": 0.1
}

useMHCLoss True
mhcNumScales 3
mhcWeights [0.5, 0.3, 0.2]

# Diffusion Settings
diffusionTimesteps 1000
diffusionSchedule cosine
noiseSchedule exponential

# Hardware Settings
numWorkers 4
accelerator gpu
devices 1

# Monitoring
logEveryNSteps 50
saveEveryNEpochs 10
validateEveryNEpochs 5

# Stage Features
useDistributed False
